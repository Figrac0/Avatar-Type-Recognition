#  Avatar Type Recognition 

## Overview
This project presents a complete experimental study on **avatar image classification** across three domains:
**real photos**, **drawn illustrations**, and **AI-generated avatars**.  
Nine convolutional architectures were trained, evaluated, and compared using a unified experimental protocol,  
focusing on **accuracy, calibration, robustness, bias, efficiency, and generalization**.

---

##  Research Goal
To design and compare deep convolutional neural network models capable of recognizing the **type of avatar image**,  
and to analyze their **explainability, domain bias, and generalization ability** under various real-world distortions.

---

##  Datasets
- **Training / Validation** ‚Äî mixed dataset of ~3K avatar images categorized into:
  - `real` ‚Äî real human photos,
  - `drawing` ‚Äî hand-drawn or illustrated portraits,
  - `generated` ‚Äî AI-synthesized avatars.
- **Test set (Block 9)** ‚Äî 300 unseen mixed images (`real_test`, `drawn_test`, `AI_test`).
- **Out-of-Domain (Block 16)** ‚Äî 5 external sets:
  - `children_adults` (human age variation)
  - `obj` (non-human objects)
  - `simpsons` (cartoon faces)
  - `animal_faces` (animal avatars)
  - `muffin_vs_chihuahua` (visual ambiguity test)

All images were normalized to `224√ó224` px, augmented (flip, rotation, noise), and loaded with PyTorch DataLoader.

---

##  Model Architectures
The study compares **9 convolutional models** trained under identical conditions:

| Architecture | Training Mode | Description |
|---------------|---------------|--------------|
| **MobileNetV3 Small 100** | Full | Lightweight CNN baseline optimized for mobile inference |
| **ResNet-50** | Full | Strong baseline with skip connections |
| **EfficientNet-B0** | Frozen Backbone | Transfer-learning baseline |
| **ConvNeXt-Tiny (Stage 1/2)** | Progressive Unfreeze | Modernized ResNet family, two-stage fine-tuning |
| **MobileNetV3 Few-Shot (4 / 12 epochs)** | Low-data | Generalization from small datasets |
| **ResNet-18 Few-Shot (4 / 12 epochs)** | Low-data | Compact residual model for small data regimes |

Each network was trained with **CrossEntropyLoss**, **Adam optimizer**,  
and validated on identical folds for comparability.

---

##  Experimental Blocks
All analysis was performed in 18 structured blocks:

| ‚Ññ | Block | Goal |
|---|-------|------|
| **9** | Unified blind test (300 images) | Evaluate all models on unseen mixed data |
| **10** | Robustness test | Blur, noise, rotation, JPEG, brightness distortions |
| **11** | Softmax confidence & calibration | Reliability diagrams, Expected Calibration Error (ECE) |
| **12** | Feature embedding visualization | t-SNE projection of latent features |
| **12B** | Cluster structure analysis | Silhouette, intra/inter-class distance |
| **13** | Ensemble soft-voting | Combined prediction across models |
| **13B‚Äì13C** | Ensemble correlation & weighting | Weighted voting based on accuracy √ó (1 ‚Äì ECE) |
| **14‚Äì14C** | Grad-CAM explainability | Heatmap visualization and model √ó class comparison walls |
| **15** | Efficiency benchmark | FPS / latency (CPU & GPU) vs. model complexity |
| **16** | Out-of-Domain & Bias test | Generalization to unseen domains |
| **17** | Error & Bias analysis | Model bias maps, confidence distribution |
| **18** | Final integrated report | Normalized metrics & integrated score synthesis |

---

##  Key Metrics
Each model was evaluated by:
- `Accuracy@300` ‚Äî main test accuracy,
- `Robustness` ‚Äî mean accuracy under 5 distortions,
- `ECE` ‚Äî calibration error,
- `Unbias` ‚Äî fairness under out-of-domain images,
- `Silhouette / SeparationRatio` ‚Äî feature space quality,
- `FPS / Params(M)` ‚Äî computational efficiency,
- `Integrated Score` ‚Äî normalized aggregate index.

---

##  Main Findings
- **ConvNeXt-Tiny Stage 2** achieved top overall accuracy and well-balanced calibration.  
- **ResNet18 Few-Shot 12 epochs** showed the best calibration (lowest ECE).  
- **MobileNetV3 Few-Shot 12 epochs** offered the best trade-off between speed and accuracy.  
- **Soft-Voting Ensemble** outperformed individual models by ‚âà 4-5 % F1-gain.  
- **Out-of-Domain Bias** revealed overconfidence on non-human imagery ‚Äî mitigated in ensemble.  
- **Grad-CAM Analysis** confirmed class-specific attention zones and interpretable focus maps.  
- **Efficiency Benchmark** showed MobileNetV3 > 100 FPS on GPU and ~15 FPS on CPU.

---

##  Visual Results

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/ResNet18_FewShot12ep_reliability.png" width="450"/><br/>
  <em>Reliability diagram ‚Äî ResNet18 Few-Shot 12 epochs (calibration curve)</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/bias_map_models.png" width="450"/><br/>
  <em>Bias map ‚Äî class distribution across models on out-of-domain data</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/gradcam_wall_all_models.jpg" width="600"/><br/>
  <em>Cross-model Grad-CAM wall ‚Äî attention comparison for all architectures</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/model_prediction_correlation.png" width="450"/><br/>
  <em>Prediction correlation between models (ensemble diversity matrix)</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/ood_heatmap.png" width="450"/><br/>
  <em>Heatmap of predictions on Out-of-Domain datasets</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/robustness_bar.png" width="450"/><br/>
  <em>Model robustness under blur, noise, rotation, brightness, JPEG compression</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/speed_vs_complexity_gpu.png" width="450"/><br/>
  <em>Speed vs Complexity ‚Äî GPU FPS vs model size (efficiency benchmark)</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/tsne_ConvNeXt-Tiny_Stage2.png" width="450"/><br/>
  <em>t-SNE embedding ‚Äî ConvNeXt-Tiny Stage 2 feature separation</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/tsne_ResNet50.png" width="450"/><br/>
  <em>t-SNE embedding ‚Äî ResNet50 latent space clusters</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/tsne_MobileNetV3_FewShot12ep.png" width="450"/><br/>
  <em>t-SNE embedding ‚Äî MobileNetV3 Few-Shot 12 epochs feature distribution</em>
</p>

---

##  Summary
This project provides a **full-stack deep-learning benchmark** for avatar type recognition:
- from **training and evaluation** to **explainability and bias auditing**,
- fully reproducible via Colab (works on CPU or GPU),
- delivering both **scientific metrics** and **visual analytics**.

# üß† –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –∞–≤–∞—Ç–∞—Ä–æ–≤ ‚Äî –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–µ–∫—Ç

## –û–±–∑–æ—Ä
–î–∞–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–ª–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ü–∏–∫–ª –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∞–≤–∞—Ç–∞—Ä–æ–≤ –ø–æ —Ç—Ä—ë–º –¥–æ–º–µ–Ω–∞–º:  
**—Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏**, **—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–∏** –∏ **AI-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è**.  
–ë—ã–ª–∏ –æ–±—É—á–µ–Ω—ã, –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ —Å—Ä–∞–≤–Ω–µ–Ω—ã –¥–µ–≤—è—Ç—å —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤ –µ–¥–∏–Ω–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥–µ,  
—Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ **—Ç–æ—á–Ω–æ—Å—Ç—å, –∫–∞–ª–∏–±—Ä–æ–≤–∫—É, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å, —Å–º–µ—â–µ–Ω–∏–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å**.

---

## –¶–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å **—Ç–∏–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∞–≤–∞—Ç–∞—Ä–∞**,  
–∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö **–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å, —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —Å–º–µ—â–µ–Ω–∏—é –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é**  
–Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç –æ–±—É—á–∞—é—â–µ–≥–æ –¥–æ–º–µ–Ω–∞.

---

## –î–∞—Ç–∞—Å–µ—Ç—ã
- **–û–±—É—á–∞—é—â–∞—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∏** ‚Äî –æ–∫–æ–ª–æ 3 000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã—Ö –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:
  - `real` ‚Äî —Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ª—é–¥–µ–π;
  - `drawing` ‚Äî —Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–∏ –∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä—Ç—Ä–µ—Ç—ã;
  - `generated` ‚Äî –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π.
- **–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ (–ë–ª–æ–∫ 9)** ‚Äî 300 —Ä–∞–Ω–µ–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–≤—à–∏—Ö—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (`real_test`, `drawn_test`, `AI_test`).
- **–í–Ω–µ-–¥–æ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ë–ª–æ–∫ 16)** ‚Äî 5 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤:
  - `children_adults` ‚Äî –¥–µ—Ç—Å–∫–∏–µ –∏ –≤–∑—Ä–æ—Å–ª—ã–µ –ª–∏—Ü–∞;
  - `obj` ‚Äî —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ñ—Ä—É–∫—Ç–æ–≤;
  - `simpsons` ‚Äî –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ –º—É–ª—å—Ç—Ñ–∏–ª—å–º–∞ ¬´–°–∏–º–ø—Å–æ–Ω—ã¬ª;
  - `animal_faces` ‚Äî –ª–∏—Ü–∞ –∂–∏–≤–æ—Ç–Ω—ã—Ö;
  - `muffin_vs_chihuahua` ‚Äî –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

–í—Å–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –¥–æ —Ä–∞–∑–º–µ—Ä–∞ `224√ó224`, –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã (–ø–æ–≤–æ—Ä–æ—Ç—ã, —à—É–º, –∑–µ—Ä–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ)  
–∏ –∑–∞–≥—Ä—É–∂–∞–ª–∏—Å—å —á–µ—Ä–µ–∑ `PyTorch DataLoader`.

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π
–í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–ª–∏—Å—å **9 –º–æ–¥–µ–ª–µ–π CNN**, –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö:

| –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------------|----------------|-----------|
| **MobileNetV3 Small 100** | –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | –õ–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ |
| **ResNet-50** | –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | –ë–∞–∑–æ–≤–∞—è –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ |
| **EfficientNet-B0** | –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–π –±—ç–∫–±–æ–Ω | –¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ—ë–≤ |
| **ConvNeXt-Tiny (Stage 1/2)** | –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ | –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ResNet —Å –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–π –¥–æ–æ–±—É—á–∞–µ–º–æ—Å—Ç—å—é |
| **MobileNetV3 Few-Shot (4 / 12 —ç–ø–æ—Ö)** | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ |
| **ResNet-18 Few-Shot (4 / 12 —ç–ø–æ—Ö)** | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è |

–í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **CrossEntropyLoss**, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ **Adam**  
–∏ –ø—Ä–æ–≤–µ—Ä—è–ª–∏—Å—å –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

---

## –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏
–†–∞–±–æ—Ç–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 18 –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤:

| ‚Ññ | –ë–ª–æ–∫ | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|---|------|-------------|
| **9** | –°–ª–µ–ø–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (300 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π) | –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö |
| **10** | –¢–µ—Å—Ç –Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å | –®—É–º, —Ä–∞–∑–º—ã—Ç–∏–µ, —è—Ä–∫–æ—Å—Ç—å, –ø–æ–≤–æ—Ä–æ—Ç, JPEG-—Å–∂–∞—Ç–∏–µ |
| **11** | –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ Softmax | –î–∏–∞–≥—Ä–∞–º–º—ã –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏, –æ—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (ECE) |
| **12** | –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ | –ü—Ä–æ–µ–∫—Ü–∏–∏ t-SNE –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π |
| **12B** | –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ | –ú–µ—Ç—Ä–∏–∫–∏ Silhouette –∏ –º–µ–∂-/–≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è |
| **13** | –≠–Ω—Å–µ–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (Soft-Voting) | –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π |
| **13B‚Äì13C** | –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –∏ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ | Soft-Voting —Å –≤–µ—Å–∞–º–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–µ |
| **14‚Äì14C** | Grad-CAM –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è | –¢–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–ª–∞–∂–∏ |
| **15** | –ë–µ–Ω—á–º–∞—Ä–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ | –ò–∑–º–µ—Ä–µ–Ω–∏–µ FPS, –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π |
| **16** | –í–Ω–µ-–¥–æ–º–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç | –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö |
| **17** | –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏ —Å–º–µ—â–µ–Ω–∏–π | –ö–∞—Ä—Ç—ã —Å–º–µ—â–µ–Ω–∏–π –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ |
| **18** | –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç | –°–≤–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ |

---

## –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
–ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º:
- **Accuracy@300** ‚Äî —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ;
- **Robustness** ‚Äî —Å—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –∏—Å–∫–∞–∂–µ–Ω–∏—è—Ö;
- **ECE** ‚Äî –æ—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏;
- **Unbias** ‚Äî —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–º–µ—â–µ–Ω–∏—é –Ω–∞ –≤–Ω–µ-–¥–æ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö;
- **Silhouette / SeparationRatio** ‚Äî –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤;
- **FPS / Params(M)** ‚Äî –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å;
- **Integrated Score** ‚Äî –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞.

---

## –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **ConvNeXt-Tiny Stage 2** –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏.  
- **ResNet-18 Few-Shot 12 —ç–ø–æ—Ö** –∏–º–µ–ª–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π ECE (–ª—É—á—à–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞).  
- **MobileNetV3 Few-Shot 12 —ç–ø–æ—Ö** –æ–±–µ—Å–ø–µ—á–∏–ª–∞ –ª—É—á—à–∏–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º.  
- **Soft-Voting Ensemble** —É–ª—É—á—à–∏–ª F1-–º–µ—Ç—Ä–∏–∫—É –Ω–∞ ‚âà 4‚Äì5 %.  
- **Out-of-Domain Test** –≤—ã—è–≤–∏–ª –∏–∑–±—ã—Ç–æ—á–Ω—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö,  
  —á—Ç–æ –±—ã–ª–æ —Å–≥–ª–∞–∂–µ–Ω–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∞–Ω—Å–∞–º–±–ª—è.  
- **Grad-CAM** –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–æ–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π.  
- **–ë–µ–Ω—á–º–∞—Ä–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏** –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ MobileNetV3 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç > 100 FPS –Ω–∞ GPU –∏ ‚âà 15 FPS –Ω–∞ CPU.

---

## –ò—Ç–æ–≥
–ü—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π **–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Å—Ç–µ–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤** –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é —Ç–∏–ø–æ–≤ –∞–≤–∞—Ç–∞—Ä–æ–≤:
- –æ—Ç **–æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è** –¥–æ **–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏ –∞—É–¥–∏—Ç–∞ —Å–º–µ—â–µ–Ω–∏–π**;
- –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –≤ **Google Colab** (CPU / GPU);
- —Å–æ—á–µ—Ç–∞–µ—Ç **–Ω–∞—É—á–Ω—É—é —Å—Ç—Ä–æ–≥–æ—Å—Ç—å** –∏ **–≤–∏–∑—É–∞–ª—å–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏—á–Ω–æ—Å—Ç—å**.
