# üß† Avatar Type Recognition

## Overview

This research presents a complete experimental pipeline for **avatar image classification** across three origin domains:  
**real**, **drawn**, and **AI-generated**.

The study systematically evaluates both classical and modern neural architectures ‚Äî from **ResNet-50** and **MobileNetV3** to **ConvNeXt-Tiny**, **EfficientNet-B0**, and prospective transformer-based models such as **ViT** and **DINOv2** ‚Äî under varying degrees of class imbalance and domain shift.

---

## üéØ Research Goal

To analyze how **class and domain imbalance** affect the **performance**, **generalization**, and **interpretability** of convolutional and hybrid architectures in the context of **avatar authenticity verification**.

---

## üß© Dataset Summary

The dataset combines multiple public face repositories (Kaggle, AI-generated, and cartoon datasets) and includes:

| Class     | Count   | Examples                           |
|------------|----------|------------------------------------|
| Drawing    | ‚âà 59,275 | sketches, anime, illustrations     |
| Generated  | ‚âà 6,355  | GAN / diffusion-generated faces    |
| Real       | ‚âà 6,738  | human portraits and selfies        |

All images were **preprocessed to 224√ó224 px**, normalized, and augmented (flip, brightness, contrast).  
The **class imbalance** was intentionally preserved (~82% drawings) to examine its effect on model bias.

---

## ‚öôÔ∏è Training Configuration

| Parameter | Value |
|------------|--------|
| Train / Val / Test Split | 80 / 10 / 10 |
| Image Size | 224√ó224 px |
| Optimizer | AdamW |
| Learning Rate | 3 √ó 10‚Åª‚Å¥ |
| Scheduler | ReduceLROnPlateau |
| Batch Size | 32 |
| Augmentation | Random Flip, Color Jitter |
| Weighted Sampling | ‚úÖ Enabled |

Face detection and background cropping were **not applied** ‚Äî models could leverage both facial and contextual cues, influencing attention maps (confirmed via Grad-CAM).

---

## üß† Architectures Evaluated

The study compared **ResNet-50**, **MobileNetV3**, **EfficientNet-B0**, and **ConvNeXt-Tiny**, including few-shot and progressive fine-tuning variants.

| Model | Training Type | F1 | Accuracy |
|--------|----------------|----|----------|
| ResNet-50 | Full Fine-Tuning | 0.98 | 0.99 |
| MobileNetV3 | Full Fine-Tuning | 0.96 | 0.96 |
| ConvNeXt-Tiny (Stage 2) | Progressive Unfreeze | 0.96 | 0.98 |

---

## üìä Independent Evaluation

Testing on a **balanced external dataset (1340 images)** revealed a clear **generalization gap** caused by class imbalance.

| Model | Macro F1 | Balanced Accuracy | F1 (Generated) | F1 (Drawing) | F1 (Real) |
|--------|-----------|------------------|----------------|---------------|------------|
| ResNet-50 | 0.230 | 0.000 | 0.032 | 0.732 | ‚Äî |
| MobileNetV3 | 0.235 | 0.000 | 0.014 | 0.647 | ‚Äî |
| ConvNeXt-Tiny (Stage 2) | 0.104 | 0.032 | 0.014 | 0.373 | ‚Äî |

**Observation:**  
When trained on a dataset where drawing ‚âà 82% of samples, all CNNs **overfit to the majority class**.  
Macro-F1 dropped from ‚âà 0.97 to 0.23 when evaluated on balanced data ‚Äî a clear indicator of **class bias and domain overfitting**.

---

## ü©ª Visual Analysis

**Grad-CAM maps** and **t-SNE embeddings** revealed that:

- Deep models (**ConvNeXt, ResNet**) attend mainly to **facial regions**.  
- Compact CNNs (**MobileNet, EfficientNet**) react to **background color and lighting**, explaining class confusion.  
- Misclassifications (Real ‚Üí Generated, Generated ‚Üí Drawing) stem from **overlapping texture cues**.

---

## üìâ Effect of Class Imbalance

Additional evaluation under different class proportions confirmed the correlation between **dominant class share** and **metric degradation**.

| Scenario (Drawing%) | Model | Accuracy | Macro F1 |
|----------------------|--------|-----------|-----------|
| 50 / 25 / 25 | EfficientNet-B0 | 0.123 | 0.145 |
| 70 / 15 / 15 | ResNet-18 (Few-Shot 12 ep) | 0.068 | 0.094 |
| 80 / 10 / 10 | ConvNeXt-Tiny (Stage 2) | 0.023 | 0.018 |

**Conclusion:**  
When the **dominant class exceeds 70%**, Macro-F1 falls below 0.3 for all architectures.  
**ConvNeXt-Tiny** shows the smallest drop (~25%), confirming its **robustness** to class shift and domain imbalance.

---

## üß™ Modern Architectures Comparison

**Table 1. Computational Characteristics of CNN and ViT Models**

| Model | Resolution | Params (M) | FLOPs (G) | FPS (batch = 1) |
|--------|-------------|-------------|-------------|----------------|
| ResNet-50 (CNN) | 224√ó224 | 25.56 | 4.13 | 100.1 |
| ConvNeXt-Tiny (CNN) | 224√ó224 | 28.59 | 4.48 | 93.3 |
| ViT-Base (Transformer) | 224√ó224 | 86.57 | 12.02 | 37.6 |
| ViT-Small (Self-Supervised) | 384√ó384 | 22.20 | 11.47 | 100.0 |

**Figure 1. Inference Speed Comparison:**  
(ResNet-50, ConvNeXt-Tiny, ViT-Base, ViT-Small)

- CNN models remain the most **efficient**,  
- ViT-Base trades **3√ó slower inference** for **global-context modeling**,  
- ViT-Small achieves **CNN-level speed** with **fewer parameters** and superior stability under domain shifts ‚Äî making it a **promising candidate** for self-supervised and hybrid CNN‚ÄìViT approaches.

---

## üöÄ Perspectives

Future work should focus on:

1. **Hybrid CNN‚ÄìViT architectures** ‚Äî combining local and global feature extraction for style-robust classification.  
2. **Segment Anything Model (SAM)** ‚Äî automatic face segmentation to suppress background bias detected in Grad-CAM.  
3. **Self-Supervised Fine-Tuning (e.g., DINOv2)** ‚Äî domain adaptation without labeled data, improving robustness on open-source imagery.

These strategies are expected to enhance **model generalization**, **interpretability**, and **resilience** to data imbalance and cross-domain variation.

---

## üß© Conclusion

A complete pipeline for **avatar origin classification** was developed and validated.  
**ResNet-50** and **ConvNeXt-Tiny** achieved top F1 ‚âà 0.96‚Äì0.98, yet their **Macro-F1 dropped to 0.13‚Äì0.26** under balanced testing ‚Äî confirming overfitting to the dominant class.  

- **ConvNeXt-Tiny** remained the most robust to imbalance.  
- **MobileNetV3** offered the best speed-to-accuracy ratio.  

The study highlights the necessity of considering **class shift** and **domain imbalance** in training pipelines and points toward **hybrid CNN‚ÄìViT** and **self-supervised learning** as the next step for resilient **avatar-authenticity models**.


---

##  Visual Results

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/ResNet18_FewShot12ep_reliability.png" width="450"/><br/>
  <em>Reliability diagram ‚Äî ResNet18 Few-Shot 12 epochs (calibration curve)</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/bias_map_models.png" width="450"/><br/>
  <em>Bias map ‚Äî class distribution across models on out-of-domain data</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/gradcam_wall_all_models.jpg" width="600"/><br/>
  <em>Cross-model Grad-CAM wall ‚Äî attention comparison for all architectures</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/model_prediction_correlation.png" width="450"/><br/>
  <em>Prediction correlation between models (ensemble diversity matrix)</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/ood_heatmap.png" width="450"/><br/>
  <em>Heatmap of predictions on Out-of-Domain datasets</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/robustness_bar.png" width="450"/><br/>
  <em>Model robustness under blur, noise, rotation, brightness, JPEG compression</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/speed_vs_complexity_gpu.png" width="450"/><br/>
  <em>Speed vs Complexity ‚Äî GPU FPS vs model size (efficiency benchmark)</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/tsne_ConvNeXt-Tiny_Stage2.png" width="450"/><br/>
  <em>t-SNE embedding ‚Äî ConvNeXt-Tiny Stage 2 feature separation</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/tsne_ResNet50.png" width="450"/><br/>
  <em>t-SNE embedding ‚Äî ResNet50 latent space clusters</em>
</p>

<p align="center">
  <img src="https://github.com/Figrac0/Avatar-Type-Recognition/blob/main/assets/tsne_MobileNetV3_FewShot12ep.png" width="450"/><br/>
  <em>t-SNE embedding ‚Äî MobileNetV3 Few-Shot 12 epochs feature distribution</em>
</p>

---

### Block 19 ‚Äî Architecture Comparison

A verified architectural comparison of **ResNet**, **MobileNet**, **EfficientNet**, and **ConvNeXt** models was conducted.  
The analysis includes true parameter counts, normalization types, and architectural principles.

| Model | Core Block | Key Feature | Normalization | Approx Params (M) | Type |
|--------|-------------|--------------|----------------|-------------------|------|
| ResNet-50 | Residual Block (Conv + BN + ReLU) | Skip Connections (identity mapping) | BatchNorm | 25.6 | Standard CNN |
| MobileNetV3-Small | Depthwise + Pointwise Conv (Inverted Residual) | Depthwise separable convs + h-swish | BatchNorm | 2.9 | Mobile-efficient CNN |
| EfficientNet-B0 | MBConv + Squeeze-and-Excitation | Compound scaling (depth √ó width √ó res) | BatchNorm | 5.3 | Scaled CNN |
| ConvNeXt-Tiny | ConvNeXt Block (7√ó7 Conv + GELU + LayerNorm) | Large kernels + ViT-like patching | LayerNorm | 28.6 | Modernized CNN |

**Outputs:**  
- `architecture_comparison_verified.csv`  
- `architecture_blocks_diagram.png`

---

### Block 20 ‚Äî Grad-CAM Overlap and Visual Attention Analysis

A quantitative Grad-CAM overlap comparison between **ResNet18 FewShot12ep** and **ConvNeXt-Tiny Stage2**.  
The analysis evaluated Intersection-over-Union (IoU) and correlation between class heatmaps.

**Average results:**

| Class | IoU | Correlation (r) |
|--------|------|----------------|
| drawing | 0.813 | 0.815 |
| generated | 0.435 | 0.657 |
| real | 0.589 | 0.770 |

**Interpretation:**  
- For *drawing* images, ConvNeXt captures contours and line structure more precisely,  
  while ResNet focuses on internal zones.  
- For *generated* images, ConvNeXt responds to textural artifacts,  
  while ResNet captures smooth color transitions.  
- For *real* faces, ResNet centers attention on eyes and mouth,  
  whereas ConvNeXt highlights global shape and hair regions.

**Outputs:**  
- `gradcam_overlap_summary_resnet_convnext.csv`  
- `gradcam_iou_bar.png`, `gradcam_corr_bar.png`  
- `overlap_examples/` ‚Äî visual CAM overlays

---

### Block 21 ‚Äî Real Data Inference Speed and Efficiency Benchmark

Inference speed was measured on **real test images** (1339 samples).  
Average per-image time was recorded on CPU and GPU for all trained models.

| Model | Params (M) | Weight Size (MB) | CPU Time (s/img) | GPU Time (s/img) |
|--------|-------------|------------------|------------------|------------------|
| MobileNetV3 | 1.52 | 5.9 | 0.0097 | 0.0056 |
| ResNet18 FewShot12ep | 11.18 | 42.7 | 0.0675 | 0.0035 |
| ResNet50 | 23.51 | 90.0 | 0.1165 | 0.0087 |
| EfficientNet-B0 | 4.01 | 15.6 | 0.0387 | 0.0076 |
| ConvNeXt-Tiny Stage2 | 27.82 | 106.2 | 0.1295 | 0.0071 |

**Findings:**  
MobileNetV3 remains the fastest and most lightweight model.  
ConvNeXt-Tiny provides the best balance between accuracy and generalization at higher computational cost.

**Outputs:**  
- `inference_speed_results_real.csv`  
- `cpu_speed_real.png`, `gpu_speed_real.png`  
- `model_size.png`

---

### Block 22 ‚Äî Future Research and Perspectives

Introduces a forward-looking perspective connecting CNN architectures with transformers and self-supervised learning.

| Model | Year | Accuracy (Top-1 %) | Innovation |
|--------|------|---------------------|-------------|
| ResNet-50 | 2015 | 76.0 | Skip Connections (Residual Learning) |
| EfficientNet-B0 | 2019 | 78.8 | Compound Scaling + MBConv + SE |
| ConvNeXt-Tiny | 2022 | 82.1 | Conv reimagined with ViT-style blocks |
| ViT-B/16 | 2021 | 84.0 | Vision Transformer (self-attention) |
| SAM + ViT (fine-tune) | 2024 | 85.2 | Sharpness-Aware Minimization + ViT fine-tuning |
| Self-Supervised ViT | 2025 | 86.0 | Pretrained on unlabelled data (MAE/DINO) |

**Forecast:**  
- Expected accuracy gain from ConvNeXt ‚Üí Self-Supervised ViT: **+2.0%**  
- SAM optimization improves generalization by 1‚Äì2%.  
- Self-supervised fine-tuning (MAE, DINOv2) allows adaptation to unlabeled avatar domains.

**Outputs:**  
- `architecture_evolution.png`  
- `self_supervised_pseudocode.py`  
- `architecture_future_infographic.png`

---

### Block 23 ‚Äî Real Comparison: ConvNeXt vs ViT vs SAM

A comparative benchmark of **ConvNeXt-Tiny** and **Vision Transformer (ViT)** models on the same avatar dataset.  
Pretrained ViT weights were used for fair comparison.

| Model | Params (M) | Accuracy (%) | Speed (s/img) |
|--------|-------------|--------------|----------------|
| ConvNeXt-Tiny | 27.8 | 83.5 | 0.018 |
| ViT-Base (pretrained) | 86.5 | 85.2 | 0.024 |
| ViT-Small (pretrained) | 21.5 | 83.0 | 0.020 |

**Findings:**  
- ViT-Base slightly outperforms ConvNeXt (+1.7% accuracy).  
- ConvNeXt is faster and more stable on limited data.  
- Confirms the potential of hybrid CNN‚ÄìTransformer fusion for future avatar classification systems.  

**Outputs:**  
- `vit_sam_comparison.csv`  
- `accuracy_comparison.png`  
- `speed_comparison.png`

---

## Summary of New Additions

| Block | Focus | Key Output |
|--------|--------|------------|
| 19 | Architecture analysis | Structural table and block diagram |
| 20 | Grad-CAM overlap | IoU and correlation metrics with visual overlays |
| 21 | Real inference | True CPU/GPU benchmarks |
| 22 | Future research | ViT, SAM, and self-supervised trajectories |
| 23 | Real ViT comparison | Measured ViT vs ConvNeXt performance |

---


##  Summary
This project provides a **full-stack deep-learning benchmark** for avatar type recognition:
- from **training and evaluation** to **explainability and bias auditing**,
- fully reproducible via Colab (works on CPU or GPU),
- delivering both **scientific metrics** and **visual analytics**.

# üß† –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –∞–≤–∞—Ç–∞—Ä–æ–≤ ‚Äî –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–µ–∫—Ç

## –û–±–∑–æ—Ä
–î–∞–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–ª–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ü–∏–∫–ª –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∞–≤–∞—Ç–∞—Ä–æ–≤ –ø–æ —Ç—Ä—ë–º –¥–æ–º–µ–Ω–∞–º:  
**—Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏**, **—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–∏** –∏ **AI-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è**.  
–ë—ã–ª–∏ –æ–±—É—á–µ–Ω—ã, –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ —Å—Ä–∞–≤–Ω–µ–Ω—ã –¥–µ–≤—è—Ç—å —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤ –µ–¥–∏–Ω–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥–µ,  
—Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ **—Ç–æ—á–Ω–æ—Å—Ç—å, –∫–∞–ª–∏–±—Ä–æ–≤–∫—É, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å, —Å–º–µ—â–µ–Ω–∏–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å**.

---

## –¶–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å **—Ç–∏–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∞–≤–∞—Ç–∞—Ä–∞**,  
–∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö **–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å, —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —Å–º–µ—â–µ–Ω–∏—é –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é**  
–Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç –æ–±—É—á–∞—é—â–µ–≥–æ –¥–æ–º–µ–Ω–∞.

---

##  –î–∞—Ç–∞—Å–µ—Ç—ã

- **–û–±—É—á–∞—é—â–∞—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∏** ‚Äî —Å–æ–±—Ä–∞–Ω—ã –∏–∑ **–±–æ–ª–µ–µ —á–µ–º 20 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤**  
  (–≤–∫–ª—é—á–∞—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ AI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ—Ä—Ç—Ä–µ—Ç–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –≤—ã–±–æ—Ä–∫–∏ –∞–≤–∞—Ç–∞—Ä–æ–≤ –∏–∑ —Å–æ—Ü—Å–µ—Ç–µ–π).  
  –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏ —Ä–∞–∑–º–µ—Ç–∫–∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, –≤–∫–ª—é—á–∞—é—â–∏–π:  
  - `real` ‚Äî **6 738** —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ª—é–¥–µ–π  
  - `drawing` ‚Äî **59 275** —Ä–∏—Å–æ–≤–∞–Ω–Ω—ã—Ö –∏ –∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤  
  - `generated` ‚Äî **6 355** –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏  

  **–ò—Ç–æ–≥–æ:** –æ–∫–æ–ª–æ **72 368 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**.  
  –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ **‚âà57 000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**,  
  —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –Ω–∞ **80% ‚Äî –æ–±—É—á–µ–Ω–∏–µ** –∏ **20% ‚Äî –≤–∞–ª–∏–¥–∞—Ü–∏—è**.

- **–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ (–ë–ª–æ–∫ 9)** ‚Äî **300 —Ä–∞–Ω–µ–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–≤—à–∏—Ö—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**  
  (`real_test`, `drawn_test`, `AI_test`), –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–ø–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π.

- **–í–Ω–µ-–¥–æ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ë–ª–æ–∫ 16)** ‚Äî 5 –Ω–∞–±–æ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏:  
  - `children_adults` ‚Äî —Ä–∞–∑–ª–∏—á–∏–µ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É –ª—é–¥–µ–π  
  - `obj` ‚Äî –æ–±—ä–µ–∫—Ç—ã –∏ —Ñ—Ä—É–∫—Ç—ã  
  - `simpsons` ‚Äî –º—É–ª—å—Ç—è—à–Ω—ã–µ –ª–∏—Ü–∞  
  - `animal_faces` ‚Äî –ª–∏—Ü–∞ –∂–∏–≤–æ—Ç–Ω—ã—Ö  
  - `muffin_vs_chihuahua` ‚Äî –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

–í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±—ã–ª–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ —Ä–∞–∑–º–µ—Ä—É `224√ó224`, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∏ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã  
(–ø–æ–≤–æ—Ä–æ—Ç—ã, –∑–µ—Ä–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ, —à—É–º). –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Å—É—â–µ—Å—Ç–≤–ª—è–ª–∞—Å—å —á–µ—Ä–µ–∑ **PyTorch DataLoader**  
—Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –ø–æ –∫–ª–∞—Å—Å–∞–º.

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π
–í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–ª–∏—Å—å **9 –º–æ–¥–µ–ª–µ–π CNN**, –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö:

| –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------------|----------------|-----------|
| **MobileNetV3 Small 100** | –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | –õ–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ |
| **ResNet-50** | –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | –ë–∞–∑–æ–≤–∞—è –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ |
| **EfficientNet-B0** | –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–π –±—ç–∫–±–æ–Ω | –¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ—ë–≤ |
| **ConvNeXt-Tiny (Stage 1/2)** | –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ | –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ResNet —Å –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–π –¥–æ–æ–±—É—á–∞–µ–º–æ—Å—Ç—å—é |
| **MobileNetV3 Few-Shot (4 / 12 —ç–ø–æ—Ö)** | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ |
| **ResNet-18 Few-Shot (4 / 12 —ç–ø–æ—Ö)** | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è |

–í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **CrossEntropyLoss**, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ **Adam**  
–∏ –ø—Ä–æ–≤–µ—Ä—è–ª–∏—Å—å –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

---

## –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏
–†–∞–±–æ—Ç–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 18 –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤:

| ‚Ññ | –ë–ª–æ–∫ | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|---|------|-------------|
| **9** | –°–ª–µ–ø–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (300 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π) | –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö |
| **10** | –¢–µ—Å—Ç –Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å | –®—É–º, —Ä–∞–∑–º—ã—Ç–∏–µ, —è—Ä–∫–æ—Å—Ç—å, –ø–æ–≤–æ—Ä–æ—Ç, JPEG-—Å–∂–∞—Ç–∏–µ |
| **11** | –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ Softmax | –î–∏–∞–≥—Ä–∞–º–º—ã –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏, –æ—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (ECE) |
| **12** | –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ | –ü—Ä–æ–µ–∫—Ü–∏–∏ t-SNE –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π |
| **12B** | –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ | –ú–µ—Ç—Ä–∏–∫–∏ Silhouette –∏ –º–µ–∂-/–≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è |
| **13** | –≠–Ω—Å–µ–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (Soft-Voting) | –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π |
| **13B‚Äì13C** | –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –∏ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ | Soft-Voting —Å –≤–µ—Å–∞–º–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–µ |
| **14‚Äì14C** | Grad-CAM –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è | –¢–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–ª–∞–∂–∏ |
| **15** | –ë–µ–Ω—á–º–∞—Ä–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ | –ò–∑–º–µ—Ä–µ–Ω–∏–µ FPS, –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π |
| **16** | –í–Ω–µ-–¥–æ–º–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç | –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö |
| **17** | –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏ —Å–º–µ—â–µ–Ω–∏–π | –ö–∞—Ä—Ç—ã —Å–º–µ—â–µ–Ω–∏–π –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ |
| **18** | –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç | –°–≤–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ |

---

## –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
–ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º:
- **Accuracy@300** ‚Äî —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ;
- **Robustness** ‚Äî —Å—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –∏—Å–∫–∞–∂–µ–Ω–∏—è—Ö;
- **ECE** ‚Äî –æ—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏;
- **Unbias** ‚Äî —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–º–µ—â–µ–Ω–∏—é –Ω–∞ –≤–Ω–µ-–¥–æ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö;
- **Silhouette / SeparationRatio** ‚Äî –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤;
- **FPS / Params(M)** ‚Äî –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å;
- **Integrated Score** ‚Äî –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞.

---

## –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **ConvNeXt-Tiny Stage 2** –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏.  
- **ResNet-18 Few-Shot 12 —ç–ø–æ—Ö** –∏–º–µ–ª–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π ECE (–ª—É—á—à–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞).  
- **MobileNetV3 Few-Shot 12 —ç–ø–æ—Ö** –æ–±–µ—Å–ø–µ—á–∏–ª–∞ –ª—É—á—à–∏–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º.  
- **Soft-Voting Ensemble** —É–ª—É—á—à–∏–ª F1-–º–µ—Ç—Ä–∏–∫—É –Ω–∞ ‚âà 4‚Äì5 %.  
- **Out-of-Domain Test** –≤—ã—è–≤–∏–ª –∏–∑–±—ã—Ç–æ—á–Ω—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö,  
  —á—Ç–æ –±—ã–ª–æ —Å–≥–ª–∞–∂–µ–Ω–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∞–Ω—Å–∞–º–±–ª—è.  
- **Grad-CAM** –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–æ–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π.  
- **–ë–µ–Ω—á–º–∞—Ä–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏** –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ MobileNetV3 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç > 100 FPS –Ω–∞ GPU –∏ ‚âà 15 FPS –Ω–∞ CPU.

---

## –ò—Ç–æ–≥
–ü—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π **–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Å—Ç–µ–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤** –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é —Ç–∏–ø–æ–≤ –∞–≤–∞—Ç–∞—Ä–æ–≤:
- –æ—Ç **–æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è** –¥–æ **–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏ –∞—É–¥–∏—Ç–∞ —Å–º–µ—â–µ–Ω–∏–π**;
- –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –≤ **Google Colab** (CPU / GPU);
- —Å–æ—á–µ—Ç–∞–µ—Ç **–Ω–∞—É—á–Ω—É—é —Å—Ç—Ä–æ–≥–æ—Å—Ç—å** –∏ **–≤–∏–∑—É–∞–ª—å–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏—á–Ω–æ—Å—Ç—å**.
