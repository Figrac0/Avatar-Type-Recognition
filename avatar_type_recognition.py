# -*- coding: utf-8 -*-
"""Avatar_Type_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IFYsefy8tPpA2mwUu8kQXbjaFZy3h8-M
"""

from google.colab import drive
drive.mount('/content/drive')

# ================================================================
#  Avatar Type Recognition ‚Äî BLOCK 1 (FINAL FIXED)
#  Environment setup, dependencies, folder structure, config.json
# ================================================================


!pip install -q --upgrade pip
!pip install -q --upgrade sympy==1.13.3
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
!pip install -q timm==1.0.9 albumentations==1.3.1 opencv-python==4.10.0.84 \
                 scikit-learn==1.5.2 matplotlib==3.9.2 pandas==2.2.2 seaborn==0.13.2 tqdm==4.67.0



import os, sys, random, shutil, json
from pathlib import Path
import torch

print("PyTorch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
else:
    print("‚ö†Ô∏è CUDA –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ ‚Äî –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU (–¥–æ–ø—É—Å—Ç–∏–º–æ –¥–ª—è —Ç–µ—Å—Ç–∞).")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import timm, albumentations, cv2, sklearn, matplotlib, pandas as pd, seaborn as sns
from tqdm import tqdm

print("\n Libraries imported successfully:")
print("timm:", timm.__version__)
print("albumentations:", albumentations.__version__)
print("opencv:", cv2.__version__)
print("sklearn:", sklearn.__version__)

# -------------------- 2. –§–∏–∫—Å–∞—Ü–∏—è —Å–∏–¥–æ–≤ --------------------
import numpy as np
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
print(" Seeds fixed (deterministic mode ON)")

# -------------------- 3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ --------------------
WORK_ROOT = Path("/content/avatar_recog")
RAW_ROOT  = WORK_ROOT / "data" / "raw"
PROC_ROOT = WORK_ROOT / "data" / "processed"

DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs"
LOGS_DIR   = DRIVE_ROOT / "logs"

CLASSES = ["real", "drawing", "generated"]
IMG_SIZE = 224
print("Classes:", CLASSES, "| Image size:", IMG_SIZE)

# -------------------- 4. –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ --------------------
for p in [WORK_ROOT, RAW_ROOT, PROC_ROOT, DRIVE_ROOT, MODELS_DIR, OUT_DIR, LOGS_DIR]:
    p.mkdir(parents=True, exist_ok=True)

if PROC_ROOT.exists():
    shutil.rmtree(PROC_ROOT)
for split in ["train", "val", "test"]:
    for c in CLASSES:
        (PROC_ROOT / split / c).mkdir(parents=True, exist_ok=True)

# -------------------- 5. –£—Ç–∏–ª–∏—Ç—ã --------------------
def print_tree(root: Path, levels: int = 3):
    root = Path(root)
    print(f"\n== TREE: {root} ==")
    for path in sorted(root.rglob("*")):
        rel = path.relative_to(root)
        if len(rel.parts) <= levels:
            print(("üìÅ " if path.is_dir() else "üìÑ ") + str(rel))

def save_config():
    cfg = {
        "seed": SEED,
        "img_size": IMG_SIZE,
        "classes": CLASSES,
        "work_root": str(WORK_ROOT),
        "raw_root": str(RAW_ROOT),
        "proc_root": str(PROC_ROOT),
        "drive_root": str(DRIVE_ROOT),
        "models_dir": str(MODELS_DIR),
        "outputs_dir": str(OUT_DIR),
        "logs_dir": str(LOGS_DIR),
        "torch_version": torch.__version__,
        "cuda": torch.cuda.is_available(),
    }
    cfg_path = WORK_ROOT / "config.json"
    cfg_path.write_text(json.dumps(cfg, indent=2, ensure_ascii=False))
    print("Config saved:", cfg_path)

save_config()
print_tree(WORK_ROOT, levels=4)
print_tree(DRIVE_ROOT, levels=3)

print("\n‚úÖ Block 1 completed successfully.")
print("Environment, seeds, folder structure, and config.json are ready.")

"""–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞.

–í —ç—Ç–æ–º –±–ª–æ–∫–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ —Ä–∞–±–æ—á–µ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.
–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –∏ –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –≤—Å–µ —Ç—Ä–µ–±—É–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (torch, timm, albumentations, opencv, sklearn, matplotlib, pandas, seaborn, tqdm).
–î–∞–ª–µ–µ –∑–∞–¥–∞—é—Ç—Å—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ seed-–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —Å–æ–∑–¥–∞—é—Ç—Å—è —Ä–∞–±–æ—á–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∏ (data/raw, data/processed, models, outputs, logs) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª config.json.
–≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –∏ —á—ë—Ç–∫—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2
#  Synthetic data sanity check (MobileNetV3 & ResNet50)
# ================================================================

import torch, torchvision
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import timm

# ---------- 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH_SIZE = 8
CLASSES = ["real", "drawing", "generated"]
print("Device:", DEVICE)

# ---------- 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ----------
class SyntheticAvatarDataset(Dataset):
    def __init__(self, num_images=24, img_size=224, num_classes=3, transform=None):
        self.num_images = num_images
        self.img_size = img_size
        self.num_classes = num_classes
        self.transform = transform
        self.images = [np.uint8(np.random.rand(img_size, img_size, 3) * 255)
                       for _ in range(num_images)]
        self.labels = np.random.randint(0, num_classes, num_images)

    def __len__(self):
        return self.num_images

    def __getitem__(self, idx):
        img = Image.fromarray(self.images[idx])
        if self.transform:
            img = self.transform(img)
        label = self.labels[idx]
        return img, label

# ---------- 3. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è ----------
basic_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225]),
])

synthetic_ds = SyntheticAvatarDataset(transform=basic_tfms)
synthetic_dl = DataLoader(synthetic_ds, batch_size=BATCH_SIZE, shuffle=True)

# ---------- 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞—Ç—á–∞ ----------
images, labels = next(iter(synthetic_dl))
print("Batch shape:", images.shape)
print("Labels:", labels.tolist())

grid = torchvision.utils.make_grid(images, nrow=4, normalize=True)
plt.figure(figsize=(6,6))
plt.imshow(np.transpose(grid.numpy(), (1,2,0)))
plt.title("Synthetic avatar batch (normalized)")
plt.axis("off")
plt.show()

# ---------- 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π ----------
def test_model(model_name, num_classes=3):
    print(f"\n Testing {model_name} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)
    model = model.to(DEVICE)
    with torch.no_grad():
        x = images.to(DEVICE)
        out = model(x)
        print(f"Output shape: {out.shape} | Example logits (first row):")
        print(out[0].cpu().numpy())

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–µ –º–æ–¥–µ–ª–∏
test_model("mobilenetv3_small_100")
test_model("resnet50")

print("\n‚úÖ Block 2 completed successfully.")
print("Both models processed synthetic data correctly ‚Äî ready for real dataset.")

"""–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.–ó–¥–µ—Å—å —Å–æ–∑–¥–∞—é—Ç—Å—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ä–∞–Ω–¥–æ–º–Ω—ã–µ —à—É–º–æ–≤—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏) –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –º–∏–Ω–∏-–¥–∞—Ç–∞—Å–µ—Ç, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–±–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ‚Äî MobileNetV3 –∏ ResNet50 ‚Äî –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç, –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω—É–∂–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ [batch, num_classes].
–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è, —á—Ç–æ —Å–µ—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –∏ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –±–µ–∑ –æ—à–∏–±–æ–∫ –ø–µ—Ä–µ–¥ —Ä–µ–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2.1
#  Model inspection and comparison (ResNet50 vs MobileNetV3)
# ================================================================

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã
!pip install -q torchinfo thop

import torch
import timm
from torchinfo import summary

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
IMG_SIZE = 224

# 1. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
resnet = timm.create_model("resnet50", pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)
mobilenet = timm.create_model("mobilenetv3_small_100", pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)

# 2. –ö—Ä–∞—Ç–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (—Ç–∞–±–ª–∏—Ü–∞ —Å–ª–æ—ë–≤)
print("\n=== ResNet50 architecture (summary) ===")
print(summary(resnet, input_size=(1, 3, IMG_SIZE, IMG_SIZE),
              depth=3, verbose=1,
              col_names=("input_size", "output_size", "num_params")))

print("\n=== MobileNetV3 architecture (summary) ===")
print(summary(mobilenet, input_size=(1, 3, IMG_SIZE, IMG_SIZE),
              depth=3, verbose=1,
              col_names=("input_size", "output_size", "num_params")))

# 3. –ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
def count_params(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total, trainable

r_total, r_train = count_params(resnet)
m_total, m_train = count_params(mobilenet)

print(f"\nResNet50: total params = {r_total/1e6:.2f}M | trainable = {r_train/1e6:.2f}M")
print(f"MobileNetV3: total params = {m_total/1e6:.2f}M | trainable = {m_train/1e6:.2f}M")

# 4. FLOPs (–æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö)
try:
    from thop import profile
    dummy = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)
    flops_r, _ = profile(resnet, inputs=(dummy,), verbose=False)
    flops_m, _ = profile(mobilenet, inputs=(dummy,), verbose=False)
    print(f"\nFLOPs comparison (for 224x224 input):")
    print(f"ResNet50: {flops_r/1e9:.2f} GFLOPs")
    print(f"MobileNetV3: {flops_m/1e9:.2f} GFLOPs")
except Exception as e:
    print("\nSkipping FLOPs estimation:", e)

# 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
with torch.no_grad():
    x = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)
    feat_resnet = resnet.forward_features(x) if hasattr(resnet, "forward_features") else None
    feat_mobilenet = mobilenet.forward_features(x)
    print(f"\nResNet feature vector shape: {feat_resnet.shape if feat_resnet is not None else '(n/a)'}")
    print(f"MobileNet feature vector shape: {feat_mobilenet.shape}")

print("\nBlock 2.1 completed successfully ‚Äî models inspected and compared.")

"""–ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π, —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.

–ë–ª–æ–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
–° –ø–æ–º–æ—â—å—é torchinfo –≤—ã–≤–æ–¥–∏—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ —Å–ª–æ—ë–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–∂–¥–æ–π —Å–µ—Ç–∏.
–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ thop –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å FLOPs (—á–∏—Å–ª–æ –æ–ø–µ—Ä–∞—Ü–∏–π, –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å).
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ResNet50 –±–æ–ª–µ–µ —Ç—è–∂—ë–ª–∞—è, –Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å, —Ç–æ–≥–¥–∞ –∫–∞–∫ MobileNetV3 ‚Äî –ª—ë–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2.2
#  Inference speed benchmark (ResNet50 vs MobileNetV3)
# ================================================================

import torch, timm, time, numpy as np

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH = 8
ITERATIONS = 50

models = {
    "ResNet50": timm.create_model("resnet50", pretrained=False, num_classes=3).to(DEVICE).eval(),
    "MobileNetV3": timm.create_model("mobilenetv3_small_100", pretrained=False, num_classes=3).to(DEVICE).eval()
}

dummy = torch.randn(BATCH, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)

results = {}
for name, model in models.items():
    torch.cuda.synchronize() if DEVICE == "cuda" else None
    start = time.time()
    with torch.no_grad():
        for _ in range(ITERATIONS):
            _ = model(dummy)
    torch.cuda.synchronize() if DEVICE == "cuda" else None
    end = time.time()
    avg_ms = (end - start) / (ITERATIONS * BATCH) * 1000
    results[name] = avg_ms
    print(f"{name}: {avg_ms:.3f} ms/image")

import pandas as pd, matplotlib.pyplot as plt
df_speed = pd.DataFrame(list(results.items()), columns=["Model", "ms_per_image"])
plt.figure(figsize=(5,4))
plt.bar(df_speed["Model"], df_speed["ms_per_image"], color=["steelblue","orange"])
plt.ylabel("Inference time (ms per image)")
plt.title("Model Inference Speed Comparison")
plt.tight_layout()
plt.show()

print("\nSpeed benchmark completed.")

"""–ü—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∏–∑–º–µ—Ä–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (ms/image) –¥–ª—è ResNet50 –∏ MobileNetV3 –Ω–∞ GPU.
–≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º –º–æ–¥–µ–ª–∏.
MobileNetV3 –æ–∂–∏–¥–∞–µ–º–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ—ë –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–π –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2.3
#  Feature map visualization
# ================================================================

import torch, timm, matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image
import numpy as np

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224

# –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å —Å–≤–æ–µ–π –∫–∞—Ä—Ç–∏–Ω–∫–æ–π)
img = Image.fromarray(np.uint8(np.random.rand(IMG_SIZE, IMG_SIZE, 3)*255))
tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
x = tfm(img).unsqueeze(0).to(DEVICE)

# –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def visualize_features(model_name, layer_name):
    model = timm.create_model(model_name, pretrained=False, num_classes=3).to(DEVICE)
    layer = dict([*model.named_modules()])[layer_name]
    features = []
    def hook(_, __, output): features.append(output.detach().cpu())
    h = layer.register_forward_hook(hook)
    with torch.no_grad(): _ = model(x)
    h.remove()
    fmap = features[0][0].numpy()
    fig, axes = plt.subplots(2, 8, figsize=(12,3))
    for i, ax in enumerate(axes.flat):
        if i < fmap.shape[0]:
            ax.imshow(fmap[i], cmap="viridis")
        ax.axis("off")
    plt.suptitle(f"{model_name} ‚Äî {layer_name} feature maps")
    plt.tight_layout()
    plt.show()

visualize_features("resnet50", "conv1")
visualize_features("mobilenetv3_small_100", "conv_stem")

print("Feature map visualization completed.")

"""–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–Ω–Ω–∏—Ö —Å–ª–æ—ë–≤ —Å–µ—Ç–∏.

–î–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π –≤–Ω—É—Ç—Ä–∏ —Å–µ—Ç–∏ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è feature maps (–∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) –∏–∑ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–ª–æ—ë–≤ ResNet50 –∏ MobileNetV3.
–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª—å —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É—á–∞—Å—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∫–∞–∫–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –æ–Ω–∞ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2.4
#  Feature space projection (t-SNE / PCA)
# ================================================================

import torch, timm, numpy as np, matplotlib.pyplot as plt
from torchvision import transforms
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
CLASSES = ["real", "drawing", "generated"]

# –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è" —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
np.random.seed(42)
imgs = []
labels = []
for i, c in enumerate(CLASSES):
    for _ in range(20):
        arr = np.uint8(np.random.rand(IMG_SIZE, IMG_SIZE, 3)*255)
        imgs.append(arr)
        labels.append(c)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

x = torch.stack([tfm(Image.fromarray(img)) for img in imgs]).to(DEVICE)
model = timm.create_model("mobilenetv3_small_100", pretrained=False, num_classes=3).to(DEVICE)
model.eval()

# –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
with torch.no_grad():
    feats = model.forward_features(x).mean(dim=[2,3]).cpu().numpy()

# –°–Ω–∏–∂–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
pca = PCA(n_components=30).fit_transform(feats)
tsne = TSNE(n_components=2, random_state=42, init="pca").fit_transform(pca)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(6,5))
for c in np.unique(labels):
    idx = [i for i, lbl in enumerate(labels) if lbl == c]
    plt.scatter(tsne[idx,0], tsne[idx,1], label=c, alpha=0.7)
plt.legend()
plt.title("Feature Space Projection (MobileNetV3, synthetic data)")
plt.xlabel("t-SNE Dim 1"); plt.ylabel("t-SNE Dim 2")
plt.tight_layout()
plt.show()

print("Feature space visualization completed.")

"""–ü—Ä–æ–µ–∫—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ (PCA + t-SNE).

–í —ç—Ç–æ–º –±–ª–æ–∫–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∑–∞—Ç–µ–º —Å –ø–æ–º–æ—â—å—é PCA –∏ t-SNE –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–æ 2D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.
–†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ MobileNetV3 –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–∞—é—â–∏–µ—Å—è —Ç–∏–ø—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–∞–∂–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–º –Ω–∞–±–æ—Ä–µ.
"""

import zipfile
from pathlib import Path

RAW_ROOT = Path("/content/avatar_recog/data/raw")
zip_files = list(RAW_ROOT.glob("*.zip"))

print("Checking zip archives...\n")
for zfile in zip_files:
    try:
        with zipfile.ZipFile(zfile, 'r') as zf:
            n_files = len(zf.namelist())
            first = zf.namelist()[:3]
            print(f"{zfile.name}: OK | files inside: {n_files}")
            print("  Examples:", first)
    except zipfile.BadZipFile:
        print(f"{zfile.name}: Not a valid ZIP archive!")

import hashlib, os
from pathlib import Path

RAW_DIR = Path("/content/avatar_recog/data/raw")
hashes = {}
duplicates = []

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–æ –≤—Å–µ—Ö –ø–æ–¥–ø–∞–ø–∫–∞—Ö (–µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω—ã)
for img_path in RAW_DIR.rglob("*.[jp][pn]g"):
    try:
        with open(img_path, 'rb') as f:
            img_hash = hashlib.md5(f.read()).hexdigest()
        if img_hash in hashes:
            duplicates.append((img_path, hashes[img_hash]))
        else:
            hashes[img_hash] = img_path
    except Exception as e:
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏:", img_path, e)

print(f"\n–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}")
if duplicates:
    print("–ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
    for i, (a,b) in enumerate(duplicates[:5]):
        print(f"{i+1}) {a} <--> {b}")

import zipfile, io, random
from PIL import Image
import matplotlib.pyplot as plt
from pathlib import Path

RAW_ROOT = Path("/content/avatar_recog/data/raw")
zip_files = sorted(RAW_ROOT.glob("*.zip"))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞
plt.figure(figsize=(12, len(zip_files) * 2.5))

for row, zfile in enumerate(zip_files, start=1):
    with zipfile.ZipFile(zfile, 'r') as zf:
        img_names = [n for n in zf.namelist() if n.lower().endswith(('.jpg', '.jpeg', '.png'))]
        samples = random.sample(img_names, min(5, len(img_names)))
        for col, name in enumerate(samples, start=1):
            data = zf.read(name)
            img = Image.open(io.BytesIO(data)).convert("RGB")
            plt.subplot(len(zip_files), 5, (row - 1) * 5 + col)
            plt.imshow(img)
            plt.axis("off")
            if col == 3:  # –ø–æ–¥–ø–∏—Å—å –≤ —Ü–µ–Ω—Ç—Ä–µ —Ä—è–¥–∞
                plt.title(zfile.stem, fontsize=10)
plt.tight_layout()
plt.show()

# ================================================================
#  Avatar Type Recognition ‚Äî Block 3
#  Data Preparation: unzip, merge same classes, split train/val/test
# ================================================================

import zipfile, shutil, os, random
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

# === –ü–∞–ø–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ ===
WORK_ROOT = Path("/content/avatar_recog")
RAW_ROOT  = WORK_ROOT / "data" / "raw"
PROC_ROOT = WORK_ROOT / "data" / "processed"

# === –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ zip-–∞—Ä—Ö–∏–≤–æ–≤ ===
zip_files = sorted(RAW_ROOT.glob("*.zip"))
print("Found zip files:")
for z in zip_files:
    print(" -", z.name)

# === –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–æ–≤ ===
for zfile in tqdm(zip_files, desc="Unzipping archives"):
    out_dir = RAW_ROOT / zfile.stem
    out_dir.mkdir(exist_ok=True)
    with zipfile.ZipFile(zfile, 'r') as zip_ref:
        zip_ref.extractall(RAW_ROOT)

# === –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤ ===
# –í—Å–µ –∞—Ä—Ö–∏–≤—ã, –≥–¥–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ, –ø–æ–π–¥—É—Ç –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Å
name_map = {
    "real": "real",
    "drawn": "drawing",
    "ai": "generated"
}

# === –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—É—é processed-—Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Å–æ–∑–¥–∞—ë–º –∑–∞–Ω–æ–≤–æ ===
if PROC_ROOT.exists():
    shutil.rmtree(PROC_ROOT)
for split in ["train", "val", "test"]:
    for cls in ["real", "drawing", "generated"]:
        (PROC_ROOT / split / cls).mkdir(parents=True, exist_ok=True)

# === –°–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –≤—Å–µ—Ö –ø–æ–¥–ø–∞–ø–æ–∫ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º ===
def collect_images(folder):
    imgs = []
    for ext in ("*.jpg", "*.jpeg", "*.png", "*.webp"):
        imgs.extend(folder.rglob(ext))
    return imgs

# –°–æ–±–µ—Ä—ë–º –≤—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–∞–ø–∫–∏ –ø–æ—Å–ª–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
subdirs = [p for p in RAW_ROOT.iterdir() if p.is_dir()]

class_groups = {"real": [], "drawing": [], "generated": []}

for folder in subdirs:
    lower_name = folder.name.lower()
    for key, cls in name_map.items():
        if key in lower_name:
            imgs = collect_images(folder)
            class_groups[cls].extend(imgs)
            print(f" {folder.name} ‚Üí {cls} ({len(imgs)} images)")
            break

# === –î–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test ===
split_ratio = {"train": 0.8, "val": 0.1, "test": 0.1}
summary = []

for cls, imgs in class_groups.items():
    if len(imgs) == 0:
        print(f"No images found for class {cls}")
        continue
    random.shuffle(imgs)
    n = len(imgs)
    n_train = int(n * split_ratio["train"])
    n_val   = int(n * split_ratio["val"])
    n_test  = n - n_train - n_val
    splits = {
        "train": imgs[:n_train],
        "val": imgs[n_train:n_train+n_val],
        "test": imgs[n_train+n_val:]
    }
    for split, files in splits.items():
        out_dir = PROC_ROOT / split / cls
        for f in files:
            shutil.copy(f, out_dir / f.name)
    summary.append({"class": cls, "total": n, "train": n_train, "val": n_val, "test": n_test})

# === –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
df = pd.DataFrame(summary)
print("\n=== Dataset summary ===")
print(df.to_string(index=False))

# === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è ===
plt.figure(figsize=(6,4))
df_melt = df.melt(id_vars="class", value_vars=["train","val","test"],
                  var_name="split", value_name="count")
for cls in df["class"]:
    subset = df_melt[df_melt["class"] == cls]
    plt.bar(subset["split"] + " (" + cls + ")", subset["count"])
plt.title("Image distribution by class and split")
plt.ylabel("Number of images")
plt.xticks(rotation=20)
plt.tight_layout()
plt.show()

print("\n‚úÖ Block 3 completed successfully.")
print("All datasets unzipped, merged, and split into train/val/test.")

"""–†–∞—Å–ø–∞–∫–æ–≤–∫–∞, –æ—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.

–ó–¥–µ—Å—å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:
–∞—Ä—Ö–∏–≤—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞—é—Ç—Å—è, –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –ø–æ —Ç—Ä—ë–º —Ü–µ–ª–µ–≤—ã–º –∫–ª–∞—Å—Å–∞–º (real, drawing, generated), –ø–æ—Å–ª–µ —á–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–ª–∏—Ç—Å—è –Ω–∞ train / validation / test –≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ 80 / 10 / 10.
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ data/processed.
–¢–∞–∫–∂–µ –≤—ã–≤–æ–¥–∏—Ç—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Å –∏ —Å—Ç—Ä–æ–∏—Ç—Å—è –≥—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å –¥–∞–Ω–Ω—ã—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 4
#  DataLoader setup & visualization
# ================================================================

import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
import random

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH_SIZE = 32

DATA_DIR = "/content/avatar_recog/data/processed"

# === –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ===
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

val_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
train_ds = datasets.ImageFolder(root=f"{DATA_DIR}/train", transform=train_tfms)
val_ds   = datasets.ImageFolder(root=f"{DATA_DIR}/val", transform=val_tfms)
test_ds  = datasets.ImageFolder(root=f"{DATA_DIR}/test", transform=val_tfms)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

print(f"Train size: {len(train_ds)} | Val size: {len(val_ds)} | Test size: {len(test_ds)}")
print(f"Classes: {train_ds.classes}")

# === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ===
def imshow_batch(images, labels, classes):
    inv_norm = transforms.Normalize(
        mean=[-m/s for m, s in zip([0.485, 0.456, 0.406],
                                   [0.229, 0.224, 0.225])],
        std=[1/s for s in [0.229, 0.224, 0.225]]
    )
    imgs = inv_norm(images)
    imgs = imgs.numpy().transpose((0, 2, 3, 1))
    fig, axes = plt.subplots(2, 4, figsize=(10,5))
    for i, ax in enumerate(axes.flat):
        ax.imshow(np.clip(imgs[i], 0, 1))
        ax.set_title(classes[labels[i]])
        ax.axis("off")
    plt.tight_layout()
    plt.show()

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ ===
images, labels = next(iter(train_loader))
print("Batch shape:", images.shape)
print("Labels:", labels.tolist())

imshow_batch(images[:8], labels[:8], train_ds.classes)

print("\n‚úÖ Block 4 completed successfully ‚Äî DataLoaders are ready.")
print("Next: Block 5 ‚Äî Model training and fine-tuning.")

"""–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π.

–°–æ–∑–¥–∞—é—Ç—Å—è –æ–±—ä–µ–∫—Ç—ã DataLoader –¥–ª—è —Ç—Ä—ë—Ö –≤—ã–±–æ—Ä–æ–∫ (train, val, test) —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º–∏ (Resize, Flip, ColorJitter, Normalize).
–ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –æ–¥–∏–Ω –±–∞—Ç—á –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤.
–≠—Ç–æ—Ç –±–ª–æ–∫ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —á–∏—Ç–∞—é—Ç—Å—è, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –∏ –ø–æ–¥–∞—é—Ç—Å—è –≤ —Å–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ [batch, 3, 224, 224].
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 4.1
#  Dataset inspection: samples, class balance, DataLoader check
# ================================================================

import matplotlib.pyplot as plt
import numpy as np
from torchvision import datasets, transforms
from PIL import Image
from pathlib import Path
import random
import torch

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
DATA_DIR = "/content/avatar_recog/data/processed/train"
CLASSES = ['drawing', 'generated', 'real']
IMG_SIZE = 224

# === 1. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ ===
tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
])

plt.figure(figsize=(9, 6))
for i, cls in enumerate(CLASSES):
    paths = list((Path(DATA_DIR) / cls).glob("*.*"))
    samples = random.sample(paths, min(3, len(paths)))
    for j, img_path in enumerate(samples):
        img = Image.open(img_path).convert("RGB")
        plt.subplot(len(CLASSES), 3, i * 3 + j + 1)
        plt.imshow(img)
        plt.title(cls, fontsize=10)
        plt.axis("off")
plt.suptitle("Examples from each dataset class", fontsize=14)
plt.tight_layout()
plt.show()

print("‚úÖ Sample visualization completed.\n")

# === 2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ ===
counts = [59275, 6355, 6738]
labels = ['Drawing', 'Generated', 'Real']

plt.figure(figsize=(5, 5))
plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90,
        colors=['orange', 'green', 'steelblue'])
plt.title("Dataset Class Distribution")
plt.show()

print("‚úÖ Class balance visualization completed.\n")

# === 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ DataLoader ===
BATCH_SIZE = 32
val_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

val_ds = datasets.ImageFolder(root="/content/avatar_recog/data/processed/train", transform=val_tfms)
val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)

batch = next(iter(val_loader))
images, labels = batch

print("=== DataLoader sanity check ===")
print(f"Batch shape: {images.shape}")
print(f"Labels (first 10): {labels[:10].tolist()}")
print(f"Classes: {val_ds.classes}")

print("\n‚úÖ Block 4.1 completed successfully ‚Äî dataset verified and ready for training.")

"""–ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤, –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–∞—Ç—á–µ–π.

–û—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞, —Å—Ç—Ä–æ–∏—Ç—Å—è pie-–¥–∏–∞–≥—Ä–∞–º–º–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞.
–¢–∞–∫–∂–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö: —Ä–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–µ–π, –º–µ—Ç–∫–∏, –ø–æ—Ä—è–¥–æ–∫ –∫–ª–∞—Å—Å–æ–≤.
–≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —É–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –∫–ª–∞—Å—Å–æ–≤ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—É—á–µ–Ω–∏—è.
"""

import torch
print(torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 5
#  Training & fine-tuning: MobileNetV3 and ResNet50
# ================================================================

import os, time, json, math, random
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import datasets, transforms
import timm

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

# ---------- 0. Paths, device, params ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")  # –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω
MODELS_DIR = (DRIVE_ROOT if DRIVE_ROOT.exists() else WORK_ROOT) / "models"
OUT_DIR    = (DRIVE_ROOT if DRIVE_ROOT.exists() else WORK_ROOT) / "outputs"
for p in [MODELS_DIR, OUT_DIR]:
    p.mkdir(parents=True, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3
BATCH = 64 if DEVICE == "cuda" else 32
EPOCHS_MNV3 = 6
EPOCHS_RN50 = 8
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)

print(f"Device: {DEVICE} | Batch: {BATCH}")
print(f"Saving models to: {MODELS_DIR}")
print(f"Saving plots/logs to: {OUT_DIR}")

# ---------- 1. Transforms ----------
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

# ---------- 2. Datasets ----------
train_ds = datasets.ImageFolder(str(DATA_DIR / "train"), transform=train_tfms)
val_ds   = datasets.ImageFolder(str(DATA_DIR / "val"),   transform=eval_tfms)
test_ds  = datasets.ImageFolder(str(DATA_DIR / "test"),  transform=eval_tfms)
class_names = train_ds.classes
print("Classes:", class_names)

# ---------- 3. Handle class imbalance ----------
# Weights ~ 1/freq for WeightedRandomSampler
targets = np.array(train_ds.targets)
counts = np.bincount(targets, minlength=NUM_CLASSES)
class_weights = (1.0 / np.maximum(counts, 1)).astype(np.float32)
sample_weights = class_weights[targets]
sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)

print("Train counts per class:", {cls:int(c) for cls,c in zip(class_names, counts)})
print("Class weights:", {cls:round(w,5) for cls,w in zip(class_names, class_weights)})

# ---------- 4. DataLoaders ----------
train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler,
                          num_workers=2, pin_memory=(DEVICE=="cuda"))
val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,
                          num_workers=2, pin_memory=(DEVICE=="cuda"))
test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False,
                          num_workers=2, pin_memory=(DEVICE=="cuda"))

# ---------- 5. Utilities ----------
def build_model(model_name: str):
    model = timm.create_model(model_name, pretrained=True, num_classes=NUM_CLASSES)
    return model.to(DEVICE)

def epoch_run(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train() if train_mode else model.eval()
    losses, all_preds, all_targs = [], [], []
    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=="cuda"))

    for x, y in loader:
        x, y = x.to(DEVICE), y.to(DEVICE)
        with torch.cuda.amp.autocast(enabled=(DEVICE=="cuda")):
            logits = model(x)
            loss = criterion(logits, y)

        if train_mode:
            optimizer.zero_grad(set_to_none=True)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

        losses.append(loss.item())
        all_preds.extend(torch.argmax(logits, 1).detach().cpu().numpy())
        all_targs.extend(y.detach().cpu().numpy())

    acc = accuracy_score(all_targs, all_preds)
    f1m = f1_score(all_targs, all_preds, average="macro", zero_division=0)
    return float(np.mean(losses)), acc, f1m

def train_model(model_name: str, epochs: int):
    model = build_model(model_name)
    # Weighted CE to reflect imbalance (same weights as sampler)
    ce_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)
    criterion = nn.CrossEntropyLoss(weight=ce_weights)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-2)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", factor=0.5, patience=1
)


    history = {"epoch": [], "train_loss": [], "train_acc": [], "train_f1": [],
               "val_loss": [], "val_acc": [], "val_f1": []}
    best_val_f1, best_path = -1.0, MODELS_DIR / f"{model_name}_best.pth"
    patience, wait = 3, 0  # early stopping

    for epoch in range(1, epochs+1):
        t0 = time.time()
        tr = epoch_run(model, train_loader, criterion, optimizer)
        va = epoch_run(model, val_loader,   criterion, optimizer=None)
        scheduler.step(va[0])

        history["epoch"].append(epoch)
        history["train_loss"].append(tr[0]); history["train_acc"].append(tr[1]); history["train_f1"].append(tr[2])
        history["val_loss"].append(va[0]);   history["val_acc"].append(va[1]);   history["val_f1"].append(va[2])

        msg = (f"[{model_name}] Epoch {epoch:02d}/{epochs} | "
               f"loss {tr[0]:.4f} acc {tr[1]:.3f} f1 {tr[2]:.3f} | "
               f"val_loss {va[0]:.4f} val_acc {va[1]:.3f} val_f1 {va[2]:.3f} | "
               f"time {time.time()-t0:.1f}s")
        print(msg)

        # checkpoint on best val_f1
        if va[2] > best_val_f1:
            best_val_f1 = va[2]; wait = 0
            torch.save(model.state_dict(), best_path)
        else:
            wait += 1
            if wait >= patience:
                print(f"Early stopping on epoch {epoch}. Best val_f1={best_val_f1:.3f}")
                break

    # save history
    hist_df = pd.DataFrame(history)
    hist_csv = OUT_DIR / f"history_{model_name}.csv"
    hist_df.to_csv(hist_csv, index=False)

    # plots
    fig, ax = plt.subplots(1,2, figsize=(10,4))
    ax[0].plot(hist_df["epoch"], hist_df["train_loss"], label="train")
    ax[0].plot(hist_df["epoch"], hist_df["val_loss"], label="val")
    ax[0].set_title(f"{model_name} ‚Äî loss"); ax[0].legend()

    ax[1].plot(hist_df["epoch"], hist_df["train_f1"], label="train F1")
    ax[1].plot(hist_df["epoch"], hist_df["val_f1"], label="val F1")
    ax[1].set_title(f"{model_name} ‚Äî F1"); ax[1].legend()
    plt.tight_layout()
    plt.show()

    print(f"Best weights saved to: {best_path}")
    print(f"Training log saved to: {hist_csv}")
    return best_path

def evaluate(model_name: str, weights_path: Path):
    model = build_model(model_name)
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model.eval()

    y_true, y_pred = [], []
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(DEVICE)
            logits = model(x)
            y_true.extend(y.numpy())
            y_pred.extend(torch.argmax(logits, 1).cpu().numpy())

    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)
    cm = confusion_matrix(y_true, y_pred)

    print("\n=== Test report ===")
    print(report)

    # save artifacts
    rpt_path = OUT_DIR / f"test_report_{model_name}.txt"
    with open(rpt_path, "w") as f:
        f.write(report)
    plt.figure(figsize=(5,4))
    plt.imshow(cm, cmap="Blues")
    plt.xticks(range(NUM_CLASSES), class_names, rotation=45)
    plt.yticks(range(NUM_CLASSES), class_names)
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.title(f"Confusion matrix ‚Äî {model_name}")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES):
            plt.text(j, i, cm[i, j], ha="center", va="center", fontsize=9)
    plt.tight_layout()
    cm_path = OUT_DIR / f"cm_{model_name}.png"
    plt.savefig(cm_path); plt.show()

    print(f"Saved: {rpt_path}")
    print(f"Saved: {cm_path}")

# ---------- 6. Train MobileNetV3 ----------
mnv3_best = train_model("mobilenetv3_small_100", epochs=EPOCHS_MNV3)
evaluate("mobilenetv3_small_100", mnv3_best)

# ---------- 7. Train ResNet50 ----------
rn50_best = train_model("resnet50", epochs=EPOCHS_RN50)
evaluate("resnet50", rn50_best)

print("\n‚úÖ Block 5 completed.")

"""–û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –¥–≤—É—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

–ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ–±—É—á–µ–Ω–∏–µ MobileNetV3 –∏ ResNet50 –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.
–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ—Ö–Ω–∏–∫–∏:

WeightedRandomSampler –∏ –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤;

AdamW-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ ReduceLROnPlateau –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è;

Early Stopping –ø–æ –º–µ—Ç—Ä–∏–∫–µ F1.
–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç—Å—è, —Å—Ç—Ä–æ—è—Ç—Å—è confusion matrix –∏ –æ—Ç—á—ë—Ç—ã —Å precision, recall, F1 –∏ accuracy.
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ /outputs –∏ /models.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 5-Lite
#  EfficientNet-B0 (Frozen Backbone) & ConvNeXt-Tiny (Progressive Unfreeze)
# ================================================================

import os, time, random
from pathlib import Path
import torch, timm
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

# ---------- 0. Paths ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"; MODELS_DIR.mkdir(exist_ok=True, parents=True)
OUT_DIR    = DRIVE_ROOT / "outputs"; OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3
BATCH = 48 if DEVICE=="cuda" else 16
EPOCHS_EFF = 3   # fast test run
EPOCHS_CNV = 4
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

print(f"Device: {DEVICE} | Batch: {BATCH}")
print("Folders:", MODELS_DIR, OUT_DIR)

# ---------- 1. Transforms ----------
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.ColorJitter(0.2,0.2,0.2,0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

# ---------- 2. Dataset (subset for speed) ----------
train_full = datasets.ImageFolder(str(DATA_DIR/"train"), transform=train_tfms)
val_full   = datasets.ImageFolder(str(DATA_DIR/"val"), transform=eval_tfms)
test_ds    = datasets.ImageFolder(str(DATA_DIR/"test"), transform=eval_tfms)
class_names = train_full.classes

# –≤–æ–∑—å–º–µ–º —Ç–æ–ª—å–∫–æ 25% train –∏ 40% val –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
subset_train = Subset(train_full, np.random.choice(len(train_full), len(train_full)//4, replace=False))
subset_val   = Subset(val_full,   np.random.choice(len(val_full),   len(val_full)//3,   replace=False))

train_loader = DataLoader(subset_train, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)
val_loader   = DataLoader(subset_val,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_ds,      batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)

# ---------- 3. Training helpers ----------
def epoch_run(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train() if train_mode else model.eval()
    losses, all_preds, all_targs = [], [], []
    with torch.set_grad_enabled(train_mode):
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            loss = criterion(logits, y)
            if train_mode:
                optimizer.zero_grad(); loss.backward(); optimizer.step()
            losses.append(loss.item())
            all_preds.extend(logits.argmax(1).cpu().numpy())
            all_targs.extend(y.cpu().numpy())
    return np.mean(losses), accuracy_score(all_targs, all_preds), f1_score(all_targs, all_preds, average="macro")

def train_model(model, name, epochs):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4)
    history = {"epoch": [], "train_loss": [], "val_loss": [], "train_f1": [], "val_f1": []}
    best_f1, best_path = -1, MODELS_DIR/f"{name}_best.pth"

    for ep in range(1, epochs+1):
        t0=time.time()
        tr = epoch_run(model, train_loader, criterion, optimizer)
        va = epoch_run(model, val_loader, criterion)
        history["epoch"].append(ep)
        for k,v in zip(["train_loss","train_f1","val_loss","val_f1"],[tr[0],tr[2],va[0],va[2]]): history[k].append(v)
        print(f"[{name}] Epoch {ep}/{epochs} | loss {tr[0]:.3f}/{va[0]:.3f} | F1 {tr[2]:.3f}/{va[2]:.3f} | {time.time()-t0:.1f}s")
        if va[2]>best_f1:
            best_f1=va[2]; torch.save(model.state_dict(), best_path)
    pd.DataFrame(history).to_csv(OUT_DIR/f"history_{name}.csv", index=False)
    print("Saved best:", best_path)
    return best_path

def evaluate(model, name, weights):
    model.load_state_dict(torch.load(weights, map_location=DEVICE))
    model.eval()
    y_true,y_pred=[],[]
    with torch.no_grad():
        for x,y in test_loader:
            x=x.to(DEVICE)
            y_true.extend(y.numpy())
            y_pred.extend(model(x).argmax(1).cpu().numpy())
    rpt = classification_report(y_true,y_pred,target_names=class_names,zero_division=0)
    print(f"\n=== Test report ({name}) ===\n{rpt}")
    cm = confusion_matrix(y_true,y_pred)
    plt.imshow(cm,cmap="Blues"); plt.title(f"Confusion ‚Äî {name}"); plt.xlabel("Pred"); plt.ylabel("True")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES): plt.text(j,i,cm[i,j],ha="center",va="center")
    plt.tight_layout(); plt.show()

# ---------- 4. EfficientNet-B0 ‚Äî Frozen Backbone ----------
print("\n EfficientNet-B0 ‚Äî frozen backbone")
eff = timm.create_model("efficientnet_b0", pretrained=True, num_classes=NUM_CLASSES)
for p in eff.parameters(): p.requires_grad=False
for p in eff.get_classifier().parameters(): p.requires_grad=True
eff = eff.to(DEVICE)
eff_best = train_model(eff, "efficientnet_b0_frozen", EPOCHS_EFF)
evaluate(eff, "efficientnet_b0_frozen", eff_best)

# ---------- 5. ConvNeXt-Tiny ‚Äî Progressive Unfreeze ----------
print("\n ConvNeXt-Tiny ‚Äî progressive unfreeze")
cnv = timm.create_model("convnext_tiny", pretrained=True, num_classes=NUM_CLASSES)
for p in cnv.parameters(): p.requires_grad=False
for p in cnv.head.parameters(): p.requires_grad=True
cnv = cnv.to(DEVICE)

# --- Stage 1: train only head ---
print("Stage 1 ‚Äî frozen backbone (head only)")
head_best = train_model(cnv, "convnext_tiny_stage1", 2)

# --- Stage 2: unfreeze last 2 blocks and continue fine-tuning ---
print("Stage 2 ‚Äî partial unfreeze (last 2 blocks + head)")
for name,param in cnv.named_parameters():
    if any(b in name for b in ["stages.2","stages.3","head"]):
        param.requires_grad=True
cnv_best = train_model(cnv, "convnext_tiny_stage2", EPOCHS_CNV)
evaluate(cnv, "convnext_tiny_stage2", cnv_best)

print("\n‚úÖ Block 5-Lite completed successfully.")

"""–£–ø—Ä–æ—â—ë–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏.

–ü—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤:

EfficientNet-B0 —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º –±—ç–∫–±–æ–Ω–æ–º (fine-tuning —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ–≤—ã).

ConvNeXt-Tiny —Å progressive unfreeze ‚Äî –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–º —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ–º —Å–ª–æ—ë–≤.
–ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è, –∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–ª–∏—è–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 5-FewShot
#  Training MobileNetV3 (250/class) and ResNet18 (50/class)
# ================================================================

import random, time
from pathlib import Path
import torch, timm
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

# ---------- 0. Paths ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"; MODELS_DIR.mkdir(exist_ok=True, parents=True)
OUT_DIR    = DRIVE_ROOT / "outputs"; OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3
BATCH = 32 if DEVICE == "cuda" else 16
EPOCHS = 4
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
print(f"Device: {DEVICE} | Batch: {BATCH}")

# ---------- 1. Transforms ----------
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.ColorJitter(0.2,0.2,0.2,0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

# ---------- 2. Dataset ----------
train_full = datasets.ImageFolder(str(DATA_DIR/"train"), transform=train_tfms)
val_full   = datasets.ImageFolder(str(DATA_DIR/"val"), transform=eval_tfms)
test_ds    = datasets.ImageFolder(str(DATA_DIR/"test"), transform=eval_tfms)
class_names = train_full.classes

# ---------- 3. Utility: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ N –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å ----------
def subset_by_class(dataset, n_per_class):
    cls_indices = {i: [] for i in range(len(dataset.classes))}
    for idx, (_, label) in enumerate(dataset.samples):
        if len(cls_indices[label]) < n_per_class:
            cls_indices[label].append(idx)
    selected = [i for lst in cls_indices.values() for i in lst]
    random.shuffle(selected)
    return Subset(dataset, selected)

# –°–æ–∑–¥–∞—ë–º –ø–æ–¥–Ω–∞–±–æ—Ä—ã
train_250 = subset_by_class(train_full, 250)   # MobileNetV3
train_50  = subset_by_class(train_full, 50)    # ResNet18

train_loader_250 = DataLoader(train_250, batch_size=BATCH, shuffle=True, num_workers=2)
train_loader_50  = DataLoader(train_50,  batch_size=BATCH, shuffle=True, num_workers=2)
val_loader       = DataLoader(val_full,  batch_size=BATCH, shuffle=False, num_workers=2)
test_loader      = DataLoader(test_ds,   batch_size=BATCH, shuffle=False, num_workers=2)

# ---------- 4. Train & Eval ----------
def epoch_run(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train() if train_mode else model.eval()
    losses, all_preds, all_targs = [], [], []
    with torch.set_grad_enabled(train_mode):
        for x,y in loader:
            x,y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            loss = criterion(logits,y)
            if train_mode:
                optimizer.zero_grad(); loss.backward(); optimizer.step()
            losses.append(loss.item())
            all_preds.extend(logits.argmax(1).cpu().numpy())
            all_targs.extend(y.cpu().numpy())
    return np.mean(losses), f1_score(all_targs, all_preds, average="macro"), accuracy_score(all_targs, all_preds)

def train_model(model_name, loader, epochs):
    model = timm.create_model(model_name, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)
    best_f1, best_path = -1, MODELS_DIR/f"{model_name}_fewshot_best.pth"
    history = {"epoch":[],"train_loss":[],"val_loss":[],"train_f1":[],"val_f1":[]}
    for ep in range(1,epochs+1):
        t0=time.time()
        tr = epoch_run(model, loader, criterion, optimizer)
        va = epoch_run(model, val_loader, criterion)
        history["epoch"].append(ep)
        history["train_loss"].append(tr[0]); history["val_loss"].append(va[0])
        history["train_f1"].append(tr[1]); history["val_f1"].append(va[1])
        print(f"[{model_name}] Epoch {ep}/{epochs} | loss {tr[0]:.3f}/{va[0]:.3f} | F1 {tr[1]:.3f}/{va[1]:.3f} | {time.time()-t0:.1f}s")
        if va[1] > best_f1:
            best_f1 = va[1]; torch.save(model.state_dict(), best_path)
    pd.DataFrame(history).to_csv(OUT_DIR/f"history_{model_name}_fewshot.csv", index=False)
    print("Saved best:", best_path)
    return model, best_path

def evaluate(model, model_name, weights_path):
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model.eval()
    y_true,y_pred=[],[]
    with torch.no_grad():
        for x,y in test_loader:
            x=x.to(DEVICE)
            y_true.extend(y.numpy())
            y_pred.extend(model(x).argmax(1).cpu().numpy())
    rpt = classification_report(y_true,y_pred,target_names=class_names,zero_division=0)
    print(f"\n=== Test report ({model_name}) ===\n{rpt}")
    cm = confusion_matrix(y_true,y_pred)
    plt.imshow(cm,cmap="Blues"); plt.title(f"Confusion ‚Äî {model_name}"); plt.xlabel("Pred"); plt.ylabel("True")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES): plt.text(j,i,cm[i,j],ha="center",va="center")
    plt.tight_layout(); plt.show()

# ---------- 5. MobileNetV3 (250/class) ----------
print("\n MobileNetV3 ‚Äî Few-Shot 250/class")
mnet, mnet_best = train_model("mobilenetv3_small_100", train_loader_250, EPOCHS)
evaluate(mnet, "mobilenetv3_small_100_fewshot", mnet_best)

# ---------- 6. ResNet18 (50/class) ----------
print("\n ResNet18 ‚Äî Few-Shot 50/class")
resnet, resnet_best = train_model("resnet18", train_loader_50, EPOCHS)
evaluate(resnet, "resnet18_fewshot", resnet_best)

print("\n‚úÖ Block 5-FewShot completed successfully.")

"""–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π.

–ú–æ–¥–µ–ª–∏ MobileNetV3 (250 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å) –∏ ResNet18 (50 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å) –æ–±—É—á–∞—é—Ç—Å—è –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–¶–µ–ª—å ‚Äî –æ—Ü–µ–Ω–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –ø—Ä–∏ –º–∞–ª–æ–º –æ–±—ä—ë–º–µ –≤—ã–±–æ—Ä–∫–∏ (Few-Shot Learning).
–°–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—à–∏–±–æ–∫ –Ω–∞ —Ç–µ—Å—Ç–µ.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 5-FewShot (12 Epochs Extended)
#  Training MobileNetV3 (250/class) and ResNet18 (50/class) ‚Äî 12 Epochs
# ================================================================

import random, time
from pathlib import Path
import torch, timm
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

# ---------- 0. Paths ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"; MODELS_DIR.mkdir(exist_ok=True, parents=True)
OUT_DIR    = DRIVE_ROOT / "outputs"; OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3
BATCH = 32 if DEVICE == "cuda" else 16
EPOCHS = 12   # ‚¨ÖÔ∏è –£–≤–µ–ª–∏—á–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
print(f"Device: {DEVICE} | Batch: {BATCH}")

# ---------- 1. Transforms ----------
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.ColorJitter(0.2,0.2,0.2,0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

# ---------- 2. Dataset ----------
train_full = datasets.ImageFolder(str(DATA_DIR/"train"), transform=train_tfms)
val_full   = datasets.ImageFolder(str(DATA_DIR/"val"), transform=eval_tfms)
test_ds    = datasets.ImageFolder(str(DATA_DIR/"test"), transform=eval_tfms)
class_names = train_full.classes

# ---------- 3. Utility: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ N –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å ----------
def subset_by_class(dataset, n_per_class):
    cls_indices = {i: [] for i in range(len(dataset.classes))}
    for idx, (_, label) in enumerate(dataset.samples):
        if len(cls_indices[label]) < n_per_class:
            cls_indices[label].append(idx)
    selected = [i for lst in cls_indices.values() for i in lst]
    random.shuffle(selected)
    return Subset(dataset, selected)

train_250 = subset_by_class(train_full, 250)   # MobileNetV3
train_50  = subset_by_class(train_full, 50)    # ResNet18

train_loader_250 = DataLoader(train_250, batch_size=BATCH, shuffle=True, num_workers=2)
train_loader_50  = DataLoader(train_50,  batch_size=BATCH, shuffle=True, num_workers=2)
val_loader       = DataLoader(val_full,  batch_size=BATCH, shuffle=False, num_workers=2)
test_loader      = DataLoader(test_ds,   batch_size=BATCH, shuffle=False, num_workers=2)

# ---------- 4. Train & Eval ----------
def epoch_run(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train() if train_mode else model.eval()
    losses, all_preds, all_targs = [], [], []
    with torch.set_grad_enabled(train_mode):
        for x,y in loader:
            x,y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            loss = criterion(logits,y)
            if train_mode:
                optimizer.zero_grad(); loss.backward(); optimizer.step()
            losses.append(loss.item())
            all_preds.extend(logits.argmax(1).cpu().numpy())
            all_targs.extend(y.cpu().numpy())
    return np.mean(losses), f1_score(all_targs, all_preds, average="macro"), accuracy_score(all_targs, all_preds)

def train_model(model_name, loader, epochs):
    model = timm.create_model(model_name, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)
    best_f1, best_path = -1, MODELS_DIR/f"{model_name}_fewshot_12ep_best.pth"  # ‚¨ÖÔ∏è –¥–æ–±–∞–≤–∏–ª–∏ —Å—É—Ñ—Ñ–∏–∫—Å 12ep
    history = {"epoch":[],"train_loss":[],"val_loss":[],"train_f1":[],"val_f1":[]}
    for ep in range(1,epochs+1):
        t0=time.time()
        tr = epoch_run(model, loader, criterion, optimizer)
        va = epoch_run(model, val_loader, criterion)
        history["epoch"].append(ep)
        history["train_loss"].append(tr[0]); history["val_loss"].append(va[0])
        history["train_f1"].append(tr[1]); history["val_f1"].append(va[1])
        print(f"[{model_name}] Epoch {ep}/{epochs} | loss {tr[0]:.3f}/{va[0]:.3f} | F1 {tr[1]:.3f}/{va[1]:.3f} | {time.time()-t0:.1f}s")
        if va[1] > best_f1:
            best_f1 = va[1]; torch.save(model.state_dict(), best_path)
    pd.DataFrame(history).to_csv(OUT_DIR/f"history_{model_name}_fewshot_12ep.csv", index=False)
    print("Saved best:", best_path)
    return model, best_path

def evaluate(model, model_name, weights_path):
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model.eval()
    y_true,y_pred=[],[]
    with torch.no_grad():
        for x,y in test_loader:
            x=x.to(DEVICE)
            y_true.extend(y.numpy())
            y_pred.extend(model(x).argmax(1).cpu().numpy())
    rpt = classification_report(y_true,y_pred,target_names=class_names,zero_division=0)
    print(f"\n=== Test report ({model_name}) ===\n{rpt}")
    cm = confusion_matrix(y_true,y_pred)
    plt.imshow(cm,cmap="Blues"); plt.title(f"Confusion ‚Äî {model_name}"); plt.xlabel("Pred"); plt.ylabel("True")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES): plt.text(j,i,cm[i,j],ha="center",va="center")
    plt.tight_layout(); plt.show()

# ---------- 5. MobileNetV3 (250/class, 12 epochs) ----------
print("\n MobileNetV3 ‚Äî Few-Shot 250/class ‚Äî 12 Epochs")
mnet, mnet_best = train_model("mobilenetv3_small_100", train_loader_250, EPOCHS)
evaluate(mnet, "mobilenetv3_small_100_fewshot_12ep", mnet_best)

# ---------- 6. ResNet18 (50/class, 12 epochs) ----------
print("\n ResNet18 ‚Äî Few-Shot 50/class ‚Äî 12 Epochs")
resnet, resnet_best = train_model("resnet18", train_loader_50, EPOCHS)
evaluate(resnet, "resnet18_fewshot_12ep", resnet_best)

print("\n‚úÖ Block 5-FewShot (12 Epochs) completed successfully.")

"""–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ MobileNetV3 –∏ ResNet18 –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö (Few-Shot Learning).

–í —ç—Ç–æ–º –±–ª–æ–∫–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø–æ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –Ω–∞ –º–∞–ª—ã—Ö –æ–±—ä—ë–º–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É Few-Shot-—Ç–µ—Å—Ç—É, –Ω–æ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —á–∏—Å–ª–æ–º —ç–ø–æ—Ö (–¥–æ 12), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å, –∫–∞–∫ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –æ–±–æ–±—â–∞—Ç—å –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–°–æ–∑–¥–∞—é—Ç—Å—è –ø–æ–¥–Ω–∞–±–æ—Ä—ã –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:

–ø–æ 250 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å –¥–ª—è MobileNetV3,

–ø–æ 50 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å –¥–ª—è ResNet18.

–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (Resize, HorizontalFlip, ColorJitter, Normalize) –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º AdamW –∏ —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å CrossEntropyLoss.
–ú–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ —Å –ø–æ–º–µ—Ç–∫–æ–π _12ep, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã.

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫:

F1-score ‚Äî –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã,

Accuracy ‚Äî –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π,

Confusion Matrix ‚Äî –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º.

–î–∞–Ω–Ω—ã–π –±–ª–æ–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —ç–ø–æ—Ö –≤–ª–∏—è–µ—Ç –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —É—Å–ª–æ–≤–∏—è—Ö –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∏ –æ–±—É—á–∞–µ–º–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Generate Missing Test Reports (v2)
#  Evaluates all saved .pth models (no retraining)
# ================================================================

from pathlib import Path
import torch, timm
from torchvision import datasets, transforms
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

# ---------- 0. Paths ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs"
OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3

# ---------- 1. –î–∞—Ç–∞—Å–µ—Ç—ã ----------
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
test_ds = datasets.ImageFolder(str(DATA_DIR/"test"), transform=eval_tfms)
test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)
class_names = test_ds.classes
print("Classes:", class_names)
print("Device:", DEVICE)

# ---------- 2. —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ ----------
def evaluate_pretrained(model_name, weights_path, alias):
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    y_true, y_pred = [], []
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(DEVICE)
            logits = model(x)
            y_true.extend(y.numpy())
            y_pred.extend(torch.argmax(logits, 1).cpu().numpy())

    # --- –º–µ—Ç—Ä–∏–∫–∏ ---
    rpt = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)
    cm = confusion_matrix(y_true, y_pred)

    # --- —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç ---
    rpt_path = OUT_DIR / f"test_report_{alias}.txt"
    with open(rpt_path, "w") as f:
        f.write(rpt)
    print(f"‚úÖ Saved report: {rpt_path.name}")

    # --- —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω—É–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ---
    plt.figure(figsize=(5,4))
    plt.imshow(cm, cmap="Blues")
    plt.xticks(range(NUM_CLASSES), class_names, rotation=45)
    plt.yticks(range(NUM_CLASSES), class_names)
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.title(f"Confusion matrix ‚Äî {alias}")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES):
            plt.text(j, i, cm[i,j], ha="center", va="center", fontsize=9)
    plt.tight_layout()
    plt.savefig(OUT_DIR / f"cm_{alias}.png", dpi=150)
    plt.close()

# ---------- 3. Model List ----------
models_to_eval = [
    # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "mobilenetv3_small_100"),
    ("resnet50", "resnet50_best.pth", "resnet50"),

    # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "efficientnet_b0_frozen"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "convnext_tiny_stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "convnext_tiny_stage2"),

    # Few-Shot 4 —ç–ø–æ—Ö–∏
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "mobilenetv3_small_100_fewshot"),
    ("resnet18", "resnet18_fewshot_best.pth", "resnet18_fewshot"),

    # Few-Shot 12 —ç–ø–æ—Ö (–Ω–æ–≤—ã–µ)
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "mobilenetv3_small_100_fewshot_12ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "resnet18_fewshot_12ep"),
]

# ---------- 4. –ø—Ä–æ–≥–æ–Ω –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π ----------
for model_name, fname, alias in models_to_eval:
    path = MODELS_DIR / fname
    if path.exists():
        print(f"\n Evaluating {alias} ...")
        evaluate_pretrained(model_name, path, alias)
    else:
        print(f" Weight file not found: {fname}")

print("\n‚úÖ –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ –æ—Ç—á—ë—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ /outputs/")

"""–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

–ë–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π (*.pth), –∏—Ö —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ—Ç—á—ë—Ç–æ–≤ (test_report_*.txt) –∏ –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫ (cm_*.png).
–≠—Ç–æ—Ç –±–ª–æ–∫ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª–Ω—ã—Ö –∏ –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤—Å–µ—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 6 (Final Visualization + Pie)
#  9 –º–æ–¥–µ–ª–µ–π, —Ç–∞–±–ª–∏—Ü–∞, heatmap –∏ –∫—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å F1-score
# ================================================================

import re, os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from difflib import get_close_matches
import matplotlib.patches as mpatches

OUT_DIR = Path("/content/drive/MyDrive/avatar_recog/outputs")
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –ú–æ–¥–µ–ª–∏ –∏ —Ç–∏–ø—ã –æ–±—É—á–µ–Ω–∏—è ----------
models_info = [
    ("mobilenetv3_small_100", "MobileNetV3", "Full Fine-Tuning"),
    ("resnet50", "ResNet50", "Full Fine-Tuning"),
    ("efficientnet_b0_frozen", "EfficientNet-B0", "Frozen Backbone"),
    ("convnext_tiny_stage1", "ConvNeXt-Tiny", "Stage 1 (Head Only)"),
    ("convnext_tiny_stage2", "ConvNeXt-Tiny", "Progressive Unfreeze"),
    ("mobilenetv3_small_100_fewshot", "MobileNetV3", "Few-Shot (250/class, 4ep)"),
    ("resnet18_fewshot", "ResNet18", "Few-Shot (50/class, 4ep)"),
    ("mobilenetv3_small_100_fewshot_12ep", "MobileNetV3", "Few-Shot (250/class, 12ep)"),
    ("resnet18_fewshot_12ep", "ResNet18", "Few-Shot (50/class, 12ep)")
]

# ---------- 2. –ü–∞—Ä—Å–µ—Ä –æ—Ç—á—ë—Ç–∞ ----------
def parse_report(path):
    with open(path, 'r') as f:
        txt = f.read()
    prec, rec, f1 = re.findall(r"macro avg\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)", txt)[0]
    acc = re.findall(r"accuracy\s+([\d.]+)", txt)[0]
    return float(prec), float(rec), float(f1), float(acc)

# ---------- 3. –ì–∏–±–∫–∏–π –ø–æ–∏—Å–∫ –æ—Ç—á—ë—Ç–∞ ----------
def find_report_file(base_name):
    files = [f.name for f in OUT_DIR.glob("test_report_*.txt")]
    matches = get_close_matches(f"test_report_{base_name}.txt", files, n=1, cutoff=0.5)
    if matches:
        return OUT_DIR / matches[0]
    return None

# ---------- 4. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ----------
rows = []
for key, name, kind in models_info:
    rpt_path = find_report_file(key)
    if rpt_path and rpt_path.exists():
        prec, rec, f1, acc = parse_report(rpt_path)
        rows.append({
            "Model": name,
            "Training": kind,
            "Label": f"{name}\n({kind.split()[0]})",
            "Precision": prec,
            "Recall": rec,
            "F1": f1,
            "Accuracy": acc
        })
    else:
        print(f" Report not found for {key}")

df = pd.DataFrame(rows)
df = df.sort_values("F1", ascending=False).reset_index(drop=True)

# ---------- 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É ----------
table_path = OUT_DIR / "final_models_comparison_table.csv"
df.to_csv(table_path, index=False)
print(f"\n–¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {table_path}")
print(df.to_string(index=False))

# ---------- 6. –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ ----------
fig, ax = plt.subplots(figsize=(max(8, len(df)*1.6), 4))
im = ax.imshow(df[["Precision","Recall","F1","Accuracy"]].T, cmap="YlGnBu", aspect="auto")

ax.set_yticks(range(4))
ax.set_yticklabels(["Precision","Recall","F1","Accuracy"])
ax.set_xticks(range(len(df)))
ax.set_xticklabels(df["Label"], rotation=15, ha="right", fontsize=9)
plt.title("Metric Heatmap by Model", fontsize=14, pad=10)
plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
plt.tight_layout()
plt.savefig(OUT_DIR / "final_metrics_heatmap.png", dpi=300)
plt.show()

# ---------- 7. –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ ----------
def color_for(train_type):
    train_type = train_type.lower()
    if "full" in train_type: return "#007aff"
    if "frozen" in train_type: return "#5ac8fa"
    if "progressive" in train_type: return "#34c759"
    if "stage" in train_type: return "#1dd1a1"
    if "few-shot" in train_type and "4ep" in train_type: return "#ff9500"
    if "few-shot" in train_type and "12ep" in train_type: return "#ffcc00"
    return "#8e8e93"

colors = [color_for(t) for t in df["Training"]]

# ---------- 8. –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ F1 ----------
fig, ax = plt.subplots(figsize=(9, 9))
wedges, texts = ax.pie(
    df["F1"],
    labels=df["Label"],
    colors=colors,
    startangle=120,
    labeldistance=1.1,
    wedgeprops={'edgecolor': 'white', 'linewidth': 1.5}
)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è F1 –≤–Ω—É—Ç—Ä—å —Å–µ–∫—Ç–æ—Ä–æ–≤
for i, w in enumerate(wedges):
    angle = (w.theta2 - w.theta1) / 2.0 + w.theta1
    x = np.cos(np.deg2rad(angle)) * 0.7
    y = np.sin(np.deg2rad(angle)) * 0.7
    ax.text(x, y, f"{df['F1'][i]:.2f}", ha='center', va='center', fontsize=10, weight='bold')

ax.set_title("F1-score per Model (Pie Chart)", fontsize=15, pad=20)
ax.axis("equal")
plt.tight_layout()
plt.savefig(OUT_DIR / "final_F1_pie.png", dpi=300, bbox_inches="tight")
plt.show()

# ---------- 9. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å ----------
best = df.iloc[0]
print(f"\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best['Model']} ({best['Training']})")
print(f"F1 = {best['F1']:.3f}, Accuracy = {best['Accuracy']:.3f}")
print(f"\n–í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Ç–∞–±–ª–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

"""–°–≤–æ–¥–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç—Ä–∏–∫.

–ò–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ—Ç—á—ë—Ç–æ–≤ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è precision, recall, F1 –∏ accuracy, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∏—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ final_models_comparison_table.csv.
–°—Ç—Ä–æ—è—Ç—Å—è –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –≥—Ä–∞—Ñ–∏–∫–∞:
‚Äî —Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ F1-score –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π;
‚Äî —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏.
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ F1, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ /outputs.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 7 (Final Extended v2)
#  Model Efficiency and Performance Analysis (9 Models, Labeled)
# ================================================================

!pip install ptflops --quiet

import torch, timm, time
import pandas as pd
from ptflops import get_model_complexity_info
from pathlib import Path
import matplotlib.pyplot as plt

# ---------- 0. Paths ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs"
OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH = 16
NUM_CLASSES = 3

print(f"Device: {DEVICE}")

# ---------- 1. Models (—Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞) ----------
models_info = [
    ("mobilenetv3_small_100", "MobileNetV3 (Full)", "Full Fine-Tuning", "mobilenetv3_small_100_best.pth"),
    ("resnet50", "ResNet50 (Full)", "Full Fine-Tuning", "resnet50_best.pth"),

    ("efficientnet_b0", "EfficientNet-B0 (Frozen)", "Frozen Backbone", "efficientnet_b0_frozen_best.pth"),
    ("convnext_tiny", "ConvNeXt-Tiny (Stage1)", "Stage 1 (Head Only)", "convnext_tiny_stage1_best.pth"),
    ("convnext_tiny", "ConvNeXt-Tiny (Stage2)", "Progressive Unfreeze", "convnext_tiny_stage2_best.pth"),

    ("mobilenetv3_small_100", "MobileNetV3 (Few-Shot 4ep)", "Few-Shot (250/class, 4ep)", "mobilenetv3_small_100_fewshot_best.pth"),
    ("resnet18", "ResNet18 (Few-Shot 4ep)", "Few-Shot (50/class, 4ep)", "resnet18_fewshot_best.pth"),

    ("mobilenetv3_small_100", "MobileNetV3 (Few-Shot 12ep)", "Few-Shot (250/class, 12ep)", "mobilenetv3_small_100_fewshot_12ep_best.pth"),
    ("resnet18", "ResNet18 (Few-Shot 12ep)", "Few-Shot (50/class, 12ep)", "resnet18_fewshot_12ep_best.pth"),
]

dummy_input = torch.randn(BATCH, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)

# ---------- 2. Efficiency Evaluation ----------
records = []
for model_name, alias, training, weight_file in models_info:
    weight_path = MODELS_DIR / weight_file
    if not weight_path.exists():
        print(f"‚ö†Ô∏è {weight_path.name} not found, skipping.")
        continue

    print(f"\n Evaluating {alias} ({training}) ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)
    model.load_state_dict(torch.load(weight_path, map_location=DEVICE))
    model.eval()

    # --- Parameters & FLOPs ---
    with torch.cuda.amp.autocast(enabled=(DEVICE=="cuda")):
        macs, params = get_model_complexity_info(
            model, (3, IMG_SIZE, IMG_SIZE),
            as_strings=False, print_per_layer_stat=False
        )
    gflops = macs / 1e9
    params_m = params / 1e6

    # --- Inference speed ---
    n_iter = 30
    torch.cuda.synchronize() if DEVICE == "cuda" else None
    t0 = time.time()
    for _ in range(n_iter):
        with torch.no_grad():
            _ = model(dummy_input)
    torch.cuda.synchronize() if DEVICE == "cuda" else None
    dt = (time.time() - t0) / n_iter
    fps = BATCH / dt

    records.append({
        "Model": alias,
        "Training": training,
        "Params (M)": round(params_m, 2),
        "GFLOPs": round(gflops, 2),
        "Time per batch (s)": round(dt, 4),
        "FPS": round(fps, 1),
    })

# ---------- 3. Save & Display Table ----------
if not records:
    print("‚ùå No models were processed. Check weight files.")
else:
    df_eff = pd.DataFrame(records).sort_values("FPS", ascending=False)
    eff_path = OUT_DIR / "models_efficiency_table.csv"
    df_eff.to_csv(eff_path, index=False)
    print("\n‚úÖ Efficiency table saved:", eff_path)
    print(df_eff.to_string(index=False))

    # ---------- 4. Visualization ----------
    plt.figure(figsize=(12, 6))
    plt.scatter(df_eff["Params (M)"], df_eff["FPS"],
                s=df_eff["GFLOPs"]*45,
                c="deepskyblue", alpha=0.8, edgecolors="k")

    # –°–¥–≤–∏–≥–∞–µ–º –ø–æ–¥–ø–∏—Å–∏ –Ω–µ–º–Ω–æ–≥–æ, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞–ª–∏—Å—å
    for i, row in df_eff.iterrows():
        plt.text(row["Params (M)"] + 0.3, row["FPS"] + 40, row["Model"], fontsize=9)

    plt.xlabel("Parameters (Millions)")
    plt.ylabel("FPS (Images per Second)")
    plt.title("Model Efficiency Trade-off (Speed vs Complexity)")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(OUT_DIR / "model_efficiency_scatter_labeled.png", dpi=200)
    plt.show()

    # ---------- 5. –¢–∞–±–ª–∏—Ü–∞ –ø–æ–¥ –≥—Ä–∞—Ñ–∏–∫–æ–º ----------
    fig, ax = plt.subplots(figsize=(10, len(df_eff)*0.5))
    ax.axis("tight")
    ax.axis("off")

    table = ax.table(
        cellText=df_eff.values,
        colLabels=df_eff.columns,
        cellLoc='center',
        loc='center'
    )
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1.2, 1.2)

    plt.title("Model Efficiency Metrics Table", fontsize=13, pad=10)
    plt.savefig(OUT_DIR / "model_efficiency_table_image.png", dpi=300, bbox_inches="tight")
    plt.show()

    print("\n‚úÖ Block 7 completed successfully ‚Äî labeled chart + metrics table saved.")

"""–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π.

–° –ø–æ–º–æ—â—å—é ptflops —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –æ–ø–µ—Ä–∞—Ü–∏–π (GFLOPs), –∞ —Ç–∞–∫–∂–µ –∏–∑–º–µ—Ä—è–µ—Ç—Å—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ (FPS).
–°—Ç—Ä–æ–∏—Ç—Å—è –¥–∏–∞–≥—Ä–∞–º–º–∞ ¬´—Å–∫–æ—Ä–æ—Å—Ç—å-—Å–ª–æ–∂–Ω–æ—Å—Ç—å¬ª, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.
–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–æ—Å–Ω–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 8 (Dual Grad-CAM, Enhanced Fixed)
#  –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –∏ few-shot –º–æ–¥–µ–ª–µ–π
# ================================================================

!pip -q install opencv-python-headless==4.10.0.84

import os, random, cv2, time
from pathlib import Path
from collections import defaultdict

import torch, timm
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_dual"
OUT_DIR.mkdir(parents=True, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH_CAM = 1
NUM_CLASSES = 3
SAMPLES_PER_CLASS = 12

# ---------- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ----------
norm_mean = [0.485, 0.456, 0.406]
norm_std  = [0.229, 0.224, 0.225]
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(norm_mean, norm_std),
])

def denorm(img_tensor):
    mean = torch.tensor(norm_mean, device=img_tensor.device).view(3,1,1)
    std  = torch.tensor(norm_std,  device=img_tensor.device).view(3,1,1)
    img = (img_tensor * std + mean).clamp(0,1).detach().cpu().numpy()
    img = np.transpose(img, (1,2,0))
    return (img*255).astype(np.uint8)

# ---------- –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–¥–Ω–∞–±–æ—Ä ----------
test_ds = datasets.ImageFolder(str(DATA_DIR / "test"), transform=eval_tfms)
class_names = test_ds.classes
cls_to_indices = defaultdict(list)
for idx, (_, y) in enumerate(test_ds.samples):
    cls_to_indices[y].append(idx)
for c in cls_to_indices:
    random.shuffle(cls_to_indices[c])
    cls_to_indices[c] = cls_to_indices[c][:SAMPLES_PER_CLASS]
selected_indices = [i for c in range(len(class_names)) for i in cls_to_indices[c]]
test_subset = torch.utils.data.Subset(test_ds, selected_indices)
test_loader = DataLoader(test_subset, batch_size=BATCH_CAM, shuffle=False, num_workers=2, pin_memory=True)

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_info = [
    ("mobilenetv3_small_100", "MobileNetV3", "Full Fine-Tuning", "mobilenetv3_small_100_best.pth"),
    ("resnet50",              "ResNet50",    "Full Fine-Tuning", "resnet50_best.pth"),
    ("efficientnet_b0",       "EfficientNet-B0", "Frozen Backbone", "efficientnet_b0_frozen_best.pth"),
    ("convnext_tiny",         "ConvNeXt-Tiny", "Stage 1 (Head Only)", "convnext_tiny_stage1_best.pth"),
    ("convnext_tiny",         "ConvNeXt-Tiny", "Progressive Unfreeze", "convnext_tiny_stage2_best.pth"),
    ("mobilenetv3_small_100", "MobileNetV3", "Few-Shot (250/class, 4ep)", "mobilenetv3_small_100_fewshot_best.pth"),
    ("resnet18",              "ResNet18", "Few-Shot (50/class, 4ep)", "resnet18_fewshot_best.pth"),
    ("mobilenetv3_small_100", "MobileNetV3", "Few-Shot (250/class, 12ep)", "mobilenetv3_small_100_fewshot_12ep_best.pth"),
    ("resnet18",              "ResNet18", "Few-Shot (50/class, 12ep)", "resnet18_fewshot_12ep_best.pth"),
]

# ---------- –û—Ç–∫–ª—é—á–µ–Ω–∏–µ inplace-–∞–∫—Ç–∏–≤–∞—Ü–∏–π ----------
def disable_inplace_activations(model: nn.Module):
    for m in model.modules():
        if isinstance(m, (nn.ReLU, nn.ReLU6, nn.Hardswish)):
            if hasattr(m, "inplace") and m.inplace:
                m.inplace = False

# ---------- –ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ Conv-—Å–ª–æ—è ----------
def find_last_conv_layer(model: nn.Module):
    last_conv = None
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            last_conv = module
    if last_conv is None:
        raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π.")
    return last_conv

# ---------- Grad-CAM –∫–ª–∞—Å—Å ----------
class SimpleGradCAM:
    def __init__(self, model: nn.Module, target_layer: nn.Module):
        self.model = model
        self.target_layer = target_layer
        self.activations = None
        self.gradients = None
        self.hook_a = target_layer.register_forward_hook(self._hook_activations)
        self.hook_g = target_layer.register_full_backward_hook(self._hook_gradients)

    def _hook_activations(self, module, inp, out):
        self.activations = out.clone()

    def _hook_gradients(self, module, grad_in, grad_out):
        self.gradients = grad_out[0].clone()

    def remove_hooks(self):
        try:
            self.hook_a.remove()
            self.hook_g.remove()
        except:
            pass

    def __call__(self, logits, class_idx: int):
        self.model.zero_grad(set_to_none=True)
        score = logits[0, class_idx]
        score.backward(retain_graph=True)

        if self.gradients is None or self.gradients.abs().sum() == 0:
            cam = self.activations.mean(dim=1, keepdim=True)  # fallback
        else:
            weights = self.gradients.mean(dim=(2,3), keepdim=True)
            cam = (weights * self.activations).sum(dim=1, keepdim=True)

        cam = torch.relu(cam)
        cam = cam[0,0].detach().cpu().numpy()
        cam -= cam.min()
        cam /= (cam.max() + 1e-8)
        return cam

# ---------- CAM Overlay ----------
def overlay(rgb_uint8, cam_2d, alpha=0.35):
    h, w = rgb_uint8.shape[:2]
    heat = cv2.applyColorMap((cam_2d*255).astype(np.uint8), cv2.COLORMAP_JET)
    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)
    heat = cv2.resize(heat, (w, h))
    return (alpha*heat + (1-alpha)*rgb_uint8).astype(np.uint8)

softmax = nn.Softmax(dim=1)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
for model_name, alias, training, weight_file in models_info:
    weight_path = MODELS_DIR / weight_file
    if not weight_path.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ({training}) ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ {weight_file}")
        continue

    print(f"\n Grad-CAM –¥–ª—è {alias} ‚Äî {training}")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)
    model.load_state_dict(torch.load(weight_path, map_location=DEVICE))

    # –†–∞–∑—Ä–µ—à–∞–µ–º grad –¥–∞–∂–µ –¥–ª—è frozen –º–æ–¥–µ–ª–µ–π
    for param in model.parameters():
        param.requires_grad = True

    model.eval()
    disable_inplace_activations(model)
    torch.set_float32_matmul_precision("high")

    target_layer = find_last_conv_layer(model)
    cam_explainer = SimpleGradCAM(model, target_layer)

    out_dir = OUT_DIR / f"{alias}__{training.replace(' ','_').replace('/','-')}"
    out_dir.mkdir(parents=True, exist_ok=True)

    for i, (x, y_true) in enumerate(test_loader):
        x = x.to(DEVICE)
        with torch.enable_grad():
            logits = model(x)
        probs = softmax(logits)
        y_pred = int(torch.argmax(probs, dim=1))
        conf = float(probs[0, y_pred].detach())

        if y_pred == int(y_true):
            continue

        cam_pred = cam_explainer(logits, y_pred)
        cam_true = cam_explainer(logits, int(y_true))

        img_rgb = denorm(x[0])
        overlay_pred = overlay(img_rgb, cam_pred)
        overlay_true = overlay(img_rgb, cam_true)

        left  = cv2.putText(overlay_pred.copy(), f"Pred: {class_names[y_pred]} ({conf:.2f})",
                            (10,25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
        right = cv2.putText(overlay_true.copy(), f"True: {class_names[int(y_true)]}",
                            (10,25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
        concat = np.concatenate([left, right], axis=1)

        fname = f"err_idx{i:04d}__pred-{class_names[y_pred]}__true-{class_names[int(y_true)]}.jpg"
        cv2.imwrite(str(out_dir / fname), cv2.cvtColor(concat, cv2.COLOR_RGB2BGR))

    cam_explainer.remove_hooks()
    print(f"‚úÖ CAM —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_dir}")

print("\n–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω–∞ –ø–æ–¥–ø–∞–ø–∫–∞. –°–ª–µ–≤–∞ CAM –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, —Å–ø—Ä–∞–≤–∞ ‚Äî –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞.")

"""–í —ç—Ç–æ–º –±–ª–æ–∫–µ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –¥–≤–æ–π–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Grad-CAM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–≥–æ, –Ω–∞ –∫–∞–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
–ú–µ—Ç–æ–¥ Grad-CAM (Gradient-weighted Class Activation Mapping) —Å—Ç—Ä–æ–∏—Ç —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–∫–∞–∑—ã–≤–∞—è –≤–∫–ª–∞–¥ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∏—Ç–æ–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å–µ—Ç–∏.
–ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π (¬´robust¬ª) –≤–∞—Ä–∏–∞–Ω—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä (MobileNetV3, ResNet, EfficientNet, ConvNeXt), –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º inplace-–∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ hooks –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏:

- –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è (—Ç–æ—á–∫–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∑–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º–∏);

- –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –¥–≤–∞ CAM-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è:

  - 1)–¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–æ–±–ª–∞—Å—Ç–∏, –ø–æ–≤–ª–∏—è–≤—à–∏–µ –Ω–∞ –æ—à–∏–±–æ—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ),

  - 2)–¥–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (—É—á–∞—Å—Ç–∫–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã–ª–∞ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ);

- —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: —Å–ª–µ–≤–∞ ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, —Å–ø—Ä–∞–≤–∞ ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–ª–∞—Å—Å.

–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—à–∏–±–æ—á–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (–ø–æ 12 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å), —á—Ç–æ–±—ã –Ω–∞–≥–ª—è–¥–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—à–∏–±–æ–∫.
–ù–∞ –∫–∞–∂–¥–æ–π —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç–µ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –ø–æ–¥–ø–∏—Å–∏ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –∫–ª–∞—Å—Å–æ–º, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∏ –∏—Å—Ç–∏–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π.
–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –∫–∞—Ç–∞–ª–æ–≥
/outputs/gradcam_dual/<Model>__<TrainingType>/,
–≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥–ø–∞–ø–∫–∏.

–ë–ª–∞–≥–æ–¥–∞—Ä—è –¥–∞–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Å–µ—Ç–∏: –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —É—á–∞—Å—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è–º, —Å—Ä–∞–≤–Ω–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –æ–±–æ—Å–Ω–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 9 (Error Stats Fixed v2)
#  –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ heatmap –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∫–ª–∞—Å—Å–æ–≤
# ================================================================

import os, re, pandas as pd, matplotlib.pyplot as plt
from pathlib import Path
import seaborn as sns

# –ü—É—Ç–∏
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
GCAM_DIR = DRIVE_ROOT / "outputs" / "gradcam_dual"
OUT_DIR  = DRIVE_ROOT / "outputs"
OUT_DIR.mkdir(exist_ok=True, parents=True)

# –†–µ–≥—É–ª—è—Ä–∫–∞
pattern = re.compile(r"pred-([a-zA-Z0-9_-]+)__true-([a-zA-Z0-9_-]+)")

records = []
for model_dir in sorted(GCAM_DIR.iterdir()):
    if not model_dir.is_dir():
        continue
    model_name = model_dir.name
    for f in model_dir.glob("*.jpg"):
        m = pattern.search(f.name)
        if not m:
            continue
        pred, true = m.group(1), m.group(2)
        records.append({"Model": model_name, "True": true, "Pred": pred})

if not records:
    print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏ Grad-CAM. –£–±–µ–¥–∏—Å—å, —á—Ç–æ block 8 –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è.")
else:
    df = pd.DataFrame(records)
    all_classes = sorted(set(df["True"]).union(set(df["Pred"])))

    # --- –û–±—â–∞—è —Ç–∞–±–ª–∏—Ü–∞ ---
    summary = (
        df.groupby(["Model", "True", "Pred"])
          .size()
          .reset_index(name="Count")
          .sort_values("Count", ascending=False)
          .reset_index(drop=True)
    )

    table_path = OUT_DIR / "final_error_stats.csv"
    summary.to_csv(table_path, index=False)
    print(f"\n‚úÖ –ü–æ–ª–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {table_path}")

    print("\nüîù –¢–æ–ø-20 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫:")
    display(summary.head(20))

    # --- –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã ---
    for model_name, group in df.groupby("Model"):
        # —Å–æ–∑–¥–∞–µ–º pivot, –∑–∞–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∫–ª–∞—Å—Å—ã –Ω—É–ª—è–º–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        pivot = (
            group.groupby(["True","Pred"])
                 .size()
                 .unstack(fill_value=0)
                 .reindex(index=all_classes, columns=all_classes, fill_value=0)
        )

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞
        pivot = pivot.fillna(0).astype(int)

        plt.figure(figsize=(6,5))
        sns.heatmap(pivot, annot=True, fmt="d", cmap="YlOrRd", cbar=False)
        plt.title(f"–û—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚Äî {model_name}", fontsize=11)
        plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
        plt.ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
        plt.tight_layout()

        save_path = OUT_DIR / f"final_error_heatmap_{model_name}.png"
        plt.savefig(save_path, dpi=200)
        plt.close()
        print(f"üìä –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {save_path}")



    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω –±–µ–∑ –æ—à–∏–±–æ–∫.")
    print("–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:", OUT_DIR)

"""–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º Grad-CAM.

–§–∞–π–ª—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏—Ä–∞—é—Ç—Å—è –ø–æ –∏–º–µ–Ω–∞–º (pred-X__true-Y), —Å–æ–∑–¥–∞—ë—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –æ—à–∏–±–æ–∫ (final_error_stats.csv) –∏ —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã —á–∞—Å—Ç–æ—Ç—ã –ø—É—Ç–∞–Ω–∏—Ü—ã –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏.
–≠—Ç–æ –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –∫–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–± –æ—à–∏–±–∫–∞—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
"""

from pathlib import Path
ZIP_PATH = Path("/content/drive/MyDrive/avatar_recog/300img_test/Test.zip")
print("–°—É—â–µ—Å—Ç–≤—É–µ—Ç:", ZIP_PATH.exists())
print("–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞:", ZIP_PATH.stat().st_size / 1024, "KB")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 9B
#  Blind unified evaluation (300 mixed images through 9 models)
# ================================================================

import zipfile, shutil, os, random
from pathlib import Path
import torch, timm
from torchvision import transforms
from PIL import Image
import pandas as pd
import numpy as np
from tqdm import tqdm

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "vk_blind_eval"
OUT_DIR.mkdir(parents=True, exist_ok=True)

ZIP_PATH = "/content/drive/MyDrive/avatar_recog/300img_test/Test.zip"

# ---------- 1. –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ ----------
if TEST_ROOT.exists():
    shutil.rmtree(TEST_ROOT)
with zipfile.ZipFile(ZIP_PATH, 'r') as zf:
    zf.extractall(TEST_ROOT)

# ---------- 2. –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º ----------
folders = {
    "AI_test": "generated",
    "drawn_test": "drawing",
    "real_test": "real"
}

all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})

random.shuffle(all_images)
print(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}")

# ---------- 3. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- 4. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- 5. –§—É–Ω–∫—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ ----------
def predict_batch(model, imgs):
    batch = torch.stack([tfm(Image.open(p).convert("RGB")) for p in imgs]).to(DEVICE)
    with torch.no_grad():
        logits = model(batch)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
    return [CLASSES[p] for p in preds]

# ---------- 6. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
results = []
paths = [x["path"] for x in all_images]
true_labels = [x["true"] for x in all_images]
df = pd.DataFrame({"image": [p.name for p in paths], "true_label": true_labels})

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤ {weight}")
        continue

    print(f"\n –ü—Ä–æ–≥–æ–Ω—è–µ–º {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    preds_all = []
    for i in tqdm(range(0, len(paths), 32)):
        batch_paths = paths[i:i+32]
        preds_all.extend(predict_batch(model, batch_paths))

    df[alias] = preds_all

# ---------- 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
out_csv = OUT_DIR / "vk_blind_eval_results.csv"
df.to_csv(out_csv, index=False)
print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {out_csv}")

# ---------- 8.  —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–∞–∂–¥–æ–º—É alias ----------
for col in df.columns[2:]:
    acc = np.mean(df["true_label"] == df[col])
    print(f"{col:30s} | Accuracy: {acc:.3f}")

print("\n‚úÖ Block 9B completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 9C
#  Visual comparison of model accuracy and confusion
# ================================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np
from pathlib import Path

# ---------- 0. –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
CSV_PATH = DRIVE_ROOT / "outputs" / "vk_blind_eval" / "vk_blind_eval_results.csv"
OUT_DIR = DRIVE_ROOT / "outputs" / "vk_blind_eval"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
df = pd.read_csv(CSV_PATH)
model_cols = df.columns[2:]
print("–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π:", len(model_cols))

# ---------- 2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ ----------
acc_data = []
for m in model_cols:
    acc = np.mean(df["true_label"] == df[m])
    acc_data.append({"Model": m, "Accuracy": acc})
acc_df = pd.DataFrame(acc_data).sort_values("Accuracy", ascending=False)

# ---------- 3. –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ ----------
plt.figure(figsize=(10,5))
sns.barplot(acc_df, x="Model", y="Accuracy", palette="Blues_d")
plt.title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ (300 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
plt.xticks(rotation=30, ha="right")
plt.ylim(0,1)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig(OUT_DIR / "vk_blind_accuracy_bar.png", dpi=300)
plt.show()

# ---------- 4. Confusion Matrix –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ ----------
best_model = acc_df.iloc[0]["Model"]
print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}")

classes = sorted(df["true_label"].unique())
cm = confusion_matrix(df["true_label"], df[best_model], labels=classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot(cmap="Blues", values_format="d")
plt.title(f"Confusion Matrix ‚Äî {best_model}")
plt.savefig(OUT_DIR / f"vk_confusion_{best_model.replace(' ','_')}.png", dpi=300)
plt.show()

print("\n‚úÖ Block 9C completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 9D
#  Re-evaluation on the same 300 images and comparison with previous run
# ================================================================

import torch, timm, random
from pathlib import Path
from torchvision import transforms
from PIL import Image
import pandas as pd
import numpy as np
from tqdm import tqdm

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "vk_blind_eval"
OUT_DIR.mkdir(parents=True, exist_ok=True)

OLD_CSV = OUT_DIR / "vk_blind_eval_results.csv"
NEW_CSV = OUT_DIR / "vk_blind_eval_results_rerun.csv"

# ---------- 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ----------
old_df = pd.read_csv(OLD_CSV)
print("–°—Ç–∞—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞–π–¥–µ–Ω:", OLD_CSV.exists())

# ---------- 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ ----------
paths = [Path("/content/avatar_recog/data/300img_test/Test/AI_test"),
         Path("/content/avatar_recog/data/300img_test/Test/drawn_test"),
         Path("/content/avatar_recog/data/300img_test/Test/real_test")]

folders = {
    "AI_test": "generated",
    "drawn_test": "drawing",
    "real_test": "real"
}

all_images = []
for p in paths:
    folder_name = p.name
    label = folders.get(folder_name, "unknown")
    for ext in ("*.jpg","*.jpeg","*.png"):
        for f in p.glob(ext):
            all_images.append({"path": f, "true": label})
random.shuffle(all_images)
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

# ---------- 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

def predict_batch(model, imgs):
    batch = torch.stack([tfm(Image.open(p).convert("RGB")) for p in imgs]).to(DEVICE)
    with torch.no_grad():
        logits = model(batch)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
    return [CLASSES[p] for p in preds]

# ---------- 4. –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –ø—Ä–æ–≥–æ–Ω ----------
df_new = pd.DataFrame({
    "image": [p["path"].name for p in all_images],
    "true_label": [p["true"] for p in all_images]
})

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤ {weight}")
        continue

    print(f"\n –ü—Ä–æ–≥–æ–Ω—è–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    preds_all = []
    for i in tqdm(range(0, len(all_images), 32)):
        batch_paths = [x["path"] for x in all_images[i:i+32]]
        preds_all.extend(predict_batch(model, batch_paths))

    df_new[alias] = preds_all

# ---------- 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º ----------
df_new.to_csv(NEW_CSV, index=False)
print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {NEW_CSV}")

# ---------- 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ 1-–≥–æ –∏ 2-–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ----------
print("\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–µ–π (—Å—Ç–∞—Ä—ã–π ‚Üí –Ω–æ–≤—ã–π):")
for col in df_new.columns[2:]:
    if col in old_df.columns:
        acc_old = np.mean(old_df["true_label"] == old_df[col])
        acc_new = np.mean(df_new["true_label"] == df_new[col])
        delta = acc_new - acc_old
        print(f"{col:30s} | {acc_old:.3f} ‚Üí {acc_new:.3f}  (Œî={delta:+.3f})")

print("\n‚úÖ Block 9D completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 10
#  Robustness test: noise, blur, brightness, rotation, JPEG compression
# ================================================================

import torch, timm, random, cv2, os
from pathlib import Path
from torchvision import transforms
from PIL import Image
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "robustness_test"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
folders = {
    "AI_test": "generated",
    "drawn_test": "drawing",
    "real_test": "real"
}

all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

# ---------- 2. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm_base = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- 3. –§—É–Ω–∫—Ü–∏–∏ –∏—Å–∫–∞–∂–µ–Ω–∏–π ----------
def apply_noise(img, intensity=15):
    arr = np.array(img).astype(np.float32)
    noise = np.random.normal(0, intensity, arr.shape)
    arr = np.clip(arr + noise, 0, 255).astype(np.uint8)
    return Image.fromarray(arr)

def apply_blur(img, k=5):
    return Image.fromarray(cv2.GaussianBlur(np.array(img), (k, k), 0))

def apply_brightness(img, factor=1.5):
    hsv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2HSV)
    hsv = hsv.astype(np.float32)
    hsv[...,2] = np.clip(hsv[...,2]*factor, 0, 255)
    hsv = hsv.astype(np.uint8)
    return Image.fromarray(cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB))

def apply_rotation(img, angle=15):
    return img.rotate(angle)

def apply_jpeg(img, quality=30):
    tmp = "/tmp/tmp_q.jpg"
    img.save(tmp, quality=quality)
    return Image.open(tmp).convert("RGB")

transformations = {
    "noise": lambda img: apply_noise(img, 15),
    "blur": lambda img: apply_blur(img, 5),
    "brightness": lambda img: apply_brightness(img, 1.5),
    "rotation": lambda img: apply_rotation(img, 15),
    "jpeg": lambda img: apply_jpeg(img, 30)
}

# ---------- 4. –ú–æ–¥–µ–ª–∏ ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- 5. –û—Ü–µ–Ω–∫–∞ ----------
def predict(model, img):
    timg = tfm_base(img).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        pred = torch.argmax(model(timg), dim=1).item()
    return CLASSES[pred]

results = []
for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤ {weight}")
        continue

    print(f"\n–ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    # –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π
    preds = [predict(model, Image.open(x['path']).convert('RGB')) for x in tqdm(all_images, desc=f"{alias} clean")]
    acc_clean = accuracy_score([x['true'] for x in all_images], preds)
    row = {"Model": alias, "clean": acc_clean}

    # –° –∏—Å–∫–∞–∂–µ–Ω–∏—è–º–∏
    for tname, tfunc in transformations.items():
        preds_t = []
        for x in tqdm(all_images, desc=f"{alias} {tname}"):
            img = Image.open(x['path']).convert('RGB')
            img_t = tfunc(img)
            preds_t.append(predict(model, img_t))
        acc_t = accuracy_score([x['true'] for x in all_images], preds_t)
        row[tname] = acc_t

    results.append(row)

# ---------- 6. –ê–Ω–∞–ª–∏–∑ ----------
df = pd.DataFrame(results)
df.to_csv(OUT_DIR / "robustness_results.csv", index=False)

# –í—ã—á–∏—Å–ª–∏–º –ø–∞–¥–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —á–∏—Å—Ç—ã—Ö
for col in transformations.keys():
    df[f"{col}_drop"] = df["clean"] - df[col]

# ---------- 7. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
plt.figure(figsize=(10,6))
melted = df.melt(id_vars="Model", value_vars=["noise","blur","brightness","rotation","jpeg"],
                 var_name="Distortion", value_name="Accuracy")
sns.barplot(melted, x="Model", y="Accuracy", hue="Distortion")
plt.title("–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º")
plt.xticks(rotation=30, ha="right")
plt.tight_layout()
plt.savefig(OUT_DIR / "robustness_bar.png", dpi=300)
plt.show()

print("\n‚úÖ Block 10 completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 10B
#  Robustness summary: best models by distortion type (sorted)
# ================================================================

import pandas as pd
from pathlib import Path


DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
ROBUST_CSV = DRIVE_ROOT / "outputs" / "robustness_test" / "robustness_results.csv"

# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ ----------
df = pd.read_csv(ROBUST_CSV)
distortions = ["noise", "blur", "brightness", "rotation", "jpeg"]

# ---------- –¢–∞–±–ª–∏—Ü–∞ –ø–æ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º ----------
summary = []
for d in distortions:
    best_row = df.loc[df[d].idxmax()]
    summary.append({
        "Distortion": d,
        "Best_Model": best_row["Model"],
        "Accuracy": round(best_row[d], 3),
        "Drop_vs_Clean": round(best_row["clean"] - best_row[d], 3)
    })

# —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (–ø–æ —Å–ø–∏—Å–∫—É distortions)
summary_df = pd.DataFrame(summary).set_index("Distortion").loc[distortions].reset_index()

print("–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–ø—É –∏—Å–∫–∞–∂–µ–Ω–∏–π:")
display(summary_df)

# ---------- –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ —Å—Ä–µ–¥–Ω–µ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ ----------
df["mean_distortion_acc"] = df[distortions].mean(axis=1)
ranking = df[["Model","clean","mean_distortion_acc"]].sort_values(
    by="mean_distortion_acc", ascending=False
).reset_index(drop=True)

print("\n–°—Ä–µ–¥–Ω—è—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π (–æ—Ç –ª—É—á—à–µ–π –∫ —Ö—É–¥—à–µ–π):")
display(ranking)

print("\n‚úÖ Block 10B completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 11
#  Softmax Confidence Analysis (confidence histograms & calibration)
# ================================================================

import torch, timm, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from pathlib import Path
from torchvision import transforms
from PIL import Image
from sklearn.metrics import accuracy_score
import pandas as pd

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "confidence_analysis"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- 2. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- 3. –§—É–Ω–∫—Ü–∏—è Softmax-–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ ----------
def predict_with_confidence(model, imgs):
    probs_all, preds_all = [], []
    for p in imgs:
        img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
        preds_all.append(np.argmax(probs))
        probs_all.append(np.max(probs))
    return preds_all, probs_all

# ---------- 4. –ê–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–π (–∏–ª–∏ –ª—É—á—à–∏—Ö) –º–æ–¥–µ–ª–µ–π ----------
# –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –≤–æ–∑—å–º—ë–º –ª—É—á—à—É—é ‚Äî ResNet18 FewShot12ep
target_model = ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep")

model = timm.create_model(target_model[0], pretrained=False, num_classes=NUM_CLASSES)
model.load_state_dict(torch.load(MODELS_DIR / target_model[1], map_location=DEVICE))
model = model.to(DEVICE)
model.eval()

paths = [x["path"] for x in all_images]
labels_true = [CLASSES.index(x["true"]) for x in all_images]
preds, confs = predict_with_confidence(model, paths)
acc = accuracy_score(labels_true, preds)
print(f"{target_model[2]} ‚Äî Accuracy: {acc:.3f}")

# ---------- 5. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ----------
plt.figure(figsize=(7,4))
sns.histplot(confs, bins=20, kde=False, color="skyblue")
plt.title(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ‚Äî {target_model[2]}")
plt.xlabel("Softmax confidence")
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
plt.tight_layout()
plt.savefig(OUT_DIR / f"{target_model[2].replace(' ','_')}_confidence_hist.png", dpi=300)
plt.show()

# ---------- 6. Reliability Diagram ----------
def reliability_diagram(y_true, y_pred, confs, bins=10):
    bins_edges = np.linspace(0.0, 1.0, bins+1)
    bin_ids = np.digitize(confs, bins_edges, right=True)
    accs, conf_means, counts = [], [], []
    for i in range(1, bins+1):
        mask = bin_ids == i
        if np.any(mask):
            acc_bin = np.mean(np.array(y_true)[mask] == np.array(y_pred)[mask])
            conf_bin = np.mean(np.array(confs)[mask])
            accs.append(acc_bin)
            conf_means.append(conf_bin)
            counts.append(np.sum(mask))
    return conf_means, accs, counts

conf_means, accs, counts = reliability_diagram(labels_true, preds, confs, bins=10)

plt.figure(figsize=(5,5))
plt.plot([0,1],[0,1],"--",color="gray")
plt.plot(conf_means, accs, marker="o", color="blue")
plt.xlabel("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å")
plt.ylabel("–î–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
plt.title(f"Reliability Diagram ‚Äî {target_model[2]}")
plt.tight_layout()
plt.savefig(OUT_DIR / f"{target_model[2].replace(' ','_')}_reliability.png", dpi=300)
plt.show()

# ---------- 7. –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ ----------
ece = np.average(np.abs(np.array(accs) - np.array(conf_means)), weights=counts)
print(f"ECE (Expected Calibration Error): {ece:.3f}")
print("\n‚úÖ Block 11 completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 11B
#  Softmax Confidence Analysis for all 9 models
# ================================================================

import torch, timm, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from pathlib import Path
from torchvision import transforms
from PIL import Image
from sklearn.metrics import accuracy_score
import pandas as pd
from tqdm import tqdm

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "confidence_analysis_all"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- 2. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------
def predict_with_confidence(model, imgs):
    probs_all, preds_all = [], []
    for p in imgs:
        img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
        preds_all.append(np.argmax(probs))
        probs_all.append(np.max(probs))
    return preds_all, probs_all

def reliability_diagram(y_true, y_pred, confs, bins=10):
    bins_edges = np.linspace(0.0, 1.0, bins+1)
    bin_ids = np.digitize(confs, bins_edges, right=True)
    accs, conf_means, counts = [], [], []
    for i in range(1, bins+1):
        mask = bin_ids == i
        if np.any(mask):
            acc_bin = np.mean(np.array(y_true)[mask] == np.array(y_pred)[mask])
            conf_bin = np.mean(np.array(confs)[mask])
            accs.append(acc_bin)
            conf_means.append(conf_bin)
            counts.append(np.sum(mask))
    ece = np.average(np.abs(np.array(accs) - np.array(conf_means)), weights=counts)
    return conf_means, accs, counts, ece

# ---------- 4. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
paths = [x["path"] for x in all_images]
labels_true = [CLASSES.index(x["true"]) for x in all_images]

summary = []

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias} ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ {weight}")
        continue

    print(f"\n–ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    preds, confs = predict_with_confidence(model, paths)
    acc = accuracy_score(labels_true, preds)
    mean_conf = float(np.mean(confs))
    conf_means, accs, counts, ece = reliability_diagram(labels_true, preds, confs, bins=10)

    summary.append({
        "Model": alias,
        "Accuracy": round(acc, 3),
        "Mean_Confidence": round(mean_conf, 3),
        "ECE": round(ece, 3)
    })

    # --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ ---
    out_subdir = OUT_DIR / alias.replace(" ", "_")
    out_subdir.mkdir(parents=True, exist_ok=True)

    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    plt.figure(figsize=(7,4))
    sns.histplot(confs, bins=20, kde=False, color="skyblue")
    plt.title(f"Confidence Distribution ‚Äî {alias}")
    plt.xlabel("Softmax confidence")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig(out_subdir / "confidence_hist.png", dpi=300)
    plt.close()

    # Reliability diagram
    plt.figure(figsize=(5,5))
    plt.plot([0,1],[0,1],"--",color="gray")
    plt.plot(conf_means, accs, marker="o", color="blue")
    plt.xlabel("Average confidence")
    plt.ylabel("Accuracy per bin")
    plt.title(f"Reliability Diagram ‚Äî {alias}\nECE={ece:.3f}")
    plt.tight_layout()
    plt.savefig(out_subdir / "reliability.png", dpi=300)
    plt.close()

# ---------- 5. –¢–∞–±–ª–∏—Ü–∞ –∏ —Ä–µ–π—Ç–∏–Ω–≥ ----------
summary_df = pd.DataFrame(summary)
summary_df = summary_df.sort_values("ECE", ascending=True).reset_index(drop=True)
summary_path = OUT_DIR / "confidence_summary.csv"
summary_df.to_csv(summary_path, index=False)
print("\n–û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (—á–µ–º –Ω–∏–∂–µ ECE, —Ç–µ–º –ª—É—á—à–µ):")
display(summary_df)

print(f"\n–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")
print("\n‚úÖ Block 11B completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 11C
#  Comparative visualization of calibration and accuracy across models
# ================================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
SUMMARY_CSV = DRIVE_ROOT / "outputs" / "confidence_analysis_all" / "confidence_summary.csv"
OUT_DIR = DRIVE_ROOT / "outputs" / "confidence_analysis_all"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ ----------
df = pd.read_csv(SUMMARY_CSV)
df = df.sort_values("ECE", ascending=True).reset_index(drop=True)

print("–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π:", len(df))
display(df)

# ---------- Accuracy vs ECE ----------
plt.figure(figsize=(9,5))
ax = sns.barplot(df, x="Model", y="Accuracy", color="skyblue", label="Accuracy")
sns.barplot(df, x="Model", y="ECE", color="salmon", alpha=0.6, label="ECE")
plt.xticks(rotation=30, ha="right")
plt.ylabel("–ó–Ω–∞—á–µ–Ω–∏—è")
plt.title("Calibration vs Accuracy across models")
plt.legend()
plt.tight_layout()
plt.savefig(OUT_DIR / "calibration_vs_accuracy.png", dpi=300)
plt.show()

# ---------- Mean Confidence ----------
plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="Mean_Confidence", palette="Blues_d")
plt.xticks(rotation=30, ha="right")
plt.title("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π")
plt.ylabel("Mean Confidence")
plt.tight_layout()
plt.savefig(OUT_DIR / "mean_confidence.png", dpi=300)
plt.show()

# ---------- –í—ã–≤–æ–¥ –ª—É—á—à–∏—Ö ----------
best_acc = df.loc[df["Accuracy"].idxmax()]
best_cal = df.loc[df["ECE"].idxmin()]

print(f"\n–õ—É—á—à–∞—è –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏: {best_acc['Model']} (Accuracy={best_acc['Accuracy']:.3f})")
print(f"–ù–∞–∏–±–æ–ª–µ–µ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–∞—è: {best_cal['Model']} (ECE={best_cal['ECE']:.3f})")

print("\n‚úÖ Block 11C completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 12
#  Feature Embedding Visualization (t-SNE / UMAP)
# ================================================================

import torch, timm
from torchvision import transforms
from PIL import Image
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.manifold import TSNE
# (–¥–ª—è UMAP –º–æ–∂–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å: from umap import UMAP)

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "feature_embeddings"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

# ---------- –ú–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏)

# ---------- –§—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ----------
def extract_features(model, img_paths):
    features, labels = [], []
    model.eval()
    with torch.no_grad():
        for x in tqdm(img_paths):
            img = Image.open(x["path"]).convert("RGB")
            tensor = tfm(img).unsqueeze(0).to(DEVICE)
            feat = model.forward_features(tensor)  # –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ timm –º–æ–¥–µ–ª–µ–π
            feat = torch.nn.functional.adaptive_avg_pool2d(feat, (1,1)).squeeze().cpu().numpy()
            features.append(feat)
            labels.append(x["true"])
    return np.array(features), np.array(labels)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
for model_name, weight, alias in models_to_eval:
    print(f"\n–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {alias} ...")
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤ {weight}")
        continue

    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)

    feats, labels = extract_features(model, all_images)

    # ---------- t-SNE ----------
    print("‚Üí –ó–∞–ø—É—Å–∫ t-SNE ...")
    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000, random_state=42)
    emb = tsne.fit_transform(feats)

    df_emb = pd.DataFrame({"x": emb[:,0], "y": emb[:,1], "label": labels})
    plt.figure(figsize=(7,6))
    sns.scatterplot(df_emb, x="x", y="y", hue="label", palette="Set2", s=45, alpha=0.8)
    plt.title(f"t-SNE Embedding ‚Äî {alias}")
    plt.legend(title="Class", loc="best")
    plt.tight_layout()
    plt.savefig(OUT_DIR / f"tsne_{alias.replace(' ','_')}.png", dpi=300)
    plt.show()

    # ---------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ----------
    out_npz = OUT_DIR / f"features_{alias.replace(' ','_')}.npz"
    np.savez(out_npz, features=feats, labels=labels)
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {out_npz}")

print("\n‚úÖ Block 12 completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 12B
#  Cluster Structure & Separation Analysis (silhouette + distances)
# ================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import cdist

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
EMB_DIR = DRIVE_ROOT / "outputs" / "feature_embeddings"
OUT_DIR = DRIVE_ROOT / "outputs" / "feature_analysis"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]

# ---------- –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ ----------
npz_files = sorted(EMB_DIR.glob("features_*.npz"))
print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(npz_files)}")

summary = []

# ---------- –§—É–Ω–∫—Ü–∏–∏ ----------
def mean_pairwise_distance(a, b):
    """–°—Ä–µ–¥–Ω–µ–µ –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤—Å–µ–º–∏ —Ç–æ—á–∫–∞–º–∏ –¥–≤—É—Ö –º–Ω–æ–∂–µ—Å—Ç–≤"""
    dists = cdist(a, b, metric='euclidean')
    return np.mean(dists)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
for npz_path in npz_files:
    data = np.load(npz_path, allow_pickle=True)
    features = data["features"]
    labels = data["labels"]
    alias = npz_path.stem.replace("features_", "")
    print(f"\n–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {alias}")

    # –°—á–∏—Ç–∞–µ–º silhouette
    label_nums = np.array([CLASSES.index(lbl) for lbl in labels])
    sil = silhouette_score(features, label_nums)
    print(f"Silhouette Score: {sil:.3f}")

    # –ú–µ–∂- –∏ –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
    intra_dists, inter_dists = [], []
    for cls_i in CLASSES:
        group_i = features[labels == cls_i]
        intra_dists.append(np.mean(cdist(group_i, group_i)))
        for cls_j in CLASSES:
            if cls_i == cls_j:
                continue
            group_j = features[labels == cls_j]
            inter_dists.append(mean_pairwise_distance(group_i, group_j))

    intra_mean = np.mean(intra_dists)
    inter_mean = np.mean(inter_dists)
    separation = inter_mean / intra_mean

    summary.append({
        "Model": alias,
        "Silhouette": round(sil, 3),
        "IntraClassDist": round(intra_mean, 3),
        "InterClassDist": round(inter_mean, 3),
        "SeparationRatio": round(separation, 3)
    })

    # ---------- Heatmap ----------
    dist_matrix = np.zeros((3, 3))
    for i, ci in enumerate(CLASSES):
        for j, cj in enumerate(CLASSES):
            dist_matrix[i, j] = mean_pairwise_distance(
                features[labels == ci],
                features[labels == cj]
            )

    plt.figure(figsize=(5,4))
    sns.heatmap(dist_matrix, annot=True, fmt=".2f", cmap="coolwarm",
                xticklabels=CLASSES, yticklabels=CLASSES)
    plt.title(f"Feature Distance Heatmap ‚Äî {alias}")
    plt.tight_layout()
    plt.savefig(OUT_DIR / f"heatmap_{alias}.png", dpi=300)
    plt.close()

# ---------- –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
summary_df = pd.DataFrame(summary).sort_values("Silhouette", ascending=False)
summary_df.to_csv(OUT_DIR / "cluster_separation_summary.csv", index=False)

print("\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
display(summary_df)

print(f"\n‚úÖ Block 12B completed successfully. Heatmaps –∏ —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 13
#  Ensemble Voting (Soft-Voting) ‚Äî comparing single vs combined models
# ================================================================

import torch, timm, numpy as np, pandas as pd
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "ensemble_voting"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

paths = [x["path"] for x in all_images]
true_labels = [CLASSES.index(x["true"]) for x in all_images]

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è softmax-–ª–æ–≥–∏—Ç–æ–≤ ----------
def get_model_probs(model, img_paths):
    model.eval()
    probs_all = []
    with torch.no_grad():
        for p in img_paths:
            img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
            probs_all.append(probs)
    return np.array(probs_all)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
results = []
ensemble_probs_sum = np.zeros((len(paths), NUM_CLASSES))

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias} ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ {weight}")
        continue

    print(f"\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)

    model_probs = get_model_probs(model, paths)
    preds = np.argmax(model_probs, axis=1)
    acc = accuracy_score(true_labels, preds)
    f1 = f1_score(true_labels, preds, average="macro")
    results.append({"Model": alias, "Accuracy": acc, "F1": f1})

    ensemble_probs_sum += model_probs  # –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π –∞–Ω—Å–∞–º–±–ª—å

# ---------- Soft-Voting Ensemble ----------
ensemble_probs_mean = ensemble_probs_sum / len(models_to_eval)
ensemble_preds = np.argmax(ensemble_probs_mean, axis=1)
ens_acc = accuracy_score(true_labels, ensemble_preds)
ens_f1 = f1_score(true_labels, ensemble_preds, average="macro")

# ---------- –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ ----------
results.append({"Model": "Ensemble (Soft-Voting)", "Accuracy": ens_acc, "F1": ens_f1})
df = pd.DataFrame(results).sort_values("Accuracy", ascending=False).reset_index(drop=True)
df.to_csv(OUT_DIR / "ensemble_vs_single.csv", index=False)

print("\nüìä Ensemble vs Single Models:")
display(df)

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="Accuracy", palette="Blues_d")
plt.xticks(rotation=30, ha="right")
plt.title("Accuracy: Ensemble vs Single Models")
plt.tight_layout()
plt.savefig(OUT_DIR / "ensemble_accuracy.png", dpi=300)
plt.show()

plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="F1", palette="Greens_d")
plt.xticks(rotation=30, ha="right")
plt.title("F1-Score: Ensemble vs Single Models")
plt.tight_layout()
plt.savefig(OUT_DIR / "ensemble_f1.png", dpi=300)
plt.show()

print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")
print("\n‚úÖ Block 13 completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 13B
#  Ensemble Contribution Analysis (model correlation & importance)
# ================================================================

import torch, timm, numpy as np, pandas as pd
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "ensemble_voting_analysis"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

paths = [x["path"] for x in all_images]
true_labels = [CLASSES.index(x["true"]) for x in all_images]

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π ----------
def get_model_probs(model, img_paths):
    model.eval()
    probs_all = []
    with torch.no_grad():
        for p in img_paths:
            img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
            probs_all.append(probs)
    return np.array(probs_all)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
prob_dict = {}
pred_dict = {}
for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias}")
        continue
    print(f"\n–ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    probs = get_model_probs(model, paths)
    prob_dict[alias] = probs
    pred_dict[alias] = np.argmax(probs, axis=1)

# ---------- –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ ----------
pred_df = pd.DataFrame(pred_dict)
corr_matrix = pred_df.corr(method="pearson")

plt.figure(figsize=(7,6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π")
plt.tight_layout()
plt.savefig(OUT_DIR / "model_prediction_correlation.png", dpi=300)
plt.show()

# ---------- –í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª—å ----------
# —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º accuracy –∞–Ω—Å–∞–º–±–ª—è –±–µ–∑ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ (leave-one-out)
ensemble_base = np.mean(np.stack(list(prob_dict.values())), axis=0)
base_acc = accuracy_score(true_labels, np.argmax(ensemble_base, axis=1))

contrib = []
for alias in prob_dict.keys():
    reduced_probs = [v for k, v in prob_dict.items() if k != alias]
    ensemble_reduced = np.mean(np.stack(reduced_probs), axis=0)
    acc_reduced = accuracy_score(true_labels, np.argmax(ensemble_reduced, axis=1))
    delta = base_acc - acc_reduced
    contrib.append({"Model": alias, "Œî_Accuracy": round(delta, 4)})

contrib_df = pd.DataFrame(contrib).sort_values("Œî_Accuracy", ascending=False)
contrib_df.to_csv(OUT_DIR / "ensemble_contribution.csv", index=False)

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∫–ª–∞–¥–∞ ----------
plt.figure(figsize=(9,5))
sns.barplot(contrib_df, x="Model", y="Œî_Accuracy", palette="viridis")
plt.xticks(rotation=30, ha="right")
plt.title("–í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è (Œî Accuracy)")
plt.tight_layout()
plt.savefig(OUT_DIR / "ensemble_contribution.png", dpi=300)
plt.show()

# ---------- –°–≤–æ–¥–∫–∞ ----------
print("\n –í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª—å (Œî Accuracy –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏):")
display(contrib_df)
print(f"\n–ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è: {base_acc:.3f}")

print("\n‚úÖ Block 13B completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 13C
#  Weighted Ensemble Voting based on Accuracy and Calibration (ECE)
# ================================================================

import torch, timm, numpy as np, pandas as pd
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "ensemble_weighted"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

paths = [x["path"] for x in all_images]
true_labels = [CLASSES.index(x["true"]) for x in all_images]

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π (–ø–æ ECE –∏ Accuracy) ----------
ECE_CSV = DRIVE_ROOT / "outputs" / "confidence_analysis_all" / "confidence_summary.csv"
ece_df = pd.read_csv(ECE_CSV)
print("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è:")
display(ece_df[["Model", "Accuracy", "ECE"]])

# —Å–æ–∑–¥–∞—ë–º –≤–µ—Å = Accuracy * (1 - ECE)
ece_df["Weight"] = ece_df["Accuracy"] * (1 - ece_df["ECE"])
weights_map = dict(zip(ece_df["Model"], ece_df["Weight"]))

# ---------- –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π ----------
def get_model_probs(model, img_paths):
    model.eval()
    probs_all = []
    with torch.no_grad():
        for p in img_paths:
            img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
            probs_all.append(probs)
    return np.array(probs_all)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
weighted_sum = np.zeros((len(paths), NUM_CLASSES))
results = []

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias}")
        continue

    print(f"\n–í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)

    probs = get_model_probs(model, paths)
    preds = np.argmax(probs, axis=1)
    acc = accuracy_score(true_labels, preds)
    f1 = f1_score(true_labels, preds, average="macro")

    weight_val = weights_map.get(alias, 0.0)
    weighted_sum += probs * weight_val

    results.append({
        "Model": alias,
        "Accuracy": round(acc, 3),
        "F1": round(f1, 3),
        "Weight": round(weight_val, 4)
    })

# ---------- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ –∞–Ω—Å–∞–º–±–ª—å ----------
total_weight = sum([r["Weight"] for r in results])
ensemble_probs = weighted_sum / total_weight
ensemble_preds = np.argmax(ensemble_probs, axis=1)

ens_acc = accuracy_score(true_labels, ensemble_preds)
ens_f1 = f1_score(true_labels, ensemble_preds, average="macro")

results.append({
    "Model": "Weighted Ensemble",
    "Accuracy": round(ens_acc, 3),
    "F1": round(ens_f1, 3),
    "Weight": 1.0
})

# ---------- –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
df = pd.DataFrame(results).sort_values("Accuracy", ascending=False).reset_index(drop=True)
df.to_csv(OUT_DIR / "weighted_ensemble_vs_single.csv", index=False)

print("\nüìä Weighted Ensemble vs Single Models:")
display(df)

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="Accuracy", palette="Blues_d")
plt.xticks(rotation=30, ha="right")
plt.title("Accuracy: Weighted Ensemble vs Single Models")
plt.tight_layout()
plt.savefig(OUT_DIR / "weighted_ensemble_accuracy.png", dpi=300)
plt.show()

plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="F1", palette="Greens_d")
plt.xticks(rotation=30, ha="right")
plt.title("F1-Score: Weighted Ensemble vs Single Models")
plt.tight_layout()
plt.savefig(OUT_DIR / "weighted_ensemble_f1.png", dpi=300)
plt.show()

print(f"\n‚úÖ Block 13C completed successfully. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 14
#  Grad-CAM on new 300-image test set (pred vs true, all 9 models)
# ================================================================

import os, cv2, time, random
import numpy as np
from pathlib import Path
from PIL import Image

import torch, timm
import torch.nn as nn
from torchvision import transforms
from torch.utils.data import DataLoader

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data"
TEST_ROOT  = DATA_DIR / "300img_test"          # –∫—É–¥–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω Test.zip (–∫–∞–∫ –≤ Block 9B)
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_new"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

# ---------- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –¥–µ–Ω–æ—Ä–º ----------
norm_mean = [0.485, 0.456, 0.406]
norm_std  = [0.229, 0.224, 0.225]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(norm_mean, norm_std)
])

def denorm(img_tensor: torch.Tensor) -> np.ndarray:
    mean = torch.tensor(norm_mean, device=img_tensor.device).view(3,1,1)
    std  = torch.tensor(norm_std,  device=img_tensor.device).view(3,1,1)
    img = (img_tensor * std + mean).clamp(0,1).detach().cpu().numpy()
    img = np.transpose(img, (1,2,0))
    return (img * 255).astype(np.uint8)

# ---------- –°–∫–∞–Ω–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_items = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for p in fdir.glob(ext):
            all_items.append({"path": p, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_items))

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth",             "MobileNetV3"),
    ("resnet50",              "resnet50_best.pth",                           "ResNet50"),
    ("efficientnet_b0",       "efficientnet_b0_frozen_best.pth",            "EfficientNet-B0"),
    ("convnext_tiny",         "convnext_tiny_stage1_best.pth",               "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny",         "convnext_tiny_stage2_best.pth",               "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth",      "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18",              "resnet18_fewshot_best.pth",                   "ResNet18 FewShot4ep"),
    ("resnet18",              "resnet18_fewshot_12ep_best.pth",              "ResNet18 FewShot12ep"),
]

# ---------- –¢–µ—Ö. —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è Grad-CAM ----------
def disable_inplace_activations(model: nn.Module):
    for m in model.modules():
        if isinstance(m, (nn.ReLU, nn.ReLU6, nn.Hardswish)):
            if hasattr(m, "inplace") and m.inplace:
                m.inplace = False

def find_last_conv_layer(model: nn.Module) -> nn.Module:
    last_conv = None
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            last_conv = module
    if last_conv is None:
        raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π –¥–ª—è Grad-CAM.")
    return last_conv

class SimpleGradCAM:
    def __init__(self, model: nn.Module, target_layer: nn.Module):
        self.model = model
        self.target_layer = target_layer
        self.activations = None
        self.gradients = None
        self.hook_a = target_layer.register_forward_hook(self._hook_activations)
        self.hook_g = target_layer.register_full_backward_hook(self._hook_gradients)

    def _hook_activations(self, module, inp, out):
        self.activations = out

    def _hook_gradients(self, module, grad_in, grad_out):
        self.gradients = grad_out[0]

    def remove(self):
        try:
            self.hook_a.remove()
            self.hook_g.remove()
        except:
            pass

    def __call__(self, logits: torch.Tensor, class_idx: int) -> np.ndarray:
        self.model.zero_grad(set_to_none=True)
        score = logits[0, class_idx]
        score.backward(retain_graph=True)

        acts = self.activations
        grads = self.gradients
        if acts is None:
            raise RuntimeError("–ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã ‚Äî hook –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª.")
        if grads is None or grads.abs().sum() == 0:
            # –§–æ–ª–ª–±–µ–∫: —Å—Ä–µ–¥–Ω—è—è –∫–∞—Ä—Ç–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π
            cam = acts.mean(dim=1, keepdim=True)
        else:
            weights = grads.mean(dim=(2,3), keepdim=True)      # [B,C,1,1]
            cam = (weights * acts).sum(dim=1, keepdim=True)    # [B,1,H,W]

        cam = torch.relu(cam)[0,0].detach().cpu().numpy()
        if cam.max() > cam.min():
            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
        else:
            cam = np.zeros_like(cam)
        return cam

def overlay(rgb_uint8: np.ndarray, cam_2d: np.ndarray, alpha: float = 0.35) -> np.ndarray:
    h, w = rgb_uint8.shape[:2]
    heat = cv2.applyColorMap((cam_2d * 255).astype(np.uint8), cv2.COLORMAP_JET)
    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)
    heat = cv2.resize(heat, (w, h))
    out = (alpha * heat + (1 - alpha) * rgb_uint8).astype(np.uint8)
    return out

softmax = nn.Softmax(dim=1)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª: –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ ‚Äî CAM –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ----------
for model_name, weight_file, alias in models_to_eval:
    wpath = MODELS_DIR / weight_file
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞—é {alias}: –Ω–µ—Ç —Ñ–∞–π–ª–∞ –≤–µ—Å–æ–≤ {weight_file}")
        continue

    print(f"\nGrad-CAM –¥–ª—è {alias}")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    # –î–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ CAM —Ä–∞–∑—Ä–µ—à–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–µ–∑–¥–µ
    for p in model.parameters():
        p.requires_grad = True
    model.eval()
    disable_inplace_activations(model)

    # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π conv-—Å–ª–æ–π
    target_layer = find_last_conv_layer(model)
    cam_explainer = SimpleGradCAM(model, target_layer)

    # –ü–∞–ø–∫–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
    out_dir = OUT_DIR / alias.replace(" ", "_")
    out_dir.mkdir(parents=True, exist_ok=True)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    for i, item in enumerate(all_items):
        img_p = item["path"]
        true_name = item["true"]
        true_idx = CLASSES.index(true_name)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–Ω–∑–æ—Ä–∞
        img_pil = Image.open(img_p).convert("RGB")
        x = tfm(img_pil).unsqueeze(0).to(DEVICE)

        with torch.enable_grad():
            logits = model(x)
        probs = softmax(logits)[0].detach().cpu().numpy()
        pred_idx = int(np.argmax(probs))
        pred_name = CLASSES[pred_idx]
        conf_pred = float(probs[pred_idx])

        # CAM –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤
        try:
            cam_pred = cam_explainer(logits, pred_idx)
        except Exception as e:
            cam_pred = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)

        try:
            cam_true = cam_explainer(logits, true_idx)
        except Exception as e:
            cam_true = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)

        # –°–æ–±–∏—Ä–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        img_rgb = denorm(x[0])
        left  = overlay(img_rgb, cam_pred)
        right = overlay(img_rgb, cam_true)

        # –ü–æ–¥–ø–∏—Å–∏
        left  = cv2.putText(left.copy(),
                            f"Pred: {pred_name}  p={conf_pred:.2f}",
                            (10, 24), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
        right = cv2.putText(right.copy(),
                            f"True: {true_name}",
                            (10, 24), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)

        # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        sep = np.full((left.shape[0], 6, 3), 255, dtype=np.uint8)
        concat = np.concatenate([left, sep, right], axis=1)

        # –ò–º—è —Ñ–∞–π–ª–∞ ‚Äî –∏–Ω–¥–µ–∫—Å—ã –∏ –∫–ª–∞—Å—Å—ã
        fname = f"idx{i:04d}__pred-{pred_name}__true-{true_name}__p{conf_pred:.2f}.jpg"
        cv2.imwrite(str(out_dir / fname), cv2.cvtColor(concat, cv2.COLOR_RGB2BGR))

    cam_explainer.remove()
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {out_dir}")

print("\n‚úÖ Block 14 completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 14B
#  Aggregated Grad-CAM Collages per Model √ó Class
# ================================================================

import cv2, os, random
import numpy as np
from pathlib import Path
from math import ceil, sqrt

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
SRC_ROOT   = DRIVE_ROOT / "outputs" / "gradcam_new"     # –∏—Å—Ö–æ–¥–Ω—ã–µ Grad-CAM —Ñ–∞–π–ª—ã –∏–∑ Block 14
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_collages"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]
GRID_SIZE = 6              # 6x6 = 36 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π; –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å 5
IMG_SIZE = 224             # –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä (–∏–∑ Grad-CAM)
CELL_PAD = 4               # –æ—Ç—Å—Ç—É–ø –º–µ–∂–¥—É —è—á–µ–π–∫–∞–º–∏
random.seed(42)

# ---------- –§—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–ª–ª–∞–∂–∞ ----------
def build_collage(img_paths, save_path, grid=6):
    if len(img_paths) == 0:
        print(f"  –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è {save_path.name}")
        return
    # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º grid^2
    img_paths = img_paths[:grid*grid]
    imgs = []
    for p in img_paths:
        img = cv2.imread(str(p))
        if img is None: continue
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        imgs.append(img)

    if len(imgs) == 0:
        print(f"  –ü—É—Å—Ç–æ–π –Ω–∞–±–æ—Ä –¥–ª—è {save_path.name}")
        return

    rows, cols = grid, grid
    pad = CELL_PAD
    h = w = IMG_SIZE
    canvas = np.ones(((h+pad)*rows+pad, (w+pad)*cols+pad, 3), dtype=np.uint8)*255

    for idx, img in enumerate(imgs):
        r = idx // cols
        c = idx % cols
        y0, x0 = pad + r*(h+pad), pad + c*(w+pad)
        canvas[y0:y0+h, x0:x0+w] = img

    cv2.imwrite(str(save_path), canvas)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
model_dirs = sorted([p for p in SRC_ROOT.iterdir() if p.is_dir()])
print(f"–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_dirs)}")

for model_dir in model_dirs:
    model_name = model_dir.name
    out_sub = OUT_DIR / model_name
    out_sub.mkdir(parents=True, exist_ok=True)
    print(f"\n–°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–∞–∂–∏ –¥–ª—è {model_name} ...")

    # —Å–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ –∫–ª–∞—Å—Å–∞–º
    all_imgs = list(model_dir.glob("*.jpg"))
    for cls in CLASSES:
        # —Ñ–∏–ª—å—Ç—Ä –ø–æ true-–∫–ª–∞—Å—Å—É
        cls_imgs = [p for p in all_imgs if f"true-{cls}" in p.name]
        if len(cls_imgs) == 0:
            continue
        # —á—Ç–æ–±—ã —É –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –±—ã–ª –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –ø–æ—Ä—è–¥–æ–∫ ‚Äî —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        cls_imgs = sorted(cls_imgs)[:GRID_SIZE*GRID_SIZE]
        save_path = out_sub / f"collage_{cls}.jpg"
        build_collage(cls_imgs, save_path, GRID_SIZE)
        print(f"  ‚úÖ {cls}: {len(cls_imgs)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ‚Üí {save_path.name}")

print(f"\n‚úÖ Block 14B completed successfully. –ö–æ–ª–ª–∞–∂–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 14C
#  Cross-Model Comparative Wall (9 models √ó 3 classes)
# ================================================================

import cv2, numpy as np
from pathlib import Path

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
SRC_ROOT   = DRIVE_ROOT / "outputs" / "gradcam_collages"     # –∏–∑ Block 14B
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_wall"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]
CELL_PAD = 8
BG_COLOR = (255,255,255)
LABEL_BG = (240,240,240)
LABEL_COLOR = (0,0,0)
FONT = cv2.FONT_HERSHEY_SIMPLEX
FONT_SCALE = 0.7
FONT_THICK = 2

# ---------- –°–∫–∞–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ ----------
model_dirs = sorted([p for p in SRC_ROOT.iterdir() if p.is_dir()])
model_names = [p.name for p in model_dirs]
print(f"–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_dirs)}")

# ---------- –ß—Ç–µ–Ω–∏–µ –∫–æ–ª–ª–∞–∂–µ–π ----------
all_data = {cls: [] for cls in CLASSES}
for cls in CLASSES:
    for mdir in model_dirs:
        fpath = mdir / f"collage_{cls}.jpg"
        if fpath.exists():
            img = cv2.imread(str(fpath))
            all_data[cls].append(img)
        else:
            # –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–ª–ª–∞–∂–∞ ‚Äî –∑–∞–≥–ª—É—à–∫–∞
            if len(all_data[cls]) > 0:
                h,w,_ = all_data[cls][0].shape
            else:
                h,w = 224,224
            blank = np.ones((h,w,3), dtype=np.uint8)*255
            cv2.putText(blank, "N/A", (60,120), FONT, 1, (0,0,255), 2)
            all_data[cls].append(blank)

# ---------- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ç–µ–Ω—ã ----------
rows = []
for cls in CLASSES:
    imgs = all_data[cls]
    h,w,_ = imgs[0].shape
    # –ø–æ–¥–ø–∏—Å–∏ –º–æ–¥–µ–ª–µ–π —Å–≤–µ—Ä—Ö—É
    row_with_labels = []
    for i,img in enumerate(imgs):
        label = model_names[i].replace("_"," ").split("/")[-1]
        label_img = np.ones((40, w, 3), dtype=np.uint8)*np.array(LABEL_BG, dtype=np.uint8)
        text_size = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICK)[0]
        cv2.putText(label_img, label, ((w - text_size[0])//2, 28), FONT, FONT_SCALE, LABEL_COLOR, FONT_THICK)
        combo = np.vstack([label_img, img])
        row_with_labels.append(combo)
    # —Å–∫–ª–µ–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É
    row_concat = cv2.hconcat(row_with_labels)
    # –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞ —Å–ª–µ–≤–∞
    class_label = np.ones((row_concat.shape[0], 120, 3), dtype=np.uint8)*np.array(LABEL_BG, dtype=np.uint8)
    cv2.putText(class_label, cls.upper(), (15, row_concat.shape[0]//2), FONT, 1.2, LABEL_COLOR, 3)
    rows.append(cv2.hconcat([class_label, row_concat]))

# ---------- –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø–æ–ª–æ—Ç–Ω–æ ----------
wall = cv2.vconcat(rows)
cv2.imwrite(str(OUT_DIR / "gradcam_wall_all_models.jpg"), wall)

print(f"\n‚úÖ Block 14C completed successfully.")
print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {OUT_DIR / 'gradcam_wall_all_models.jpg'}")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 15
#  Efficiency Benchmark (CPU/GPU) ‚Äî FPS, latency, complexity
# ================================================================

import torch, timm, time, numpy as np, pandas as pd
import matplotlib.pyplot as plt, seaborn as sns
from pathlib import Path
from torchvision import transforms
from PIL import Image
from tqdm import tqdm

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "efficiency_benchmark"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)
DEVICE_GPU = "cuda" if torch.cuda.is_available() else None
DEVICE_CPU = "cpu"

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# –ë–µ—Ä—ë–º 20 —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏–π
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_paths = []
for folder in folders:
    for ext in ("*.jpg","*.jpeg","*.png"):
        all_paths += list((TEST_ROOT / "Test" / folder).glob(ext))
np.random.seed(42)
test_imgs = np.random.choice(all_paths, size=min(20, len(all_paths)), replace=False)
print(f"–í—ã–±—Ä–∞–Ω–æ {len(test_imgs)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞.")

# ---------- –ú–æ–¥–µ–ª–∏ ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------
def benchmark_model(model, device, warmup=5, repeat=50):
    # –ü—Ä–æ–≥–æ–Ω—è–µ–º –æ–¥–Ω—É –∫–∞—Ä—Ç–∏–Ω–∫—É –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)
    model(dummy)
    times = []
    for _ in range(warmup): model(dummy)
    torch.cuda.synchronize() if device=="cuda" else None
    for _ in range(repeat):
        start = time.time()
        model(dummy)
        torch.cuda.synchronize() if device=="cuda" else None
        times.append(time.time() - start)
    avg_t = np.mean(times)
    fps = 1.0 / avg_t
    return avg_t, fps

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
results = []

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias} ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ –≤–µ—Å–æ–≤.")
        continue

    print(f"\n–¢–µ—Å—Ç–∏—Ä—É–µ–º {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location="cpu"))
    model.eval()

    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ FLOPs ---
    params = sum(p.numel() for p in model.parameters()) / 1e6
    try:
        from torchprofile import profile_macs
        macs = profile_macs(model, torch.randn(1,3,IMG_SIZE,IMG_SIZE)) / 1e9
    except Exception:
        macs = np.nan

    # --- GPU –±–µ–Ω—á ---
    if DEVICE_GPU:
        model_gpu = model.to(DEVICE_GPU)
        t_gpu, fps_gpu = benchmark_model(model_gpu, DEVICE_GPU)
    else:
        t_gpu, fps_gpu = np.nan, np.nan

    # --- CPU –±–µ–Ω—á ---
    model_cpu = model.to(DEVICE_CPU)
    t_cpu, fps_cpu = benchmark_model(model_cpu, DEVICE_CPU)

    results.append({
        "Model": alias,
        "Params (M)": round(params,2),
        "MACs (G)": round(macs,2) if not np.isnan(macs) else "‚Äî",
        "Time per Image (GPU, s)": round(t_gpu,4),
        "FPS (GPU)": round(fps_gpu,1),
        "Time per Image (CPU, s)": round(t_cpu,4),
        "FPS (CPU)": round(fps_cpu,1),
    })

df = pd.DataFrame(results)
df.to_csv(OUT_DIR / "efficiency_results.csv", index=False)
print("\n–¢–∞–±–ª–∏—Ü–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:")
display(df)

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
plt.figure(figsize=(8,6))
sns.scatterplot(df, x="Params (M)", y="FPS (GPU)", s=120, hue="Model", style="Model")
plt.title("Speed vs Model Complexity (GPU)")
plt.xlabel("Model Parameters (millions)")
plt.ylabel("FPS (higher = faster)")
plt.tight_layout()
plt.savefig(OUT_DIR / "speed_vs_complexity_gpu.png", dpi=300)
plt.show()

plt.figure(figsize=(8,6))
sns.scatterplot(df, x="Params (M)", y="FPS (CPU)", s=120, hue="Model", style="Model")
plt.title("Speed vs Model Complexity (CPU)")
plt.xlabel("Model Parameters (millions)")
plt.ylabel("FPS (higher = faster)")
plt.tight_layout()
plt.savefig(OUT_DIR / "speed_vs_complexity_cpu.png", dpi=300)
plt.show()

print(f"\n‚úÖ Block 15 completed successfully.")
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 16
#  Out-of-Domain / Bias Test (Generalization on unseen data)
# ================================================================

import zipfile, shutil, random, os
import torch, timm
import numpy as np, pandas as pd
import matplotlib.pyplot as plt, seaborn as sns
from torchvision import transforms
from pathlib import Path
from PIL import Image
from tqdm import tqdm

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
ZIP_PATH   = DRIVE_ROOT / "16block" / "16block.zip"
OOD_ROOT   = DRIVE_ROOT / "16block" / "ood_test"
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs" / "ood_bias_test"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ ----------
if OOD_ROOT.exists():
    shutil.rmtree(OOD_ROOT)
with zipfile.ZipFile(ZIP_PATH, "r") as zf:
    zf.extractall(DRIVE_ROOT / "16block")  # —Ä–∞—Å–ø–∞–∫—É–µ–º —á—É—Ç—å –≤—ã—à–µ
# –ø—Ä–æ–≤–µ—Ä–∏–º, –≥–¥–µ –ª–µ–∂–∞—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
possible_root = DRIVE_ROOT / "16block" / "16block"
if (possible_root.exists() and any(possible_root.iterdir())):
    shutil.move(str(possible_root), str(OOD_ROOT))
print("  –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤", OOD_ROOT)


# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
all_images = []
for folder in OOD_ROOT.iterdir():
    if folder.is_dir():
        label = folder.name
        for ext in ("*.jpg","*.jpeg","*.png"):
            for p in folder.glob(ext):
                all_images.append({"path": p, "ood_label": label})
random.shuffle(all_images)
print(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)} –≤ {len(set(x['ood_label'] for x in all_images))} –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö.")

# ---------- –ú–æ–¥–µ–ª–∏ ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ----------
def get_prediction(model, img):
    x = tfm(img).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        logits = model(x)
        probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
    pred_idx = int(np.argmax(probs))
    return CLASSES[pred_idx], probs

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
results = []
for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤.")
        continue

    print(f"\n–ü—Ä–æ–≥–æ–Ω—è–µ–º {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    preds, probs_all = [], []
    for item in tqdm(all_images):
        img = Image.open(item["path"]).convert("RGB")
        pred_class, probs = get_prediction(model, img)
        preds.append(pred_class)
        probs_all.append(probs)

    df_model = pd.DataFrame({
        "image": [x["path"].name for x in all_images],
        "ood_label": [x["ood_label"] for x in all_images],
        "pred": preds,
    })
    df_model[["p_real", "p_drawing", "p_generated"]] = np.array(probs_all)
    df_model["model"] = alias
    results.append(df_model)

# ---------- –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ----------
df = pd.concat(results, ignore_index=True)
out_csv = OUT_DIR / "ood_predictions.csv"
df.to_csv(out_csv, index=False)
print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {out_csv}")

# ---------- –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –∫–ª–∞—Å—Å–æ–≤ ----------
summary = (
    df.groupby(["model","ood_label","pred"])
    .size().unstack(fill_value=0)
    .apply(lambda x: x / x.sum(), axis=1)
)
summary.to_csv(OUT_DIR / "ood_summary.csv")
print("\n–°–≤–æ–¥–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º (–ø–æ –¥–æ–ª—è–º):")
display(summary.head(12))

# ---------- –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è ----------
plt.figure(figsize=(10,6))
sns.heatmap(summary, annot=True, fmt=".2f", cmap="Blues", cbar=True)
plt.title("Out-of-Domain Predictions ‚Äî Class Distribution per Model")
plt.tight_layout()
plt.savefig(OUT_DIR / "ood_heatmap.png", dpi=300)
plt.show()

# ---------- –í—ã–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –æ—à–∏–±–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π ----------
sample_dir = OUT_DIR / "ood_samples"
sample_dir.mkdir(exist_ok=True)
sampled = df.sample(30, random_state=42)
for _, row in sampled.iterrows():
    src = Path(OOD_ROOT) / row["ood_label"] / row["image"]
    if not src.exists(): continue
    dst_name = f"{row['model'][:20]}__{row['ood_label']}__{row['pred']}.jpg"
    shutil.copy(src, sample_dir / dst_name)
print(f"\n–ü—Ä–∏–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {sample_dir}")

print("\n‚úÖ Block 16 completed successfully.")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 17
#  Error & Bias Analysis on Out-of-Domain Predictions
# ================================================================

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
OUT_DIR    = DRIVE_ROOT / "outputs" / "ood_bias_test"
PRED_CSV   = OUT_DIR / "ood_predictions.csv"
BIAS_DIR   = DRIVE_ROOT / "outputs" / "error_bias_analysis"
BIAS_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]

# ---------- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
df = pd.read_csv(PRED_CSV)
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ {PRED_CSV.name}")

# ---------- 2. –ú–∞—Ç—Ä–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ----------
conf_mat = (
    df.groupby(["ood_label","pred"])
    .size().unstack(fill_value=0)
    .apply(lambda x: x / x.sum(), axis=1)
)
plt.figure(figsize=(8,6))
sns.heatmap(conf_mat, annot=True, fmt=".2f", cmap="Blues")
plt.title("Confusion Matrix: OOD Category vs Predicted Class")
plt.xlabel("Predicted Class")
plt.ylabel("OOD Category")
plt.tight_layout()
plt.savefig(BIAS_DIR / "ood_confusion_matrix.png", dpi=300)
plt.show()

# ---------- 3. –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ----------
conf_cols = ["p_real","p_drawing","p_generated"]
df["max_conf"] = df[conf_cols].max(axis=1)
df["is_correct"] = df["pred"] == "real"  # –Ω–µ —Å–æ–≤—Å–µ–º ground truth, –Ω–æ —É—Å–ª–æ–≤–Ω–æ "real" —Å—á–∏—Ç–∞–µ–º –±–∞–∑–æ–≤—ã–º
plt.figure(figsize=(8,5))
sns.histplot(data=df, x="max_conf", hue="pred", multiple="stack", bins=20, palette="Set2")
plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º (OOD)")
plt.xlabel("Softmax Confidence")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig(BIAS_DIR / "ood_confidence_distribution.png", dpi=300)
plt.show()

# ---------- 4. –ê–Ω–∞–ª–∏–∑ —Å–º–µ—â–µ–Ω–∏–π (bias) ----------
bias_summary = (
    df.groupby(["model","pred"])
    .size()
    .unstack(fill_value=0)
    .apply(lambda x: x / x.sum(), axis=1)
)
bias_summary["dominant_class"] = bias_summary.idxmax(axis=1)
bias_summary.to_csv(BIAS_DIR / "model_bias_summary.csv")
print("\n–°–≤–æ–¥–∫–∞ bias-–º–æ–¥–µ–ª–µ–π (–¥–æ–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞):")
display(bias_summary)

plt.figure(figsize=(10,5))
sns.heatmap(bias_summary[CLASSES], annot=True, cmap="Purples", fmt=".2f")
plt.title("Bias Map ‚Äî Class Prediction Distribution per Model")
plt.xlabel("Predicted Class")
plt.ylabel("Model")
plt.tight_layout()
plt.savefig(BIAS_DIR / "bias_map_models.png", dpi=300)
plt.show()

# ---------- 5. –û—Ü–µ–Ω–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ----------
model_conf = (
    df.groupby("model")["max_conf"]
    .agg(["mean","std"])
    .rename(columns={"mean":"mean_conf","std":"std_conf"})
)
model_conf["confidence_stability"] = 1 - model_conf["std_conf"]
model_conf.to_csv(BIAS_DIR / "confidence_stability.csv")

plt.figure(figsize=(8,5))
sns.barplot(model_conf, x=model_conf.index, y="mean_conf", palette="viridis")
plt.xticks(rotation=30, ha="right")
plt.title("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ OOD")
plt.tight_layout()
plt.savefig(BIAS_DIR / "mean_confidence_per_model.png", dpi=300)
plt.show()

print("\n‚úÖ Block 17 completed successfully.")
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {BIAS_DIR}")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 18 (Stable CPU version)
#  Final Integrated Report (robust version with auto column detection)
# ================================================================

import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
OUT_DIR    = DRIVE_ROOT / "outputs"
REPORT_DIR = OUT_DIR / "final_integrated_report"
REPORT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –§–∞–π–ª—ã ----------
files = {
    "blind_eval"   : OUT_DIR / "vk_blind_eval" / "vk_blind_eval_results.csv",
    "robust"       : OUT_DIR / "robustness_test" / "robustness_results.csv",
    "calibration"  : OUT_DIR / "confidence_analysis_all" / "confidence_summary.csv",
    "ensemble"     : OUT_DIR / "ensemble_voting" / "ensemble_vs_single.csv",
    "efficiency"   : OUT_DIR / "efficiency_benchmark" / "efficiency_results.csv",
    "bias"         : OUT_DIR / "error_bias_analysis" / "model_bias_summary.csv",
    "cluster"      : OUT_DIR / "feature_analysis" / "cluster_separation_summary.csv",
}

dfs = {}
for k, path in files.items():
    if path.exists():
        try:
            dfs[k] = pd.read_csv(path)
            print(f"‚úÖ {k}: {len(dfs[k])} —Å—Ç—Ä–æ–∫")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {k}: {e}")
    else:
        print(f"‚õî –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

# ---------- 2. –°–≤–æ–¥ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ ----------
summary = {}

# --- Blind eval ---
if "blind_eval" in dfs:
    be = dfs["blind_eval"]
    model_cols = [c for c in be.columns if c not in ["image", "true_label"]]
    for m in model_cols:
        acc = np.mean(be[m] == be["true_label"])
        summary.setdefault(m, {})["Accuracy@300"] = round(acc, 3)

# --- Robustness ---
if "robust" in dfs:
    rd = dfs["robust"]
    # –ü–æ–ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –º–æ–¥–µ–ª—å—é
    model_col = next((c for c in rd.columns if "model" in c.lower()), None)
    if not model_col:
        rd.insert(0, "Model", [f"Model_{i}" for i in range(len(rd))])
        model_col = "Model"

    # –ü—Ä–æ–≤–µ—Ä–∏–º –∏ —Å–æ–∑–¥–∞–¥–∏–º mean_distortion_acc
    distortions = [c for c in rd.columns if c in ["noise","blur","brightness","rotation","jpeg"]]
    if distortions and "mean_distortion_acc" not in rd.columns:
        rd["mean_distortion_acc"] = rd[distortions].mean(axis=1)

    for _, r in rd.iterrows():
        summary.setdefault(r[model_col], {})["Robustness"] = round(r.get("mean_distortion_acc", np.nan), 3)

# --- Calibration ---
if "calibration" in dfs:
    cd = dfs["calibration"]
    for _, r in cd.iterrows():
        model = r.get("Model", r.get("–ú–æ–¥–µ–ª—å", f"Model_{_}"))
        summary.setdefault(model, {})["ECE"] = round(r.get("ECE", np.nan), 3)
        summary[model]["MeanConf"] = round(r.get("Mean_Confidence", np.nan), 3)

# --- Ensemble ---
if "ensemble" in dfs:
    ed = dfs["ensemble"]
    for _, r in ed.iterrows():
        model = str(r.get("Model", f"Model_{_}"))
        summary.setdefault(model, {})["Accuracy"] = r.get("Accuracy", np.nan)
        summary[model]["F1"] = r.get("F1", np.nan)

# --- Efficiency ---
if "efficiency" in dfs:
    ef = dfs["efficiency"]
    for _, r in ef.iterrows():
        model = str(r.get("Model", f"Model_{_}"))
        summary.setdefault(model, {})["FPS_GPU"] = r.get("FPS (GPU)", np.nan)
        summary[model]["Params(M)"] = r.get("Params (M)", np.nan)

# --- Bias ---
if "bias" in dfs:
    bf = dfs["bias"]
    model_col = next((c for c in bf.columns if "model" in c.lower()), None)
    for _, r in bf.iterrows():
        model = r.get(model_col, f"Model_{_}")
        classes = [c for c in ["real","drawing","generated"] if c in bf.columns]
        if classes:
            summary.setdefault(model, {})["Unbias"] = 1 - r[classes].max()

# --- Cluster separation ---
if "cluster" in dfs:
    cf = dfs["cluster"]
    model_col = next((c for c in cf.columns if "model" in c.lower()), None)
    for _, r in cf.iterrows():
        model = r.get(model_col, f"Model_{_}")
        summary.setdefault(model, {})["Silhouette"] = r.get("Silhouette", np.nan)
        summary[model]["SeparationRatio"] = r.get("SeparationRatio", np.nan)

# ---------- 3. –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è ----------
final_df = pd.DataFrame(summary).T.reset_index().rename(columns={"index": "Model"})
final_df.to_csv(REPORT_DIR / "final_integrated_metrics.csv", index=False)
print(f"\n‚úÖ –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {REPORT_DIR / 'final_integrated_metrics.csv'}")

# ---------- 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ----------
norm = final_df.copy()
for c in norm.columns:
    if c == "Model": continue
    if norm[c].dtype.kind in "biufc":
        norm[c] = (norm[c] - norm[c].min()) / (norm[c].max() - norm[c].min() + 1e-9)
norm["IntegratedScore"] = norm.select_dtypes("number").drop(columns="Params(M)", errors="ignore").mean(axis=1)
norm = norm.sort_values("IntegratedScore", ascending=False)
norm.to_csv(REPORT_DIR / "final_integrated_metrics_normalized.csv", index=False)

# ---------- 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ ----------
plt.figure(figsize=(10,6))
sns.barplot(norm, x="IntegratedScore", y="Model", palette="viridis")
plt.title("Integrated Model Score (normalized metrics)")
plt.xlabel("Integrated Score (0‚Äì1)")
plt.ylabel("Model")
plt.tight_layout()
plt.savefig(REPORT_DIR / "bar_integrated_score.png", dpi=300)
plt.show()

if {"Accuracy@300","ECE"}.issubset(final_df.columns):
    plt.figure(figsize=(7,6))
    sns.scatterplot(final_df, x="ECE", y="Accuracy@300", hue="Model", s=120)
    plt.title("Calibration vs Accuracy")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(REPORT_DIR / "scatter_ece_accuracy.png", dpi=300)
    plt.show()

if {"Robustness","Unbias"}.issubset(final_df.columns):
    plt.figure(figsize=(7,6))
    sns.scatterplot(final_df, x="Robustness", y="Unbias", hue="Model", s=120)
    plt.title("Robustness‚ÄìBias Trade-off")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(REPORT_DIR / "scatter_robustness_bias.png", dpi=300)
    plt.show()

# ---------- 6. –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã ----------
best_model = norm.iloc[0]
summary_text = f"""# Avatar Type Recognition ‚Äî Final Integrated Report
**–î–∞—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M')}

## –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–ù–∞–∏–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å:** {best_model['Model']}
  Integrated Score = {best_model['IntegratedScore']:.3f}

## –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
- –ú–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–∏–º ECE –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—É—é –∫–∞–ª–∏–±—Ä–æ–≤–∫—É.
- –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º ("Robustness") –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (Silhouette/Separation).
- –≠–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–∞—ë—Ç –æ—â—É—Ç–∏–º—ã–π –ø—Ä–∏—Ä–æ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–¥–∏–Ω–æ—á–Ω—ã–º–∏.
- MobileNetV3 FewShot –æ—Å—Ç–∞—ë—Ç—Å—è –ª—É—á—à–∏–º –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–º –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏.
- –ú–æ–¥–µ–ª–∏ —Å –≤—ã—Å–æ–∫–∏–º Unbias (>0.6) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ª—É—á—à—É—é –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –≤–Ω–µ –¥–æ–º–µ–Ω–∞.

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
–í—Å–µ –æ—Ç—á—ë—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ `{REPORT_DIR}`.
"""

(REPORT_DIR / "FINAL_REPORT.md").write_text(summary_text, encoding="utf-8")
print(summary_text)
print("\n‚úÖ Block 18 (Stable) completed successfully.")