# -*- coding: utf-8 -*-
"""Avatar_Type_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IFYsefy8tPpA2mwUu8kQXbjaFZy3h8-M
"""

from google.colab import drive
drive.mount('/content/drive')

# ================================================================
#  Avatar Type Recognition ‚Äî BLOCK 1 (FIXED VERSION)
#  Environment setup with dependency conflict resolution
# ================================================================

# –°–Ω–∞—á–∞–ª–∞ –æ–±–Ω–æ–≤–ª—è–µ–º pip –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Ä—Å–∏–∏
!pip install -q --upgrade pip
!pip install -q --upgrade sympy==1.13.3

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch —Å CUDA 12.6
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Ä—Å–∏–∏ –ø–∞–∫–µ—Ç–æ–≤
!pip install -q timm==1.0.9 albumentations==1.3.1 opencv-python==4.10.0.84 \
                 scikit-learn==1.6.0 matplotlib==3.9.2 pandas==2.2.2 seaborn==0.13.2 \
                 tqdm==4.67.0 umap-learn==0.5.9

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º scikit-learn –¥–æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –≤–µ—Ä—Å–∏–∏
!pip install -q --force-reinstall scikit-learn==1.6.0

import os, sys, random, shutil, json
from pathlib import Path
import torch

print("PyTorch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
else:
    print(" CUDA –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ ‚Äî –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU (–¥–æ–ø—É—Å—Ç–∏–º–æ –¥–ª—è —Ç–µ—Å—Ç–∞).")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    import timm, albumentations, cv2, sklearn, matplotlib, pandas as pd, seaborn as sns
    from tqdm import tqdm

    print("\n Libraries imported successfully:")
    print("timm:", timm.__version__)
    print("albumentations:", albumentations.__version__)
    print("opencv:", cv2.__version__)
    print("sklearn:", sklearn.__version__)

except ImportError as e:
    print(f" Import error: {e}")
    print(" Installing missing packages...")
    !pip install -q scikit-learn==1.6.0
    import sklearn
    print(" Fixed - scikit-learn version:", sklearn.__version__)

# -------------------- 2. –§–∏–∫—Å–∞—Ü–∏—è —Å–∏–¥–æ–≤ --------------------
import numpy as np
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
print(" Seeds fixed (deterministic mode ON)")

# -------------------- 3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ --------------------
WORK_ROOT = Path("/content/avatar_recog")
RAW_ROOT  = WORK_ROOT / "data" / "raw"
PROC_ROOT = WORK_ROOT / "data" / "processed"

DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs"
LOGS_DIR   = DRIVE_ROOT / "logs"

CLASSES = ["real", "drawing", "generated"]
IMG_SIZE = 224
print("Classes:", CLASSES, "| Image size:", IMG_SIZE)

# -------------------- 4. –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ --------------------
for p in [WORK_ROOT, RAW_ROOT, PROC_ROOT, DRIVE_ROOT, MODELS_DIR, OUT_DIR, LOGS_DIR]:
    p.mkdir(parents=True, exist_ok=True)

if PROC_ROOT.exists():
    shutil.rmtree(PROC_ROOT)
for split in ["train", "val", "test"]:
    for c in CLASSES:
        (PROC_ROOT / split / c).mkdir(parents=True, exist_ok=True)

# -------------------- 5. –£—Ç–∏–ª–∏—Ç—ã --------------------
def print_tree(root: Path, levels: int = 3):
    root = Path(root)
    print(f"\n== TREE: {root} ==")
    for path in sorted(root.rglob("*")):
        rel = path.relative_to(root)
        if len(rel.parts) <= levels:
            print(("üìÅ " if path.is_dir() else "üìÑ ") + str(rel))

def save_config():
    cfg = {
        "seed": SEED,
        "img_size": IMG_SIZE,
        "classes": CLASSES,
        "work_root": str(WORK_ROOT),
        "raw_root": str(RAW_ROOT),
        "proc_root": str(PROC_ROOT),
        "drive_root": str(DRIVE_ROOT),
        "models_dir": str(MODELS_DIR),
        "outputs_dir": str(OUT_DIR),
        "logs_dir": str(LOGS_DIR),
        "torch_version": torch.__version__,
        "cuda": torch.cuda.is_available(),
    }
    cfg_path = WORK_ROOT / "config.json"
    cfg_path.write_text(json.dumps(cfg, indent=2, ensure_ascii=False))
    print("Config saved:", cfg_path)

save_config()
print_tree(WORK_ROOT, levels=4)
print_tree(DRIVE_ROOT, levels=3)

print("\n‚úÖ Block 1 completed successfully.")
print("Environment, seeds, folder structure, and config.json are ready.")

"""Block 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞.
–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫ PyTorch, timm, albumentations, OpenCV, sklearn, pandas, matplotlib –∏ –¥—Ä—É–≥–∏—Ö.
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA, –ø—Ä–∏ –µ—ë –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–∞—Å—á—ë—Ç—ã –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –Ω–∞ CPU.
–§–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –≤—Å–µ —Å–ª—É—á–∞–π–Ω—ã–µ —Å–∏–¥—ã (random, numpy, torch) –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.
–û–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø—É—Ç–∏ –∫ –æ—Å–Ω–æ–≤–Ω—ã–º –ø–∞–ø–∫–∞–º –ø—Ä–æ–µ–∫—Ç–∞: —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –¥–∞–Ω–Ω—ã–µ, –º–æ–¥–µ–ª–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ª–æ–≥–∏.
–°–æ–∑–¥–∞—é—Ç—Å—è –ø–∞–ø–∫–∏ train, val, test –¥–ª—è —Ç—Ä—ë—Ö –∫–ª–∞—Å—Å–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: real, drawing, generated.
–ó–∞–¥–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã CLASSES –∏ IMG_SIZE=224 –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏.
–§—É–Ω–∫—Ü–∏—è print_tree –≤—ã–≤–æ–¥–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–∞–ª–æ–≥–æ–≤, —Ñ—É–Ω–∫—Ü–∏—è save_config —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ config.json.
–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–æ–∑–¥–∞—ë—Ç—Å—è –≥–æ—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —ç—Ç–∞–ø–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2
#  Synthetic data sanity check (MobileNetV3 & ResNet50)
# ================================================================

import torch, torchvision
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import timm

# ---------- 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH_SIZE = 8
CLASSES = ["real", "drawing", "generated"]
print("Device:", DEVICE)

# ---------- 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ----------
class SyntheticAvatarDataset(Dataset):
    def __init__(self, num_images=24, img_size=224, num_classes=3, transform=None):
        self.num_images = num_images
        self.img_size = img_size
        self.num_classes = num_classes
        self.transform = transform
        self.images = [np.uint8(np.random.rand(img_size, img_size, 3) * 255)
                       for _ in range(num_images)]
        self.labels = np.random.randint(0, num_classes, num_images)

    def __len__(self):
        return self.num_images

    def __getitem__(self, idx):
        img = Image.fromarray(self.images[idx])
        if self.transform:
            img = self.transform(img)
        label = self.labels[idx]
        return img, label

# ---------- 3. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è ----------
basic_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225]),
])

synthetic_ds = SyntheticAvatarDataset(transform=basic_tfms)
synthetic_dl = DataLoader(synthetic_ds, batch_size=BATCH_SIZE, shuffle=True)

# ---------- 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞—Ç—á–∞ ----------
images, labels = next(iter(synthetic_dl))
print("Batch shape:", images.shape)
print("Labels:", labels.tolist())

grid = torchvision.utils.make_grid(images, nrow=4, normalize=True)
plt.figure(figsize=(6,6))
plt.imshow(np.transpose(grid.numpy(), (1,2,0)))
plt.title("Synthetic avatar batch (normalized)")
plt.axis("off")
plt.show()

# ---------- 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π ----------
def test_model(model_name, num_classes=3):
    print(f"\n Testing {model_name} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)
    model = model.to(DEVICE)
    with torch.no_grad():
        x = images.to(DEVICE)
        out = model(x)
        print(f"Output shape: {out.shape} | Example logits (first row):")
        print(out[0].cpu().numpy())

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–µ –º–æ–¥–µ–ª–∏
test_model("mobilenetv3_small_100")
test_model("resnet50")

print("\n‚úÖ Block 2 completed successfully.")
print("Both models processed synthetic data correctly ‚Äî ready for real dataset.")

"""Block 2 –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
–°–æ–∑–¥–∞—ë—Ç—Å—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–æ–º 224√ó224 –ø–∏–∫—Å–µ–ª—è —Å —Ç—Ä–µ–º—è –∫–ª–∞—Å—Å–∞–º–∏ ‚Äî real, drawing, generated.
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å SyntheticAvatarDataset, —Ä–µ–∞–ª–∏–∑—É—é—â–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å PyTorch Dataset –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≤–æ–∑–≤—Ä–∞—Ç–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–µ—Ç–æ–∫.
–ü—Ä–∏–º–µ–Ω—è—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø–µ—Ä–µ–≤–æ–¥ –≤ —Ç–µ–Ω–∑–æ—Ä.
–ß–µ—Ä–µ–∑ DataLoader —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –±–∞—Ç—á –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤—ã–≤–æ–¥—è—Ç—Å—è –∏—Ö —Ä–∞–∑–º–µ—Ä—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.
–§—É–Ω–∫—Ü–∏—è test_model —Å–æ–∑–¥–∞—ë—Ç –∏ –ø—Ä–æ–≥–æ–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏ MobileNetV3 –∏ ResNet50 –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–º –±–∞—Ç—á–µ, –ø—Ä–æ–≤–µ—Ä—è—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞ –∏ —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–æ–≤.
–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –±–ª–æ–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ, —á—Ç–æ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2.1
#  Model inspection and comparison (ResNet50 vs MobileNetV3)
# ================================================================

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã
!pip install -q torchinfo thop

import torch
import timm
from torchinfo import summary

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
IMG_SIZE = 224

# 1. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
resnet = timm.create_model("resnet50", pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)
mobilenet = timm.create_model("mobilenetv3_small_100", pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)

# 2. –ö—Ä–∞—Ç–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (—Ç–∞–±–ª–∏—Ü–∞ —Å–ª–æ—ë–≤)
print("\n=== ResNet50 architecture (summary) ===")
print(summary(resnet, input_size=(1, 3, IMG_SIZE, IMG_SIZE),
              depth=3, verbose=1,
              col_names=("input_size", "output_size", "num_params")))

print("\n=== MobileNetV3 architecture (summary) ===")
print(summary(mobilenet, input_size=(1, 3, IMG_SIZE, IMG_SIZE),
              depth=3, verbose=1,
              col_names=("input_size", "output_size", "num_params")))

# 3. –ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
def count_params(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total, trainable

r_total, r_train = count_params(resnet)
m_total, m_train = count_params(mobilenet)

print(f"\nResNet50: total params = {r_total/1e6:.2f}M | trainable = {r_train/1e6:.2f}M")
print(f"MobileNetV3: total params = {m_total/1e6:.2f}M | trainable = {m_train/1e6:.2f}M")

# 4. FLOPs (–æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö)
try:
    from thop import profile
    dummy = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)
    flops_r, _ = profile(resnet, inputs=(dummy,), verbose=False)
    flops_m, _ = profile(mobilenet, inputs=(dummy,), verbose=False)
    print(f"\nFLOPs comparison (for 224x224 input):")
    print(f"ResNet50: {flops_r/1e9:.2f} GFLOPs")
    print(f"MobileNetV3: {flops_m/1e9:.2f} GFLOPs")
except Exception as e:
    print("\nSkipping FLOPs estimation:", e)

# 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
with torch.no_grad():
    x = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)
    feat_resnet = resnet.forward_features(x) if hasattr(resnet, "forward_features") else None
    feat_mobilenet = mobilenet.forward_features(x)
    print(f"\nResNet feature vector shape: {feat_resnet.shape if feat_resnet is not None else '(n/a)'}")
    print(f"MobileNet feature vector shape: {feat_mobilenet.shape}")

print("\nBlock 2.1 completed successfully ‚Äî models inspected and compared.")

"""Block 2.1 –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π ‚Äî ResNet50 –∏ MobileNetV3.
–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞–∫–µ—Ç—ã torchinfo (–¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏) –∏ thop (–¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏–π FLOPs).
–°–æ–∑–¥–∞—é—Ç—Å—è –æ–±–µ –º–æ–¥–µ–ª–∏ —Å —á–∏—Å–ª–æ–º –∫–ª–∞—Å—Å–æ–≤ 3 –∏ —Ä–∞–∑–º–µ—Ä–æ–º –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è 224√ó224.
–§—É–Ω–∫—Ü–∏—è summary –≤—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å–ª–æ—ë–≤ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞, –∞ —Ç–∞–∫–∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è count_params –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ –æ–±—â–µ–≥–æ –∏ –æ–±—É—á–∞–µ–º–æ–≥–æ —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–∏–ª–ª–∏–æ–Ω–∞—Ö.
–° –ø–æ–º–æ—â—å—é thop –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–ø–µ—Ä–∞—Ü–∏–π (GFLOPs) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏.
–í –∫–æ–Ω—Ü–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è —Ñ–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature maps), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –º–µ—Ç–æ–¥–æ–º forward_features, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –±–ª–æ–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É ResNet50 –∏ MobileNetV3 –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, —Ä–∞–∑–º–µ—Ä—É –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2.2
#  Inference speed benchmark (ResNet50 vs MobileNetV3)
# ================================================================

import torch, timm, time, numpy as np

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH = 8
ITERATIONS = 50

models = {
    "ResNet50": timm.create_model("resnet50", pretrained=False, num_classes=3).to(DEVICE).eval(),
    "MobileNetV3": timm.create_model("mobilenetv3_small_100", pretrained=False, num_classes=3).to(DEVICE).eval()
}

dummy = torch.randn(BATCH, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)

results = {}
for name, model in models.items():
    torch.cuda.synchronize() if DEVICE == "cuda" else None
    start = time.time()
    with torch.no_grad():
        for _ in range(ITERATIONS):
            _ = model(dummy)
    torch.cuda.synchronize() if DEVICE == "cuda" else None
    end = time.time()
    avg_ms = (end - start) / (ITERATIONS * BATCH) * 1000
    results[name] = avg_ms
    print(f"{name}: {avg_ms:.3f} ms/image")

import pandas as pd, matplotlib.pyplot as plt
df_speed = pd.DataFrame(list(results.items()), columns=["Model", "ms_per_image"])
plt.figure(figsize=(5,4))
plt.bar(df_speed["Model"], df_speed["ms_per_image"], color=["steelblue","orange"])
plt.ylabel("Inference time (ms per image)")
plt.title("Model Inference Speed Comparison")
plt.tight_layout()
plt.show()

print("\nSpeed benchmark completed.")

"""Block 2.2 –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏–∑–º–µ—Ä–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π) –¥–ª—è –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π ‚Äî ResNet50 –∏ MobileNetV3.
–°–æ–∑–¥–∞—é—Ç—Å—è –æ–±–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º–µ eval() –∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –∏–ª–∏ CPU).
–§–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á –∏–∑ –≤–æ—Å—å–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–æ–º 224√ó224 –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è 50 –∏—Ç–µ—Ä–∞—Ü–∏–π –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –±–µ–∑ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —á–∏—Å—Ç–æ–µ –≤—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.
–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö –Ω–∞ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π.
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü—É –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ –≤–∏–¥–µ —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏.
–ë–ª–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è, —á—Ç–æ MobileNetV3 —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–∏ —Å—Ö–æ–∂–µ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2.3
#  Feature map visualization
# ================================================================

import torch, timm, matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image
import numpy as np

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224

# –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å —Å–≤–æ–µ–π –∫–∞—Ä—Ç–∏–Ω–∫–æ–π)
img = Image.fromarray(np.uint8(np.random.rand(IMG_SIZE, IMG_SIZE, 3)*255))
tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
x = tfm(img).unsqueeze(0).to(DEVICE)

# –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def visualize_features(model_name, layer_name):
    model = timm.create_model(model_name, pretrained=False, num_classes=3).to(DEVICE)
    layer = dict([*model.named_modules()])[layer_name]
    features = []
    def hook(_, __, output): features.append(output.detach().cpu())
    h = layer.register_forward_hook(hook)
    with torch.no_grad(): _ = model(x)
    h.remove()
    fmap = features[0][0].numpy()
    fig, axes = plt.subplots(2, 8, figsize=(12,3))
    for i, ax in enumerate(axes.flat):
        if i < fmap.shape[0]:
            ax.imshow(fmap[i], cmap="viridis")
        ax.axis("off")
    plt.suptitle(f"{model_name} ‚Äî {layer_name} feature maps")
    plt.tight_layout()
    plt.show()

visualize_features("resnet50", "conv1")
visualize_features("mobilenetv3_small_100", "conv_stem")

print("Feature map visualization completed.")

"""Block 2.3 –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature maps), –∏–∑–≤–ª–µ–∫–∞–µ–º—ã—Ö –∏–∑ —Ä–∞–Ω–Ω–∏—Ö —Å–ª–æ—ë–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π ResNet50 –∏ MobileNetV3.
–°–æ–∑–¥–∞—ë—Ç—Å—è —Å–ª—É—á–∞–π–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º 224√ó224 –ø–∏–∫—Å–µ–ª—è, –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –±–∞–∑–æ–≤—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É (–∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä).
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è visualize_features, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç –º–æ–¥–µ–ª—å timm, –Ω–∞—Ö–æ–¥–∏—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π —Å–ª–æ–π –ø–æ –∏–º–µ–Ω–∏ –∏ —Å –ø–æ–º–æ—â—å—é forward hook –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –µ–≥–æ –≤—ã—Ö–æ–¥.
–ü–æ—Å–ª–µ –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–±–æ—Ä –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ—è, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ –º–∞—Å—Å–∏–≤ –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –∫–∞–∫ —Å–µ—Ç–∫–∞ –∏–∑ 16 –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —à–∞–±–ª–æ–Ω—ã (–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –∫–æ–Ω—Ç—É—Ä—ã, —Ç–µ–∫—Å—Ç—É—Ä—ã) –º–æ–¥–µ–ª—å –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —Å—Ç–∞–¥–∏—è—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
–≠—Ç–æ—Ç –±–ª–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ä–∞–±–æ—Ç—ã —Å–µ—Ç–µ–π –∏ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–∑–ª–∏—á–∏–π –≤ —Å–ø–æ—Å–æ–±–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É ResNet50 –∏ MobileNetV3.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 2.4
#  Feature space projection (t-SNE / PCA)
# ================================================================

import torch, timm, numpy as np, matplotlib.pyplot as plt
from torchvision import transforms
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
CLASSES = ["real", "drawing", "generated"]

# –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è" —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
np.random.seed(42)
imgs = []
labels = []
for i, c in enumerate(CLASSES):
    for _ in range(20):
        arr = np.uint8(np.random.rand(IMG_SIZE, IMG_SIZE, 3)*255)
        imgs.append(arr)
        labels.append(c)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

x = torch.stack([tfm(Image.fromarray(img)) for img in imgs]).to(DEVICE)
model = timm.create_model("mobilenetv3_small_100", pretrained=False, num_classes=3).to(DEVICE)
model.eval()

# –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
with torch.no_grad():
    feats = model.forward_features(x).mean(dim=[2,3]).cpu().numpy()

# –°–Ω–∏–∂–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
pca = PCA(n_components=30).fit_transform(feats)
tsne = TSNE(n_components=2, random_state=42, init="pca").fit_transform(pca)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(6,5))
for c in np.unique(labels):
    idx = [i for i, lbl in enumerate(labels) if lbl == c]
    plt.scatter(tsne[idx,0], tsne[idx,1], label=c, alpha=0.7)
plt.legend()
plt.title("Feature Space Projection (MobileNetV3, synthetic data)")
plt.xlabel("t-SNE Dim 1"); plt.ylabel("t-SNE Dim 2")
plt.tight_layout()
plt.show()

print("Feature space visualization completed.")

"""Block 2.4 –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –∏–∑ –º–æ–¥–µ–ª–∏, –≤ –¥–≤—É–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ t-SNE –∏ PCA.
–ì–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç—Ä—ë—Ö –∫–ª–∞—Å—Å–æ–≤ ‚Äî real, drawing –∏ generated ‚Äî –ø–æ 20 —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∫–∞–∂–¥–æ–≥–æ.
–ö –∫–∞–∂–¥–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä.
–°–æ–∑–¥–∞—ë—Ç—Å—è –º–æ–¥–µ–ª—å MobileNetV3, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ forward_features –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ –±–ª–æ–∫–∞.
–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç—Å—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ–º –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
–ó–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ—ç—Ç–∞–ø–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: —Å–Ω–∞—á–∞–ª–∞ PCA –¥–æ 30 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –∑–∞—Ç–µ–º t-SNE –¥–æ 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –Ω–∞ –ø–ª–æ—Å–∫–æ—Å—Ç–∏, –≥–¥–µ —Ç–æ—á–∫–∏, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏–µ —Ä–∞–∑–Ω—ã–º –∫–ª–∞—Å—Å–∞–º, –æ–∫—Ä–∞—à–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–Ω—ã–º–∏ —Ü–≤–µ—Ç–∞–º–∏.
–≠—Ç–æ—Ç –±–ª–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –∫–∞–∫ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å—ã –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∞–∂–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
"""

import zipfile
from pathlib import Path

RAW_ROOT = Path("/content/avatar_recog/data/raw")
zip_files = list(RAW_ROOT.glob("*.zip"))

print("Checking zip archives...\n")
for zfile in zip_files:
    try:
        with zipfile.ZipFile(zfile, 'r') as zf:
            n_files = len(zf.namelist())
            first = zf.namelist()[:3]
            print(f"{zfile.name}: OK | files inside: {n_files}")
            print("  Examples:", first)
    except zipfile.BadZipFile:
        print(f"{zfile.name}: Not a valid ZIP archive!")

import hashlib, os
from pathlib import Path

RAW_DIR = Path("/content/avatar_recog/data/raw")
hashes = {}
duplicates = []

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–æ –≤—Å–µ—Ö –ø–æ–¥–ø–∞–ø–∫–∞—Ö (–µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω—ã)
for img_path in RAW_DIR.rglob("*.[jp][pn]g"):
    try:
        with open(img_path, 'rb') as f:
            img_hash = hashlib.md5(f.read()).hexdigest()
        if img_hash in hashes:
            duplicates.append((img_path, hashes[img_hash]))
        else:
            hashes[img_hash] = img_path
    except Exception as e:
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏:", img_path, e)

print(f"\n–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}")
if duplicates:
    print("–ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
    for i, (a,b) in enumerate(duplicates[:5]):
        print(f"{i+1}) {a} <--> {b}")

import zipfile, io, random
from PIL import Image
import matplotlib.pyplot as plt
from pathlib import Path

RAW_ROOT = Path("/content/avatar_recog/data/raw")
zip_files = sorted(RAW_ROOT.glob("*.zip"))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞
plt.figure(figsize=(12, len(zip_files) * 2.5))

for row, zfile in enumerate(zip_files, start=1):
    with zipfile.ZipFile(zfile, 'r') as zf:
        img_names = [n for n in zf.namelist() if n.lower().endswith(('.jpg', '.jpeg', '.png'))]
        samples = random.sample(img_names, min(5, len(img_names)))
        for col, name in enumerate(samples, start=1):
            data = zf.read(name)
            img = Image.open(io.BytesIO(data)).convert("RGB")
            plt.subplot(len(zip_files), 5, (row - 1) * 5 + col)
            plt.imshow(img)
            plt.axis("off")
            if col == 3:  # –ø–æ–¥–ø–∏—Å—å –≤ —Ü–µ–Ω—Ç—Ä–µ —Ä—è–¥–∞
                plt.title(zfile.stem, fontsize=10)
plt.tight_layout()
plt.show()

"""–≠—Ç–æ—Ç –±–ª–æ–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ö—Ä–∞–Ω—è—â–∏—Ö—Å—è –≤ zip-–∞—Ä—Ö–∏–≤–∞—Ö –≤–Ω—É—Ç—Ä–∏ –ø–∞–ø–∫–∏ /content/avatar_recog/data/raw.
–°–Ω–∞—á–∞–ª–∞ —Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∞—Ä—Ö–∏–≤–æ–≤ .zip –≤ –∫–∞—Ç–∞–ª–æ–≥–µ RAW_ROOT –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è.
–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Ä—Ö–∏–≤–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç–∏–µ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å zipfile, –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (.jpg, .jpeg, .png).
–ò–∑ –∫–∞–∂–¥–æ–≥–æ –∞—Ä—Ö–∏–≤–∞ —Å–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –¥–æ –ø—è—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é random.sample.
–ö–∞–∂–¥–æ–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–∏—Ç–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∞—Ä—Ö–∏–≤–∞ –≤ –ø–∞–º—è—Ç—å, –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ PIL –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é matplotlib.
–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Ä—Ö–∏–≤–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–∑ –ø—è—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ –≤ —Ü–µ–Ω—Ç—Ä–µ —Å—Ç—Ä–æ–∫–∏ –≤—ã–≤–æ–¥–∏—Ç—Å—è –ø–æ–¥–ø–∏—Å—å —Å –∏–º–µ–Ω–µ–º –∞—Ä—Ö–∏–≤–∞.
–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤—Å–µ—Ö zip-—Ñ–∞–π–ª–æ–≤, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 3
#  Data Preparation: unzip, merge same classes, split train/val/test
# ================================================================

import zipfile, shutil, os, random
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

# === –ü–∞–ø–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ ===
WORK_ROOT = Path("/content/avatar_recog")
RAW_ROOT  = WORK_ROOT / "data" / "raw"
PROC_ROOT = WORK_ROOT / "data" / "processed"

# === –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ zip-–∞—Ä—Ö–∏–≤–æ–≤ ===
zip_files = sorted(RAW_ROOT.glob("*.zip"))
print("Found zip files:")
for z in zip_files:
    print(" -", z.name)

# === –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–æ–≤ ===
for zfile in tqdm(zip_files, desc="Unzipping archives"):
    out_dir = RAW_ROOT / zfile.stem
    out_dir.mkdir(exist_ok=True)
    with zipfile.ZipFile(zfile, 'r') as zip_ref:
        zip_ref.extractall(RAW_ROOT)

# === –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤ ===
# –í—Å–µ –∞—Ä—Ö–∏–≤—ã, –≥–¥–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ, –ø–æ–π–¥—É—Ç –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Å
name_map = {
    "real": "real",
    "drawn": "drawing",
    "ai": "generated"
}

# === –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—É—é processed-—Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Å–æ–∑–¥–∞—ë–º –∑–∞–Ω–æ–≤–æ ===
if PROC_ROOT.exists():
    shutil.rmtree(PROC_ROOT)
for split in ["train", "val", "test"]:
    for cls in ["real", "drawing", "generated"]:
        (PROC_ROOT / split / cls).mkdir(parents=True, exist_ok=True)

# === –°–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –≤—Å–µ—Ö –ø–æ–¥–ø–∞–ø–æ–∫ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º ===
def collect_images(folder):
    imgs = []
    for ext in ("*.jpg", "*.jpeg", "*.png", "*.webp"):
        imgs.extend(folder.rglob(ext))
    return imgs

# –°–æ–±–µ—Ä—ë–º –≤—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–∞–ø–∫–∏ –ø–æ—Å–ª–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
subdirs = [p for p in RAW_ROOT.iterdir() if p.is_dir()]

class_groups = {"real": [], "drawing": [], "generated": []}

for folder in subdirs:
    lower_name = folder.name.lower()
    for key, cls in name_map.items():
        if key in lower_name:
            imgs = collect_images(folder)
            class_groups[cls].extend(imgs)
            print(f" {folder.name} ‚Üí {cls} ({len(imgs)} images)")
            break

# === –î–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test ===
split_ratio = {"train": 0.8, "val": 0.1, "test": 0.1}
summary = []

for cls, imgs in class_groups.items():
    if len(imgs) == 0:
        print(f"No images found for class {cls}")
        continue
    random.shuffle(imgs)
    n = len(imgs)
    n_train = int(n * split_ratio["train"])
    n_val   = int(n * split_ratio["val"])
    n_test  = n - n_train - n_val
    splits = {
        "train": imgs[:n_train],
        "val": imgs[n_train:n_train+n_val],
        "test": imgs[n_train+n_val:]
    }
    for split, files in splits.items():
        out_dir = PROC_ROOT / split / cls
        for f in files:
            shutil.copy(f, out_dir / f.name)
    summary.append({"class": cls, "total": n, "train": n_train, "val": n_val, "test": n_test})

# === –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
df = pd.DataFrame(summary)
print("\n=== Dataset summary ===")
print(df.to_string(index=False))

# === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è ===
plt.figure(figsize=(6,4))
df_melt = df.melt(id_vars="class", value_vars=["train","val","test"],
                  var_name="split", value_name="count")
for cls in df["class"]:
    subset = df_melt[df_melt["class"] == cls]
    plt.bar(subset["split"] + " (" + cls + ")", subset["count"])
plt.title("Image distribution by class and split")
plt.ylabel("Number of images")
plt.xticks(rotation=20)
plt.tight_layout()
plt.show()

print("\n‚úÖ Block 3 completed successfully.")
print("All datasets unzipped, merged, and split into train/val/test.")

"""Block 3 –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –Ω–∞—á–∏–Ω–∞—è —Å —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ –∞—Ä—Ö–∏–≤–æ–≤ –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—è —Å–æ–∑–¥–∞–Ω–∏–µ–º –≥–æ—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ train, val –∏ test.
–í –Ω–∞—á–∞–ª–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø—É—Ç–∏ –∫ –ø–∞–ø–∫–∞–º –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –Ω–∞–ª–∏—á–∏–µ zip-–∞—Ä—Ö–∏–≤–æ–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ raw.
–ö–∞–∂–¥—ã–π –∞—Ä—Ö–∏–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É, –ø–æ—Å–ª–µ —á–µ–≥–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª–∞—Å—Å–∞–º.
–ú–∞–ø–ø–∏–Ω–≥ name_map –∑–∞–¥–∞—ë—Ç –ø—Ä–∞–≤–∏–ª–∞, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –∞—Ä—Ö–∏–≤—ã —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ ‚Äúreal‚Äù, ‚Äúdrawn‚Äù –∏ ‚Äúai‚Äù –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º real, drawing –∏ generated.
–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Ç–∞—Ä–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ processed —É–¥–∞–ª—è–µ—Ç—Å—è –∏ —Å–æ–∑–¥–∞—ë—Ç—Å—è –∑–∞–Ω–æ–≤–æ —Å –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥–∞–º–∏ train, val –∏ test –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞.
–§—É–Ω–∫—Ü–∏—è collect_images —Å–æ–±–∏—Ä–∞–µ—Ç –ø—É—Ç–∏ –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –ø–∞–ø–∫–∏.
–ü–æ—Å–ª–µ —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã–µ —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—é—Ç—Å—è –∏ –¥–µ–ª—è—Ç—Å—è –≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏—è—Ö 80/10/10 –º–µ–∂–¥—É –æ–±—É—á–∞—é—â–µ–π, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞–º–∏.
–§–∞–π–ª—ã –∫–æ–ø–∏—Ä—É—é—Ç—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ processed, –∞ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ summary —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É –∏ –≤—ã–±–æ—Ä–∫–µ.
–í –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∏—Ç—Å—è –ø—Ä–æ—Å—Ç–∞—è —Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º –∏ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 4
#  DataLoader setup & visualization
# ================================================================

import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
import random

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH_SIZE = 32

DATA_DIR = "/content/avatar_recog/data/processed"

# === –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ===
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

val_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
train_ds = datasets.ImageFolder(root=f"{DATA_DIR}/train", transform=train_tfms)
val_ds   = datasets.ImageFolder(root=f"{DATA_DIR}/val", transform=val_tfms)
test_ds  = datasets.ImageFolder(root=f"{DATA_DIR}/test", transform=val_tfms)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

print(f"Train size: {len(train_ds)} | Val size: {len(val_ds)} | Test size: {len(test_ds)}")
print(f"Classes: {train_ds.classes}")

# === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ===
def imshow_batch(images, labels, classes):
    inv_norm = transforms.Normalize(
        mean=[-m/s for m, s in zip([0.485, 0.456, 0.406],
                                   [0.229, 0.224, 0.225])],
        std=[1/s for s in [0.229, 0.224, 0.225]]
    )
    imgs = inv_norm(images)
    imgs = imgs.numpy().transpose((0, 2, 3, 1))
    fig, axes = plt.subplots(2, 4, figsize=(10,5))
    for i, ax in enumerate(axes.flat):
        ax.imshow(np.clip(imgs[i], 0, 1))
        ax.set_title(classes[labels[i]])
        ax.axis("off")
    plt.tight_layout()
    plt.show()

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ ===
images, labels = next(iter(train_loader))
print("Batch shape:", images.shape)
print("Labels:", labels.tolist())

imshow_batch(images[:8], labels[:8], train_ds.classes)

print("\n‚úÖ Block 4 completed successfully ‚Äî DataLoaders are ready.")
print("Next: Block 5 ‚Äî Model training and fine-tuning.")

"""Block 4 –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.
–û–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (CPU –∏–ª–∏ GPU), —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è 224√ó224 –∏ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ 32.
–°–æ–∑–¥–∞—é—Ç—Å—è –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π) –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:
‚Äî train_tfms –≤–∫–ª—é—á–∞–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∑–∞ —Å—á—ë—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π (—Å–ª—É—á–∞–π–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ, –∏–∑–º–µ–Ω–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏, –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏ –∏ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è);
‚Äî val_tfms –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é ImageFolder –∏–∑ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞–ø–æ–∫ train, val –∏ test, —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è DataLoader-–æ–±—ä–µ–∫—Ç—ã —Å –Ω—É–∂–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ—Ç–æ–∫–æ–≤ —á—Ç–µ–Ω–∏—è.
–í—ã–≤–æ–¥–∏—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–º–µ—Ä–µ –≤—ã–±–æ—Ä–æ–∫ –∏ —Å–ø–∏—Å–∫–µ –∫–ª–∞—Å—Å–æ–≤, –ø–æ—Å–ª–µ —á–µ–≥–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.
–§—É–Ω–∫—Ü–∏—è imshow_batch –≤—ã–≤–æ–¥–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –±–∞—Ç—á–∞ —Å –æ–±—Ä–∞—Ç–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –∫–ª–∞—Å—Å–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —á–∏—Ç–∞—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏ –∏–º–µ—é—Ç –Ω—É–∂–Ω—ã–µ –º–µ—Ç–∫–∏.
–ë–ª–æ–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç —ç—Ç–∞–ø –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–≤–∞—è –≥–æ—Ç–æ–≤—ã–µ DataLoader‚Äô—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 4.1
#  Dataset inspection: samples, class balance, DataLoader check
# ================================================================

import matplotlib.pyplot as plt
import numpy as np
from torchvision import datasets, transforms
from PIL import Image
from pathlib import Path
import random
import torch

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
DATA_DIR = "/content/avatar_recog/data/processed/train"
CLASSES = ['drawing', 'generated', 'real']
IMG_SIZE = 224

# === 1. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ ===
tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
])

plt.figure(figsize=(9, 6))
for i, cls in enumerate(CLASSES):
    paths = list((Path(DATA_DIR) / cls).glob("*.*"))
    samples = random.sample(paths, min(3, len(paths)))
    for j, img_path in enumerate(samples):
        img = Image.open(img_path).convert("RGB")
        plt.subplot(len(CLASSES), 3, i * 3 + j + 1)
        plt.imshow(img)
        plt.title(cls, fontsize=10)
        plt.axis("off")
plt.suptitle("Examples from each dataset class", fontsize=14)
plt.tight_layout()
plt.show()

print("‚úÖ Sample visualization completed.\n")

# === 2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ ===
counts = [59275, 6355, 6738]
labels = ['Drawing', 'Generated', 'Real']

plt.figure(figsize=(5, 5))
plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90,
        colors=['orange', 'green', 'steelblue'])
plt.title("Dataset Class Distribution")
plt.show()

print("‚úÖ Class balance visualization completed.\n")

# === 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ DataLoader ===
BATCH_SIZE = 32
val_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

val_ds = datasets.ImageFolder(root="/content/avatar_recog/data/processed/train", transform=val_tfms)
val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)

batch = next(iter(val_loader))
images, labels = batch

print("=== DataLoader sanity check ===")
print(f"Batch shape: {images.shape}")
print(f"Labels (first 10): {labels[:10].tolist()}")
print(f"Classes: {val_ds.classes}")

print("\n‚úÖ Block 4.1 completed successfully ‚Äî dataset verified and ready for training.")

"""Block 4.1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏.
–ó–∞–¥–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º, —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –∏ –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
–í –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –ø–æ —Ç—Ä–∏ —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (drawing, generated, real), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–±–µ–¥–∏—Ç—å—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö.
–î–∞–ª–µ–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –∫—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤, –≥–¥–µ –∑–∞—Ä–∞–Ω–µ–µ —É–∫–∞–∑–∞–Ω—ã –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞. –≠—Ç–æ –¥–∞—ë—Ç –Ω–∞–≥–ª—è–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –Ω–∞–ª–∏—á–∏–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤.
–í —Ç—Ä–µ—Ç—å–µ–π —á–∞—Å—Ç–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å DataLoader: —Å–æ–∑–¥–∞—ë—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ train —Å –±–∞–∑–æ–≤—ã–º–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è–º–∏ (resize, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è), —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –±–∞—Ç—á –∏ –≤—ã–≤–æ–¥—è—Ç—Å—è –µ–≥–æ —Ñ–æ—Ä–º–∞, –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–∫ –∏ —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤.
–ë–ª–æ–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è, –∫–ª–∞—Å—Å—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏ –≤—ã–±–æ—Ä–∫–∏ –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π.
"""

import torch
print(torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 5
#  Training & fine-tuning: MobileNetV3 and ResNet50
# ================================================================

import os, time, json, math, random
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import datasets, transforms
import timm

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

# ---------- 0. Paths, device, params ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")  # –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω
MODELS_DIR = (DRIVE_ROOT if DRIVE_ROOT.exists() else WORK_ROOT) / "models"
OUT_DIR    = (DRIVE_ROOT if DRIVE_ROOT.exists() else WORK_ROOT) / "outputs"
for p in [MODELS_DIR, OUT_DIR]:
    p.mkdir(parents=True, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3
BATCH = 64 if DEVICE == "cuda" else 32
EPOCHS_MNV3 = 6
EPOCHS_RN50 = 8
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)

print(f"Device: {DEVICE} | Batch: {BATCH}")
print(f"Saving models to: {MODELS_DIR}")
print(f"Saving plots/logs to: {OUT_DIR}")

# ---------- 1. Transforms ----------
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

# ---------- 2. Datasets ----------
train_ds = datasets.ImageFolder(str(DATA_DIR / "train"), transform=train_tfms)
val_ds   = datasets.ImageFolder(str(DATA_DIR / "val"),   transform=eval_tfms)
test_ds  = datasets.ImageFolder(str(DATA_DIR / "test"),  transform=eval_tfms)
class_names = train_ds.classes
print("Classes:", class_names)

# ---------- 3. Handle class imbalance ----------
# Weights ~ 1/freq for WeightedRandomSampler
targets = np.array(train_ds.targets)
counts = np.bincount(targets, minlength=NUM_CLASSES)
class_weights = (1.0 / np.maximum(counts, 1)).astype(np.float32)
sample_weights = class_weights[targets]
sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)

print("Train counts per class:", {cls:int(c) for cls,c in zip(class_names, counts)})
print("Class weights:", {cls:round(w,5) for cls,w in zip(class_names, class_weights)})

# ---------- 4. DataLoaders ----------
train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler,
                          num_workers=2, pin_memory=(DEVICE=="cuda"))
val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,
                          num_workers=2, pin_memory=(DEVICE=="cuda"))
test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False,
                          num_workers=2, pin_memory=(DEVICE=="cuda"))

# ---------- 5. Utilities ----------
def build_model(model_name: str):
    model = timm.create_model(model_name, pretrained=True, num_classes=NUM_CLASSES)
    return model.to(DEVICE)

def epoch_run(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train() if train_mode else model.eval()
    losses, all_preds, all_targs = [], [], []
    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=="cuda"))

    for x, y in loader:
        x, y = x.to(DEVICE), y.to(DEVICE)
        with torch.cuda.amp.autocast(enabled=(DEVICE=="cuda")):
            logits = model(x)
            loss = criterion(logits, y)

        if train_mode:
            optimizer.zero_grad(set_to_none=True)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

        losses.append(loss.item())
        all_preds.extend(torch.argmax(logits, 1).detach().cpu().numpy())
        all_targs.extend(y.detach().cpu().numpy())

    acc = accuracy_score(all_targs, all_preds)
    f1m = f1_score(all_targs, all_preds, average="macro", zero_division=0)
    return float(np.mean(losses)), acc, f1m

def train_model(model_name: str, epochs: int):
    model = build_model(model_name)
    # Weighted CE to reflect imbalance (same weights as sampler)
    ce_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)
    criterion = nn.CrossEntropyLoss(weight=ce_weights)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-2)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", factor=0.5, patience=1
)


    history = {"epoch": [], "train_loss": [], "train_acc": [], "train_f1": [],
               "val_loss": [], "val_acc": [], "val_f1": []}
    best_val_f1, best_path = -1.0, MODELS_DIR / f"{model_name}_best.pth"
    patience, wait = 3, 0  # early stopping

    for epoch in range(1, epochs+1):
        t0 = time.time()
        tr = epoch_run(model, train_loader, criterion, optimizer)
        va = epoch_run(model, val_loader,   criterion, optimizer=None)
        scheduler.step(va[0])

        history["epoch"].append(epoch)
        history["train_loss"].append(tr[0]); history["train_acc"].append(tr[1]); history["train_f1"].append(tr[2])
        history["val_loss"].append(va[0]);   history["val_acc"].append(va[1]);   history["val_f1"].append(va[2])

        msg = (f"[{model_name}] Epoch {epoch:02d}/{epochs} | "
               f"loss {tr[0]:.4f} acc {tr[1]:.3f} f1 {tr[2]:.3f} | "
               f"val_loss {va[0]:.4f} val_acc {va[1]:.3f} val_f1 {va[2]:.3f} | "
               f"time {time.time()-t0:.1f}s")
        print(msg)

        # checkpoint on best val_f1
        if va[2] > best_val_f1:
            best_val_f1 = va[2]; wait = 0
            torch.save(model.state_dict(), best_path)
        else:
            wait += 1
            if wait >= patience:
                print(f"Early stopping on epoch {epoch}. Best val_f1={best_val_f1:.3f}")
                break

    # save history
    hist_df = pd.DataFrame(history)
    hist_csv = OUT_DIR / f"history_{model_name}.csv"
    hist_df.to_csv(hist_csv, index=False)

    # plots
    fig, ax = plt.subplots(1,2, figsize=(10,4))
    ax[0].plot(hist_df["epoch"], hist_df["train_loss"], label="train")
    ax[0].plot(hist_df["epoch"], hist_df["val_loss"], label="val")
    ax[0].set_title(f"{model_name} ‚Äî loss"); ax[0].legend()

    ax[1].plot(hist_df["epoch"], hist_df["train_f1"], label="train F1")
    ax[1].plot(hist_df["epoch"], hist_df["val_f1"], label="val F1")
    ax[1].set_title(f"{model_name} ‚Äî F1"); ax[1].legend()
    plt.tight_layout()
    plt.show()

    print(f"Best weights saved to: {best_path}")
    print(f"Training log saved to: {hist_csv}")
    return best_path

def evaluate(model_name: str, weights_path: Path):
    model = build_model(model_name)
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model.eval()

    y_true, y_pred = [], []
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(DEVICE)
            logits = model(x)
            y_true.extend(y.numpy())
            y_pred.extend(torch.argmax(logits, 1).cpu().numpy())

    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)
    cm = confusion_matrix(y_true, y_pred)

    print("\n=== Test report ===")
    print(report)

    # save artifacts
    rpt_path = OUT_DIR / f"test_report_{model_name}.txt"
    with open(rpt_path, "w") as f:
        f.write(report)
    plt.figure(figsize=(5,4))
    plt.imshow(cm, cmap="Blues")
    plt.xticks(range(NUM_CLASSES), class_names, rotation=45)
    plt.yticks(range(NUM_CLASSES), class_names)
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.title(f"Confusion matrix ‚Äî {model_name}")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES):
            plt.text(j, i, cm[i, j], ha="center", va="center", fontsize=9)
    plt.tight_layout()
    cm_path = OUT_DIR / f"cm_{model_name}.png"
    plt.savefig(cm_path); plt.show()

    print(f"Saved: {rpt_path}")
    print(f"Saved: {cm_path}")

# ---------- 6. Train MobileNetV3 ----------
mnv3_best = train_model("mobilenetv3_small_100", epochs=EPOCHS_MNV3)
evaluate("mobilenetv3_small_100", mnv3_best)

# ---------- 7. Train ResNet50 ----------
rn50_best = train_model("resnet50", epochs=EPOCHS_RN50)
evaluate("resnet50", rn50_best)

print("\n‚úÖ Block 5 completed.")

"""–ë–ª–æ–∫ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π (MobileNetV3 –∏ ResNet50) –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.

–ü—É—Ç–∏, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
–ó–∞–¥–∞—é—Ç—Å—è –∫–æ—Ä–Ω–µ–≤—ã–µ –∫–∞—Ç–∞–ª–æ–≥–∏ –¥–∞–Ω–Ω—ã—Ö/–º–æ–¥–µ–ª–µ–π/–≤—ã–≤–æ–¥–æ–≤, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU/CPU), —Ä–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–∞ –∏ —á–∏—Å–ª–∞ —ç–ø–æ—Ö, —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è —Å–∏–¥—ã –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –¥–ª—è –º–æ–¥–µ–ª–µ–π (6 —ç–ø–æ—Ö –¥–ª—è MobileNetV3, 8 –¥–ª—è ResNet50).

–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
Train-–ø–∞–π–ø–ª–∞–π–Ω –≤–∫–ª—é—á–∞–µ—Ç Resize, –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π —Ñ–ª–∏–ø, ColorJitter –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ–¥ ImageNet; –≤–∞–ª–∏–¥/—Ç–µ—Å—Ç ‚Äî —Ç–æ–ª—å–∫–æ Resize –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é. –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –∫ 224√ó224.

–î–∞—Ç–∞—Å–µ—Ç—ã –∏ –∫–ª–∞—Å—Å—ã
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è torchvision.datasets.ImageFolder –¥–ª—è train/val/test. –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –±–µ—Ä—É—Ç—Å—è –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–∞—Ç–∞–ª–æ–≥–æ–≤.

–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
–°—á–∏—Ç–∞–µ—Ç—Å—è —á–∞—Å—Ç–æ—Ç–∞ –∫–ª–∞—Å—Å–æ–≤ –∏ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –≤–µ—Å–∞ 1/freq. –≠—Ç–∏ –≤–µ—Å–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏ –≤ WeightedRandomSampler (–¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞—Ç—á–µ–π), –∏ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è CrossEntropy). –≠—Ç–æ –¥–∞—ë—Ç —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –æ—Å–ª–∞–±–∏—Ç—å, –æ—Å—Ç–∞–≤–∏–≤ –æ–¥–∏–Ω –∏–∑ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤.

DataLoader‚Äô—ã
Train ‚Äî —Å WeightedRandomSampler, –≤–∞–ª–∏–¥/—Ç–µ—Å—Ç ‚Äî –±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è. –ù–∞ GPU –≤–∫–ª—é—á—ë–Ω pin_memory –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è.

–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
build_model(name): —Å–æ–∑–¥–∞—ë—Ç timm-–º–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –∏ num_classes=3.
epoch_run(...): –µ–¥–∏–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—É; –≤ train-—Ä–µ–∂–∏–º–µ –≤–∫–ª—é—á–∞–µ—Ç AMP –Ω–∞ CUDA, —Å—á–∏—Ç–∞–µ—Ç loss, accuracy, macro-F1.
train_model(name, epochs): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW, ReduceLROnPlateau –ø–æ val_loss, —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ val_F1 (patience=3), —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤ –ø–æ –º–∞–∫—Å–∏–º—É–º—É val_F1, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –≤ CSV –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ loss/F1 –¥–ª—è train/val.
evaluate(name, path): –∑–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤, –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ —Ç–µ—Å—Ç–µ, –æ—Ç—á—ë—Ç classification_report –∏ –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤.

–ó–∞–ø—É—Å–∫–∏ –æ–±—É—á–µ–Ω–∏—è
–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Ç—Ä–µ–Ω–∏—Ä—É—é—Ç—Å—è –∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è mobilenetv3_small_100 –∏ resnet50; —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ª—É—á—à–∏–µ –≤–µ—Å–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 5-Lite
#  EfficientNet-B0 (Frozen Backbone) & ConvNeXt-Tiny (Progressive Unfreeze)
# ================================================================

import os, time, random
from pathlib import Path
import torch, timm
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

# ---------- 0. Paths ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"; MODELS_DIR.mkdir(exist_ok=True, parents=True)
OUT_DIR    = DRIVE_ROOT / "outputs"; OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3
BATCH = 48 if DEVICE=="cuda" else 16
EPOCHS_EFF = 3   # fast test run
EPOCHS_CNV = 4
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

print(f"Device: {DEVICE} | Batch: {BATCH}")
print("Folders:", MODELS_DIR, OUT_DIR)

# ---------- 1. Transforms ----------
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.ColorJitter(0.2,0.2,0.2,0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

# ---------- 2. Dataset (subset for speed) ----------
train_full = datasets.ImageFolder(str(DATA_DIR/"train"), transform=train_tfms)
val_full   = datasets.ImageFolder(str(DATA_DIR/"val"), transform=eval_tfms)
test_ds    = datasets.ImageFolder(str(DATA_DIR/"test"), transform=eval_tfms)
class_names = train_full.classes

# –≤–æ–∑—å–º–µ–º —Ç–æ–ª—å–∫–æ 25% train –∏ 40% val –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
subset_train = Subset(train_full, np.random.choice(len(train_full), len(train_full)//4, replace=False))
subset_val   = Subset(val_full,   np.random.choice(len(val_full),   len(val_full)//3,   replace=False))

train_loader = DataLoader(subset_train, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)
val_loader   = DataLoader(subset_val,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_ds,      batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)

# ---------- 3. Training helpers ----------
def epoch_run(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train() if train_mode else model.eval()
    losses, all_preds, all_targs = [], [], []
    with torch.set_grad_enabled(train_mode):
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            loss = criterion(logits, y)
            if train_mode:
                optimizer.zero_grad(); loss.backward(); optimizer.step()
            losses.append(loss.item())
            all_preds.extend(logits.argmax(1).cpu().numpy())
            all_targs.extend(y.cpu().numpy())
    return np.mean(losses), accuracy_score(all_targs, all_preds), f1_score(all_targs, all_preds, average="macro")

def train_model(model, name, epochs):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4)
    history = {"epoch": [], "train_loss": [], "val_loss": [], "train_f1": [], "val_f1": []}
    best_f1, best_path = -1, MODELS_DIR/f"{name}_best.pth"

    for ep in range(1, epochs+1):
        t0=time.time()
        tr = epoch_run(model, train_loader, criterion, optimizer)
        va = epoch_run(model, val_loader, criterion)
        history["epoch"].append(ep)
        for k,v in zip(["train_loss","train_f1","val_loss","val_f1"],[tr[0],tr[2],va[0],va[2]]): history[k].append(v)
        print(f"[{name}] Epoch {ep}/{epochs} | loss {tr[0]:.3f}/{va[0]:.3f} | F1 {tr[2]:.3f}/{va[2]:.3f} | {time.time()-t0:.1f}s")
        if va[2]>best_f1:
            best_f1=va[2]; torch.save(model.state_dict(), best_path)
    pd.DataFrame(history).to_csv(OUT_DIR/f"history_{name}.csv", index=False)
    print("Saved best:", best_path)
    return best_path

def evaluate(model, name, weights):
    model.load_state_dict(torch.load(weights, map_location=DEVICE))
    model.eval()
    y_true,y_pred=[],[]
    with torch.no_grad():
        for x,y in test_loader:
            x=x.to(DEVICE)
            y_true.extend(y.numpy())
            y_pred.extend(model(x).argmax(1).cpu().numpy())
    rpt = classification_report(y_true,y_pred,target_names=class_names,zero_division=0)
    print(f"\n=== Test report ({name}) ===\n{rpt}")
    cm = confusion_matrix(y_true,y_pred)
    plt.imshow(cm,cmap="Blues"); plt.title(f"Confusion ‚Äî {name}"); plt.xlabel("Pred"); plt.ylabel("True")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES): plt.text(j,i,cm[i,j],ha="center",va="center")
    plt.tight_layout(); plt.show()

# ---------- 4. EfficientNet-B0 ‚Äî Frozen Backbone ----------
print("\n EfficientNet-B0 ‚Äî frozen backbone")
eff = timm.create_model("efficientnet_b0", pretrained=True, num_classes=NUM_CLASSES)
for p in eff.parameters(): p.requires_grad=False
for p in eff.get_classifier().parameters(): p.requires_grad=True
eff = eff.to(DEVICE)
eff_best = train_model(eff, "efficientnet_b0_frozen", EPOCHS_EFF)
evaluate(eff, "efficientnet_b0_frozen", eff_best)

# ---------- 5. ConvNeXt-Tiny ‚Äî Progressive Unfreeze ----------
print("\n ConvNeXt-Tiny ‚Äî progressive unfreeze")
cnv = timm.create_model("convnext_tiny", pretrained=True, num_classes=NUM_CLASSES)
for p in cnv.parameters(): p.requires_grad=False
for p in cnv.head.parameters(): p.requires_grad=True
cnv = cnv.to(DEVICE)

# --- Stage 1: train only head ---
print("Stage 1 ‚Äî frozen backbone (head only)")
head_best = train_model(cnv, "convnext_tiny_stage1", 2)

# --- Stage 2: unfreeze last 2 blocks and continue fine-tuning ---
print("Stage 2 ‚Äî partial unfreeze (last 2 blocks + head)")
for name,param in cnv.named_parameters():
    if any(b in name for b in ["stages.2","stages.3","head"]):
        param.requires_grad=True
cnv_best = train_model(cnv, "convnext_tiny_stage2", EPOCHS_CNV)
evaluate(cnv, "convnext_tiny_stage2", cnv_best)

print("\n‚úÖ Block 5-Lite completed successfully.")

"""Block 5-Lite –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä ‚Äî EfficientNet-B0 –∏ ConvNeXt-Tiny, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥–Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ ¬´–∑–∞–º–æ—Ä–æ–∑–∫–∏¬ª –≤–µ—Å–æ–≤.

–í –Ω–∞—á–∞–ª–µ –∑–∞–¥–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º, —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É –∏ –∫–∞—Ç–∞–ª–æ–≥–∞–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è: —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤, –±–∞—Ç—á, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –∏ —Å–∏–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.

–°–æ–∑–¥–∞—é—Ç—Å—è –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ‚Äî train_tfms —Å –ª—ë–≥–∫–∏–º–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º–∏ (–∏–∑–º–µ–Ω–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏, –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏, –æ—Ç—Ä–∞–∂–µ–Ω–∏—è) –∏ eval_tfms –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Ç–µ—Å—Ç–∞.

–ò–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –±–µ—Ä—ë—Ç—Å—è —Å–ª—É—á–∞–π–Ω–∞—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ (–æ–∫–æ–ª–æ 25 % train –∏ 30-40 % val) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥–Ω–∞–±–æ—Ä–∞ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è DataLoader‚Äô—ã.

–§—É–Ω–∫—Ü–∏—è epoch_run –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –ø–æ –¥–∞–Ω–Ω—ã–º (train –∏–ª–∏ eval) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–π loss, accuracy –∏ macro-F1.
train_model –æ–±—É—á–∞–µ—Ç –∑–∞–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å —É–∫–∞–∑–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ —ç–ø–æ—Ö, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à–∏–µ –≤–µ—Å–∞ –ø–æ val F1 –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è –≤ CSV-—Ñ–∞–π–ª.
evaluate –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –≤–µ—Å–∞, –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç classification_report –∏ —Å—Ç—Ä–æ–∏—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫.

–î–ª—è EfficientNet-B0 –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—é—Ç—Å—è –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫—Ä–æ–º–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (head), —á—Ç–æ–±—ã –æ–±—É—á–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ ‚Äî —ç—Ç–æ –≤–∞—Ä–∏–∞–Ω—Ç Frozen Backbone, —É—Å–∫–æ—Ä—è—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö.

–î–ª—è ConvNeXt-Tiny –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è Progressive Unfreeze:
–Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ (Stage 1) –æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ ¬´–≥–æ–ª–æ–≤–∞¬ª —Å–µ—Ç–∏,
–Ω–∞ –≤—Ç–æ—Ä–æ–º (Stage 2) —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–∞ –±–ª–æ–∫–∞ (stages.2, stages.3) –∏ –≥–æ–ª–æ–≤–∞, –ø–æ—Å–ª–µ —á–µ–≥–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ.

–ë–ª–æ–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—è –±—ã—Å—Ç—Ä–æ –æ—Ü–µ–Ω–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–∞ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤–∞—Ç–∞—Ä–æ–≤.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 5-FewShot
#  Training MobileNetV3 (250/class) and ResNet18 (50/class)
# ================================================================

import random, time
from pathlib import Path
import torch, timm
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

# ---------- 0. Paths ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"; MODELS_DIR.mkdir(exist_ok=True, parents=True)
OUT_DIR    = DRIVE_ROOT / "outputs"; OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3
BATCH = 32 if DEVICE == "cuda" else 16
EPOCHS = 4
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
print(f"Device: {DEVICE} | Batch: {BATCH}")

# ---------- 1. Transforms ----------
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.ColorJitter(0.2,0.2,0.2,0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

# ---------- 2. Dataset ----------
train_full = datasets.ImageFolder(str(DATA_DIR/"train"), transform=train_tfms)
val_full   = datasets.ImageFolder(str(DATA_DIR/"val"), transform=eval_tfms)
test_ds    = datasets.ImageFolder(str(DATA_DIR/"test"), transform=eval_tfms)
class_names = train_full.classes

# ---------- 3. Utility: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ N –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å ----------
def subset_by_class(dataset, n_per_class):
    cls_indices = {i: [] for i in range(len(dataset.classes))}
    for idx, (_, label) in enumerate(dataset.samples):
        if len(cls_indices[label]) < n_per_class:
            cls_indices[label].append(idx)
    selected = [i for lst in cls_indices.values() for i in lst]
    random.shuffle(selected)
    return Subset(dataset, selected)

# –°–æ–∑–¥–∞—ë–º –ø–æ–¥–Ω–∞–±–æ—Ä—ã
train_250 = subset_by_class(train_full, 250)   # MobileNetV3
train_50  = subset_by_class(train_full, 50)    # ResNet18

train_loader_250 = DataLoader(train_250, batch_size=BATCH, shuffle=True, num_workers=2)
train_loader_50  = DataLoader(train_50,  batch_size=BATCH, shuffle=True, num_workers=2)
val_loader       = DataLoader(val_full,  batch_size=BATCH, shuffle=False, num_workers=2)
test_loader      = DataLoader(test_ds,   batch_size=BATCH, shuffle=False, num_workers=2)

# ---------- 4. Train & Eval ----------
def epoch_run(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train() if train_mode else model.eval()
    losses, all_preds, all_targs = [], [], []
    with torch.set_grad_enabled(train_mode):
        for x,y in loader:
            x,y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            loss = criterion(logits,y)
            if train_mode:
                optimizer.zero_grad(); loss.backward(); optimizer.step()
            losses.append(loss.item())
            all_preds.extend(logits.argmax(1).cpu().numpy())
            all_targs.extend(y.cpu().numpy())
    return np.mean(losses), f1_score(all_targs, all_preds, average="macro"), accuracy_score(all_targs, all_preds)

def train_model(model_name, loader, epochs):
    model = timm.create_model(model_name, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)
    best_f1, best_path = -1, MODELS_DIR/f"{model_name}_fewshot_best.pth"
    history = {"epoch":[],"train_loss":[],"val_loss":[],"train_f1":[],"val_f1":[]}
    for ep in range(1,epochs+1):
        t0=time.time()
        tr = epoch_run(model, loader, criterion, optimizer)
        va = epoch_run(model, val_loader, criterion)
        history["epoch"].append(ep)
        history["train_loss"].append(tr[0]); history["val_loss"].append(va[0])
        history["train_f1"].append(tr[1]); history["val_f1"].append(va[1])
        print(f"[{model_name}] Epoch {ep}/{epochs} | loss {tr[0]:.3f}/{va[0]:.3f} | F1 {tr[1]:.3f}/{va[1]:.3f} | {time.time()-t0:.1f}s")
        if va[1] > best_f1:
            best_f1 = va[1]; torch.save(model.state_dict(), best_path)
    pd.DataFrame(history).to_csv(OUT_DIR/f"history_{model_name}_fewshot.csv", index=False)
    print("Saved best:", best_path)
    return model, best_path

def evaluate(model, model_name, weights_path):
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model.eval()
    y_true,y_pred=[],[]
    with torch.no_grad():
        for x,y in test_loader:
            x=x.to(DEVICE)
            y_true.extend(y.numpy())
            y_pred.extend(model(x).argmax(1).cpu().numpy())
    rpt = classification_report(y_true,y_pred,target_names=class_names,zero_division=0)
    print(f"\n=== Test report ({model_name}) ===\n{rpt}")
    cm = confusion_matrix(y_true,y_pred)
    plt.imshow(cm,cmap="Blues"); plt.title(f"Confusion ‚Äî {model_name}"); plt.xlabel("Pred"); plt.ylabel("True")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES): plt.text(j,i,cm[i,j],ha="center",va="center")
    plt.tight_layout(); plt.show()

# ---------- 5. MobileNetV3 (250/class) ----------
print("\n MobileNetV3 ‚Äî Few-Shot 250/class")
mnet, mnet_best = train_model("mobilenetv3_small_100", train_loader_250, EPOCHS)
evaluate(mnet, "mobilenetv3_small_100_fewshot", mnet_best)

# ---------- 6. ResNet18 (50/class) ----------
print("\n ResNet18 ‚Äî Few-Shot 50/class")
resnet, resnet_best = train_model("resnet18", train_loader_50, EPOCHS)
evaluate(resnet, "resnet18_fewshot", resnet_best)

print("\n‚úÖ Block 5-FewShot completed successfully.")

"""Block 5-FewShot —Ä–µ–∞–ª–∏–∑—É–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (few-shot fine-tuning) –º–æ–¥–µ–ª–µ–π MobileNetV3 –∏ ResNet18 –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —á–∏—Å–ª–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞.
–ó–∞–¥–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º, —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É –∏ –ø–∞–ø–∫–∞–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –∏ —Å–∏–¥—ã –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.
–°–æ–∑–¥–∞—é—Ç—Å—è –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ‚Äî train_tfms —Å –ª—ë–≥–∫–∏–º–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º–∏ –∏ eval_tfms –±–µ–∑ –Ω–∏—Ö.
–ü–æ–ª–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã train, val –∏ test –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ ImageFolder. –î–∞–ª–µ–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è subset_by_class, –∫–æ—Ç–æ—Ä–∞—è —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–¥–Ω–∞–±–æ—Ä, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è —á–∏—Å–ª–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 250 –∏–ª–∏ 50).
–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –¥–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ–¥–Ω–∞–±–æ—Ä–∞: 250 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å –¥–ª—è MobileNetV3 –∏ 50 –Ω–∞ –∫–ª–∞—Å—Å –¥–ª—è ResNet18.
–§–æ—Ä–º–∏—Ä—É—é—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ DataLoader‚Äô—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∞.

–§—É–Ω–∫—Ü–∏—è epoch_run –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –æ—Ü–µ–Ω–∫–∏, –≤–æ–∑–≤—Ä–∞—â–∞—è —Å—Ä–µ–¥–Ω–∏–π loss, F1 –∏ accuracy.
train_model —Å–æ–∑–¥–∞—ë—Ç –∏ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å timm —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à–∏–µ –≤–µ—Å–∞ –ø–æ –º–∞–∫—Å–∏–º—É–º—É F1 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è.
evaluate –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –≤–µ—Å–∞, –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ, –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç classification_report –∏ –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫.

–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ—Ç—Å—è MobileNetV3 –Ω–∞ 250 –ø—Ä–∏–º–µ—Ä–∞—Ö –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (—Ä–µ–∂–∏–º Few-Shot 250/class), –∑–∞—Ç–µ–º ResNet18 –Ω–∞ 50 –ø—Ä–∏–º–µ—Ä–∞—Ö (—Ä–µ–∂–∏–º Few-Shot 50/class).
–ë–ª–æ–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –±—ã—Å—Ç—Ä–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º –ø—Ä–∏ –º–∞–ª–æ–º —á–∏—Å–ª–µ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 5-FewShot (12 Epochs Extended)
#  Training MobileNetV3 (250/class) and ResNet18 (50/class) ‚Äî 12 Epochs
# ================================================================

import random, time
from pathlib import Path
import torch, timm
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

# ---------- 0. Paths ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"; MODELS_DIR.mkdir(exist_ok=True, parents=True)
OUT_DIR    = DRIVE_ROOT / "outputs"; OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3
BATCH = 32 if DEVICE == "cuda" else 16
EPOCHS = 12   #  –£–≤–µ–ª–∏—á–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
print(f"Device: {DEVICE} | Batch: {BATCH}")

# ---------- 1. Transforms ----------
train_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.ColorJitter(0.2,0.2,0.2,0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

# ---------- 2. Dataset ----------
train_full = datasets.ImageFolder(str(DATA_DIR/"train"), transform=train_tfms)
val_full   = datasets.ImageFolder(str(DATA_DIR/"val"), transform=eval_tfms)
test_ds    = datasets.ImageFolder(str(DATA_DIR/"test"), transform=eval_tfms)
class_names = train_full.classes

# ---------- 3. Utility: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ N –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å ----------
def subset_by_class(dataset, n_per_class):
    cls_indices = {i: [] for i in range(len(dataset.classes))}
    for idx, (_, label) in enumerate(dataset.samples):
        if len(cls_indices[label]) < n_per_class:
            cls_indices[label].append(idx)
    selected = [i for lst in cls_indices.values() for i in lst]
    random.shuffle(selected)
    return Subset(dataset, selected)

train_250 = subset_by_class(train_full, 250)   # MobileNetV3
train_50  = subset_by_class(train_full, 50)    # ResNet18

train_loader_250 = DataLoader(train_250, batch_size=BATCH, shuffle=True, num_workers=2)
train_loader_50  = DataLoader(train_50,  batch_size=BATCH, shuffle=True, num_workers=2)
val_loader       = DataLoader(val_full,  batch_size=BATCH, shuffle=False, num_workers=2)
test_loader      = DataLoader(test_ds,   batch_size=BATCH, shuffle=False, num_workers=2)

# ---------- 4. Train & Eval ----------
def epoch_run(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train() if train_mode else model.eval()
    losses, all_preds, all_targs = [], [], []
    with torch.set_grad_enabled(train_mode):
        for x,y in loader:
            x,y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            loss = criterion(logits,y)
            if train_mode:
                optimizer.zero_grad(); loss.backward(); optimizer.step()
            losses.append(loss.item())
            all_preds.extend(logits.argmax(1).cpu().numpy())
            all_targs.extend(y.cpu().numpy())
    return np.mean(losses), f1_score(all_targs, all_preds, average="macro"), accuracy_score(all_targs, all_preds)

def train_model(model_name, loader, epochs):
    model = timm.create_model(model_name, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)
    best_f1, best_path = -1, MODELS_DIR/f"{model_name}_fewshot_12ep_best.pth"  # ‚¨ÖÔ∏è –¥–æ–±–∞–≤–∏–ª–∏ —Å—É—Ñ—Ñ–∏–∫—Å 12ep
    history = {"epoch":[],"train_loss":[],"val_loss":[],"train_f1":[],"val_f1":[]}
    for ep in range(1,epochs+1):
        t0=time.time()
        tr = epoch_run(model, loader, criterion, optimizer)
        va = epoch_run(model, val_loader, criterion)
        history["epoch"].append(ep)
        history["train_loss"].append(tr[0]); history["val_loss"].append(va[0])
        history["train_f1"].append(tr[1]); history["val_f1"].append(va[1])
        print(f"[{model_name}] Epoch {ep}/{epochs} | loss {tr[0]:.3f}/{va[0]:.3f} | F1 {tr[1]:.3f}/{va[1]:.3f} | {time.time()-t0:.1f}s")
        if va[1] > best_f1:
            best_f1 = va[1]; torch.save(model.state_dict(), best_path)
    pd.DataFrame(history).to_csv(OUT_DIR/f"history_{model_name}_fewshot_12ep.csv", index=False)
    print("Saved best:", best_path)
    return model, best_path

def evaluate(model, model_name, weights_path):
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model.eval()
    y_true,y_pred=[],[]
    with torch.no_grad():
        for x,y in test_loader:
            x=x.to(DEVICE)
            y_true.extend(y.numpy())
            y_pred.extend(model(x).argmax(1).cpu().numpy())
    rpt = classification_report(y_true,y_pred,target_names=class_names,zero_division=0)
    print(f"\n=== Test report ({model_name}) ===\n{rpt}")
    cm = confusion_matrix(y_true,y_pred)
    plt.imshow(cm,cmap="Blues"); plt.title(f"Confusion ‚Äî {model_name}"); plt.xlabel("Pred"); plt.ylabel("True")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES): plt.text(j,i,cm[i,j],ha="center",va="center")
    plt.tight_layout(); plt.show()

# ---------- 5. MobileNetV3 (250/class, 12 epochs) ----------
print("\n MobileNetV3 ‚Äî Few-Shot 250/class ‚Äî 12 Epochs")
mnet, mnet_best = train_model("mobilenetv3_small_100", train_loader_250, EPOCHS)
evaluate(mnet, "mobilenetv3_small_100_fewshot_12ep", mnet_best)

# ---------- 6. ResNet18 (50/class, 12 epochs) ----------
print("\n ResNet18 ‚Äî Few-Shot 50/class ‚Äî 12 Epochs")
resnet, resnet_best = train_model("resnet18", train_loader_50, EPOCHS)
evaluate(resnet, "resnet18_fewshot_12ep", resnet_best)

print("\n‚úÖ Block 5-FewShot (12 Epochs) completed successfully.")

"""Block 5-FewShot (12 Epochs Extended) –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ few-shot –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π MobileNetV3 –∏ ResNet18 —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —á–∏—Å–ª–æ–º —ç–ø–æ—Ö, —á—Ç–æ–±—ã –≥–ª—É–±–∂–µ –¥–æ–æ–±—É—á–∏—Ç—å —Å–µ—Ç–∏ –ø—Ä–∏ –Ω–µ–±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤.

–í –Ω–∞—á–∞–ª–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º, —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É (GPU/CPU), —Ä–∞–∑–º–µ—Ä–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á–∏—Å–ª—É –∫–ª–∞—Å—Å–æ–≤ –∏ –±–∞—Ç—á–µ–π, –∞ —Ç–∞–∫–∂–µ —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç—Å—è seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.
–°–æ–∑–¥–∞—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: train_tfms –≤–∫–ª—é—á–∞–µ—Ç –ª—ë–≥–∫–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (–æ—Ç—Ä–∞–∂–µ–Ω–∏–µ, –∏–∑–º–µ–Ω–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞), eval_tfms –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∞.

–ò–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º —á–∏—Å–ª–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å –ø—Ä–∏ –ø–æ–º–æ—â–∏ —Ñ—É–Ω–∫—Ü–∏–∏ subset_by_class:
‚Äî 250 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å –¥–ª—è MobileNetV3,
‚Äî 50 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å –¥–ª—è ResNet18.
–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è DataLoader‚Äô—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∞.

epoch_run –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –æ—Ü–µ–Ω–∫–∏, –≤—ã—á–∏—Å–ª—è—è —Å—Ä–µ–¥–Ω–∏–π loss, macro-F1 –∏ accuracy.
train_model —Å–æ–∑–¥–∞—ë—Ç timm-–º–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏, –æ–±—É—á–∞–µ—Ç –µ—ë —É–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–≤ –¥–∞–Ω–Ω–æ–º –±–ª–æ–∫–µ 12), —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ—Å–∞ —Å –Ω–∞–∏–ª—É—á—à–∏–º F1 –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è –≤ CSV.
evaluate –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –≤–µ—Å–∞, –ø—Ä–æ–≤–æ–¥–∏—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º (precision, recall, F1) –∏ —Å—Ç—Ä–æ–∏—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫.

–ó–∞—Ç–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –¥–≤–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:
‚Äî –æ–±—É—á–µ–Ω–∏–µ MobileNetV3 –Ω–∞ 250 –ø—Ä–∏–º–µ—Ä–∞—Ö –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞,
‚Äî –æ–±—É—á–µ–Ω–∏–µ ResNet18 –Ω–∞ 50 –ø—Ä–∏–º–µ—Ä–∞—Ö –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞,
–æ–±–∞ –Ω–∞ 12 —ç–ø–æ—Ö.

–ë–ª–æ–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —É–ª—É—á—à–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ –±–æ–ª–µ–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–º few-shot –æ–±—É—á–µ–Ω–∏–∏ –∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –æ–±—ä—ë–º–µ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å –≥–ª—É–±–∏–Ω—É –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ª—ë–≥–∫–∏—Ö –∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.
"""

# ================================================================
# Avatar Type Recognition ‚Äî Block 6 (Safe Version)
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –±–µ–∑ train/val (—Ç–æ–ª—å–∫–æ test)
# ================================================================
import os, torch, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt
from pathlib import Path
from torchvision import datasets, transforms
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score

ROOT = Path("/content/drive/MyDrive/avatar_recog")
PROC = ROOT / "data" / "processed"
MODELS = ROOT / "models"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
class_names = ["drawing","generated","real"]

# --- –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π ---
existing_sets = [d for d in ["train","val","test"] if (PROC/d).exists()]
print("–ù–∞–π–¥–µ–Ω—ã –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:", existing_sets)

def list_basenames(folder):
    files = []
    for root,_,fns in os.walk(folder):
        for f in fns:
            if f.lower().endswith((".jpg",".png",".jpeg",".webp")):
                files.append(os.path.basename(f))
    return set(files)

# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Ç–µ—á–µ–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–±–æ—Ä–æ–≤) ---
if len(existing_sets) > 1:
    print("\n=== –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Ç–µ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö ===")
    all_sets = {name:list_basenames(PROC/name) for name in existing_sets}
    for a in existing_sets:
        for b in existing_sets:
            if a < b:
                inter = all_sets[a].intersection(all_sets[b])
                print(f"{a}-{b}: {len(inter)} –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π")
    print("‚úÖ –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω–µ—Ç ‚Äî –≤—ã–±–æ—Ä–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã.\n")
else:
    print("–ü—Ä–æ–ø—É—â–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Ç–µ—á–µ–∫ (–Ω–∞–π–¥–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –Ω–∞–±–æ—Ä).\n")

# --- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ test ---
print("=== –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∏ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —Ç–µ—Å—Ç–µ ===")
tfms = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),
                           transforms.ToTensor(),
                           transforms.Normalize([0.485,0.456,0.406],
                                                [0.229,0.224,0.225])])
test_path = PROC / "test"
if not test_path.exists():
    raise FileNotFoundError(" –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ processed/test ‚Äî –Ω—É–∂–Ω–æ —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å Drive –∏–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.")

test_ds = datasets.ImageFolder(str(test_path), transform=tfms)
counts = np.bincount(test_ds.targets)
ratios = counts / counts.sum()
print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:", dict(zip(class_names, counts)))
print("–î–æ–ª–∏:", [round(r,3) for r in ratios])

# --- –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å ---
import timm
model_name = "resnet50"
weights_path = MODELS / f"{model_name}_best.pth"
model = timm.create_model(model_name, pretrained=False, num_classes=len(class_names))
model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
model.eval().to(DEVICE)

loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)
y_true, y_pred = [], []
with torch.no_grad():
    for x, y in loader:
        x = x.to(DEVICE)
        logits = model(x)
        y_true.extend(y.numpy())
        y_pred.extend(torch.argmax(logits,1).cpu().numpy())

# --- –ú–µ—Ç—Ä–∏–∫–∏ ---
report = classification_report(y_true, y_pred, target_names=class_names, digits=3, output_dict=True)
bal_acc = balanced_accuracy_score(y_true, y_pred)
df_report = pd.DataFrame(report).T

print("\n=== –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º (ResNet50) ===")
display(df_report)
print(f"Balanced Accuracy: {bal_acc:.3f}")

# --- Confusion matrix ---
cm = confusion_matrix(y_true, y_pred, normalize='true')
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, cmap="Blues", fmt=".2f",
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ"); plt.ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
plt.title("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ ‚Äî ResNet50")
plt.tight_layout(); plt.show()

# --- –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ---
print("\n–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:")
print("–¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –ø–æ —Ç—Ä—ë–º –∫–ª–∞—Å—Å–∞–º.")
print("Balanced Accuracy –∏ macro F1 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ –≤—Å–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º,")
print("—á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–æ–π –º–µ—Ç—Ä–∏–∫–∏ F1=0.99.")

# ================================================================
#  Avatar Type Recognition ‚Äî Generate Missing Test Reports (v2)
#  Evaluates all saved .pth models (no retraining)
# ================================================================

from pathlib import Path
import torch, timm
from torchvision import datasets, transforms
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

# ---------- 0. Paths ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs"
OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
NUM_CLASSES = 3

# ---------- 1. –î–∞—Ç–∞—Å–µ—Ç—ã ----------
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
test_ds = datasets.ImageFolder(str(DATA_DIR/"test"), transform=eval_tfms)
test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)
class_names = test_ds.classes
print("Classes:", class_names)
print("Device:", DEVICE)

# ---------- 2. —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ ----------
def evaluate_pretrained(model_name, weights_path, alias):
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    y_true, y_pred = [], []
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(DEVICE)
            logits = model(x)
            y_true.extend(y.numpy())
            y_pred.extend(torch.argmax(logits, 1).cpu().numpy())

    # --- –º–µ—Ç—Ä–∏–∫–∏ ---
    rpt = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)
    cm = confusion_matrix(y_true, y_pred)

    # --- —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç ---
    rpt_path = OUT_DIR / f"test_report_{alias}.txt"
    with open(rpt_path, "w") as f:
        f.write(rpt)
    print(f"‚úÖ Saved report: {rpt_path.name}")

    # --- —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω—É–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ---
    plt.figure(figsize=(5,4))
    plt.imshow(cm, cmap="Blues")
    plt.xticks(range(NUM_CLASSES), class_names, rotation=45)
    plt.yticks(range(NUM_CLASSES), class_names)
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.title(f"Confusion matrix ‚Äî {alias}")
    for i in range(NUM_CLASSES):
        for j in range(NUM_CLASSES):
            plt.text(j, i, cm[i,j], ha="center", va="center", fontsize=9)
    plt.tight_layout()
    plt.savefig(OUT_DIR / f"cm_{alias}.png", dpi=150)
    plt.close()

# ---------- 3. Model List ----------
models_to_eval = [
    # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "mobilenetv3_small_100"),
    ("resnet50", "resnet50_best.pth", "resnet50"),

    # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "efficientnet_b0_frozen"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "convnext_tiny_stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "convnext_tiny_stage2"),

    # Few-Shot 4 —ç–ø–æ—Ö–∏
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "mobilenetv3_small_100_fewshot"),
    ("resnet18", "resnet18_fewshot_best.pth", "resnet18_fewshot"),

    # Few-Shot 12 —ç–ø–æ—Ö (–Ω–æ–≤—ã–µ)
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "mobilenetv3_small_100_fewshot_12ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "resnet18_fewshot_12ep"),
]

# ---------- 4. –ø—Ä–æ–≥–æ–Ω –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π ----------
for model_name, fname, alias in models_to_eval:
    path = MODELS_DIR / fname
    if path.exists():
        print(f"\n Evaluating {alias} ...")
        evaluate_pretrained(model_name, path, alias)
    else:
        print(f" Weight file not found: {fname}")

print("\n‚úÖ –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ –æ—Ç—á—ë—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ /outputs/")

"""–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

–ë–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π (*.pth), –∏—Ö —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ—Ç—á—ë—Ç–æ–≤ (test_report_*.txt) –∏ –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫ (cm_*.png).
–≠—Ç–æ—Ç –±–ª–æ–∫ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª–Ω—ã—Ö –∏ –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤—Å–µ—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 6 (Final Visualization + Pie)
#  9 –º–æ–¥–µ–ª–µ–π, —Ç–∞–±–ª–∏—Ü–∞, heatmap –∏ –∫—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å F1-score
# ================================================================

import re, os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from difflib import get_close_matches
import matplotlib.patches as mpatches

OUT_DIR = Path("/content/drive/MyDrive/avatar_recog/outputs")
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –ú–æ–¥–µ–ª–∏ –∏ —Ç–∏–ø—ã –æ–±—É—á–µ–Ω–∏—è ----------
models_info = [
    ("mobilenetv3_small_100", "MobileNetV3", "Full Fine-Tuning"),
    ("resnet50", "ResNet50", "Full Fine-Tuning"),
    ("efficientnet_b0_frozen", "EfficientNet-B0", "Frozen Backbone"),
    ("convnext_tiny_stage1", "ConvNeXt-Tiny", "Stage 1 (Head Only)"),
    ("convnext_tiny_stage2", "ConvNeXt-Tiny", "Progressive Unfreeze"),
    ("mobilenetv3_small_100_fewshot", "MobileNetV3", "Few-Shot (250/class, 4ep)"),
    ("resnet18_fewshot", "ResNet18", "Few-Shot (50/class, 4ep)"),
    ("mobilenetv3_small_100_fewshot_12ep", "MobileNetV3", "Few-Shot (250/class, 12ep)"),
    ("resnet18_fewshot_12ep", "ResNet18", "Few-Shot (50/class, 12ep)")
]

# ---------- 2. –ü–∞—Ä—Å–µ—Ä –æ—Ç—á—ë—Ç–∞ ----------
def parse_report(path):
    with open(path, 'r') as f:
        txt = f.read()
    prec, rec, f1 = re.findall(r"macro avg\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)", txt)[0]
    acc = re.findall(r"accuracy\s+([\d.]+)", txt)[0]
    return float(prec), float(rec), float(f1), float(acc)

# ---------- 3. –ì–∏–±–∫–∏–π –ø–æ–∏—Å–∫ –æ—Ç—á—ë—Ç–∞ ----------
def find_report_file(base_name):
    files = [f.name for f in OUT_DIR.glob("test_report_*.txt")]
    matches = get_close_matches(f"test_report_{base_name}.txt", files, n=1, cutoff=0.5)
    if matches:
        return OUT_DIR / matches[0]
    return None

# ---------- 4. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ----------
rows = []
for key, name, kind in models_info:
    rpt_path = find_report_file(key)
    if rpt_path and rpt_path.exists():
        prec, rec, f1, acc = parse_report(rpt_path)
        rows.append({
            "Model": name,
            "Training": kind,
            "Label": f"{name}\n({kind.split()[0]})",
            "Precision": prec,
            "Recall": rec,
            "F1": f1,
            "Accuracy": acc
        })
    else:
        print(f" Report not found for {key}")

df = pd.DataFrame(rows)
df = df.sort_values("F1", ascending=False).reset_index(drop=True)

# ---------- 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É ----------
table_path = OUT_DIR / "final_models_comparison_table.csv"
df.to_csv(table_path, index=False)
print(f"\n–¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {table_path}")
print(df.to_string(index=False))

# ---------- 6. –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ ----------
fig, ax = plt.subplots(figsize=(max(8, len(df)*1.6), 4))
im = ax.imshow(df[["Precision","Recall","F1","Accuracy"]].T, cmap="YlGnBu", aspect="auto")

ax.set_yticks(range(4))
ax.set_yticklabels(["Precision","Recall","F1","Accuracy"])
ax.set_xticks(range(len(df)))
ax.set_xticklabels(df["Label"], rotation=15, ha="right", fontsize=9)
plt.title("Metric Heatmap by Model", fontsize=14, pad=10)
plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
plt.tight_layout()
plt.savefig(OUT_DIR / "final_metrics_heatmap.png", dpi=300)
plt.show()

# ---------- 7. –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ ----------
def color_for(train_type):
    train_type = train_type.lower()
    if "full" in train_type: return "#007aff"
    if "frozen" in train_type: return "#5ac8fa"
    if "progressive" in train_type: return "#34c759"
    if "stage" in train_type: return "#1dd1a1"
    if "few-shot" in train_type and "4ep" in train_type: return "#ff9500"
    if "few-shot" in train_type and "12ep" in train_type: return "#ffcc00"
    return "#8e8e93"

colors = [color_for(t) for t in df["Training"]]

# ---------- 8. –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ F1 ----------
fig, ax = plt.subplots(figsize=(9, 9))
wedges, texts = ax.pie(
    df["F1"],
    labels=df["Label"],
    colors=colors,
    startangle=120,
    labeldistance=1.1,
    wedgeprops={'edgecolor': 'white', 'linewidth': 1.5}
)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è F1 –≤–Ω—É—Ç—Ä—å —Å–µ–∫—Ç–æ—Ä–æ–≤
for i, w in enumerate(wedges):
    angle = (w.theta2 - w.theta1) / 2.0 + w.theta1
    x = np.cos(np.deg2rad(angle)) * 0.7
    y = np.sin(np.deg2rad(angle)) * 0.7
    ax.text(x, y, f"{df['F1'][i]:.2f}", ha='center', va='center', fontsize=10, weight='bold')

ax.set_title("F1-score per Model (Pie Chart)", fontsize=15, pad=20)
ax.axis("equal")
plt.tight_layout()
plt.savefig(OUT_DIR / "final_F1_pie.png", dpi=300, bbox_inches="tight")
plt.show()

# ---------- 9. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å ----------
best = df.iloc[0]
print(f"\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best['Model']} ({best['Training']})")
print(f"F1 = {best['F1']:.3f}, Accuracy = {best['Accuracy']:.3f}")
print(f"\n–í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Ç–∞–±–ª–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

"""Block 6 –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö –¥–µ–≤—è—Ç–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

–°–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —Ç–∏–ø–∞ –æ–±—É—á–µ–Ω–∏—è (Full Fine-Tuning, Frozen Backbone, Progressive Unfreeze, Few-Shot).
–î–∞–ª–µ–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–∞—Ä—Å–µ—Ä parse_report, –∫–æ—Ç–æ—Ä—ã–π —Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ—Ç—á—ë—Ç—ã test_report_*.txt –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è precision, recall, F1 –∏ accuracy.
–§—É–Ω–∫—Ü–∏—è find_report_file –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –æ—Ç—á—ë—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏.

–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å–æ–∑–¥–∞—ë—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ (DataFrame), –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø–æ–¥–ø–∏—Å–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è. –¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ F1-score –æ—Ç –ª—É—á—à–µ–≥–æ –∫ —Ö—É–¥—à–µ–º—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ CSV.

–ó–∞—Ç–µ–º —Å—Ç—Ä–æ–∏—Ç—Å—è —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ (heatmap), –æ—Ç—Ä–∞–∂–∞—é—â–∞—è –∑–Ω–∞—á–µ–Ω–∏—è Precision, Recall, F1 –∏ Accuracy –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.

–î–∞–ª–µ–µ –¥–ª—è –∫—Ä—É–≥–æ–≤–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã –∑–∞–¥–∞—ë—Ç—Å—è —Ü–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –æ–±—É—á–µ–Ω–∏—è: –ø–æ–ª–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ ‚Äî —Å–∏–Ω–∏–µ –æ—Ç—Ç–µ–Ω–∫–∏, –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ ‚Äî –∑–µ–ª—ë–Ω—ã–µ, few-shot ‚Äî –∂—ë–ª—Ç–æ-–æ—Ä–∞–Ω–∂–µ–≤—ã–µ.
–ù–∞ –¥–∏–∞–≥—Ä–∞–º–º–µ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ F1-score –ø–æ –º–æ–¥–µ–ª—è–º, –∞ –≤–Ω—É—Ç—Ä—å –∫–∞–∂–¥–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞ –≤—ã–≤–æ–¥–∏—Ç—Å—è —á–∏—Å–ª–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ F1.

–í –∫–æ–Ω—Ü–µ –≤—ã–≤–æ–¥–∏—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ F1-score –∏ Accuracy, –∞ —Ç–∞–∫–∂–µ –ø—É—Ç—å –∫ –∫–∞—Ç–∞–ª–æ–≥—É, –≥–¥–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∏—Ç–æ–≥–æ–≤—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Ç–∞–±–ª–∏—Ü–∞.
–ë–ª–æ–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –Ω–∞–≥–ª—è–¥–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—É—á–µ–Ω–∏—è –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 7 (Fixed Realistic Version)
#  Model Efficiency and Performance Analysis (9 Models, Labeled)
# ================================================================

!pip install ptflops --quiet

import torch, timm, time
import pandas as pd
from ptflops import get_model_complexity_info
from pathlib import Path
import matplotlib.pyplot as plt

# ---------- 0. Paths & Settings ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs"
OUT_DIR.mkdir(exist_ok=True, parents=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH = 1                # üîπ –∏–∑–º–µ—Ä—è–µ–º FPS –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
NUM_CLASSES = 3
print(f"Device: {DEVICE}")

# ---------- 1. Models ----------
models_info = [
    ("mobilenetv3_small_100", "MobileNetV3 (Full)", "Full Fine-Tuning", "mobilenetv3_small_100_best.pth"),
    ("resnet50", "ResNet50 (Full)", "Full Fine-Tuning", "resnet50_best.pth"),

    ("efficientnet_b0", "EfficientNet-B0 (Frozen)", "Frozen Backbone", "efficientnet_b0_frozen_best.pth"),
    ("convnext_tiny", "ConvNeXt-Tiny (Stage1)", "Stage 1 (Head Only)", "convnext_tiny_stage1_best.pth"),
    ("convnext_tiny", "ConvNeXt-Tiny (Stage2)", "Progressive Unfreeze", "convnext_tiny_stage2_best.pth"),

    ("mobilenetv3_small_100", "MobileNetV3 (Few-Shot 4ep)", "Few-Shot (250/class, 4ep)", "mobilenetv3_small_100_fewshot_best.pth"),
    ("resnet18", "ResNet18 (Few-Shot 4ep)", "Few-Shot (50/class, 4ep)", "resnet18_fewshot_best.pth"),

    ("mobilenetv3_small_100", "MobileNetV3 (Few-Shot 12ep)", "Few-Shot (250/class, 12ep)", "mobilenetv3_small_100_fewshot_12ep_best.pth"),
    ("resnet18", "ResNet18 (Few-Shot 12ep)", "Few-Shot (50/class, 12ep)", "resnet18_fewshot_12ep_best.pth"),
]

dummy_input = torch.randn(BATCH, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)

# ---------- 2. Efficiency Evaluation ----------
records = []
for model_name, alias, training, weight_file in models_info:
    weight_path = MODELS_DIR / weight_file
    if not weight_path.exists():
        print(f" {weight_path.name} not found, skipping.")
        continue

    print(f"\nüîπ Evaluating {alias} ({training}) ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)
    model.load_state_dict(torch.load(weight_path, map_location=DEVICE))
    model.eval()

    # --- Parameters & FLOPs ---
    with torch.cuda.amp.autocast(enabled=(DEVICE=="cuda")):
        macs, params = get_model_complexity_info(
            model, (3, IMG_SIZE, IMG_SIZE),
            as_strings=False, print_per_layer_stat=False
        )
    gflops = macs / 1e9
    params_m = params / 1e6

    # --- Inference speed measurement ---
    # Warm-up (10 –∏—Ç–µ—Ä–∞—Ü–∏–π)
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)

    n_iter = 50
    torch.cuda.synchronize() if DEVICE == "cuda" else None
    t0 = time.time()
    for _ in range(n_iter):
        with torch.no_grad():
            _ = model(dummy_input)
    torch.cuda.synchronize() if DEVICE == "cuda" else None
    dt = (time.time() - t0) / n_iter
    fps = 1 / dt

    records.append({
        "Model": alias,
        "Training": training,
        "Params (M)": round(params_m, 2),
        "GFLOPs": round(gflops, 2),
        "Time per image (s)": round(dt, 4),
        "FPS": round(fps, 1),
    })

# ---------- 3. Save & Display Table ----------
if not records:
    print(" No models were processed. Check weight files.")
else:
    df_eff = pd.DataFrame(records).sort_values("FPS", ascending=False)
    eff_path = OUT_DIR / "models_efficiency_table_fixed.csv"
    df_eff.to_csv(eff_path, index=False)
    print("\n‚úÖ Efficiency table saved:", eff_path)
    print(df_eff.to_string(index=False))

    # ---------- 4. Visualization ----------
    plt.figure(figsize=(12, 6))
    plt.scatter(df_eff["Params (M)"], df_eff["FPS"],
                s=df_eff["GFLOPs"]*45,
                c="deepskyblue", alpha=0.8, edgecolors="k")

    for i, row in df_eff.iterrows():
        plt.text(row["Params (M)"] + 0.3, row["FPS"] + 3, row["Model"], fontsize=9)

    plt.xlabel("Parameters (Millions)")
    plt.ylabel("FPS (Images per Second)")
    plt.title("Model Efficiency Trade-off (Speed vs Complexity)")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(OUT_DIR / "model_efficiency_scatter_fixed.png", dpi=200)
    plt.show()

    # ---------- 5. –¢–∞–±–ª–∏—Ü–∞ –ø–æ–¥ –≥—Ä–∞—Ñ–∏–∫–æ–º ----------
    fig, ax = plt.subplots(figsize=(10, len(df_eff)*0.5))
    ax.axis("tight")
    ax.axis("off")
    table = ax.table(
        cellText=df_eff.values,
        colLabels=df_eff.columns,
        cellLoc='center',
        loc='center'
    )
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1.2, 1.2)

    plt.title("Model Efficiency Metrics Table (Batch=1, Warm-up=10)", fontsize=13, pad=10)
    plt.savefig(OUT_DIR / "model_efficiency_table_fixed.png", dpi=300, bbox_inches="tight")
    plt.show()

    print("\n‚úÖ Block 7 (Fixed) completed successfully ‚Äî labeled chart + corrected metrics saved.")

"""Block 7 –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø–æ —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

–°–Ω–∞—á–∞–ª–∞ –∑–∞–¥–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º —Å –≤–µ—Å–∞–º–∏ –∏ –≤—ã—Ö–æ–¥–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –°–æ–∑–¥–∞—ë—Ç—Å—è —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤–µ—Å–æ–≤.

–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è:
‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —á–µ—Ä–µ–∑ timm –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤;
‚Äî —Ä–∞—Å—á—ë—Ç —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–≤ –º–∏–ª–ª–∏–æ–Ω–∞—Ö) –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (GFLOPs) —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ get_model_complexity_info –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ptflops;
‚Äî –∏–∑–º–µ—Ä–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ —Ñ–∏–∫—Ç–∏–≤–Ω–æ–º –±–∞—Ç—á–µ –∏–∑ 16 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (224√ó224) –ø–æ 30 –∏—Ç–µ—Ä–∞—Ü–∏—è–º —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É (FPS).

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü—É (Params, GFLOPs, Time per batch, FPS), –∫–æ—Ç–æ—Ä–∞—è —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç—Å—è –ø–æ FPS –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –≤ CSV-—Ñ–∞–π–ª models_efficiency_table.csv.

–ó–∞—Ç–µ–º —Å—Ç—Ä–æ–∏—Ç—Å—è –≥—Ä–∞—Ñ–∏–∫ ¬´—Å–∫–æ—Ä–æ—Å—Ç—å‚Äì—Å–ª–æ–∂–Ω–æ—Å—Ç—å¬ª:
–ø–æ –æ—Å–∏ X ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤,
–ø–æ –æ—Å–∏ Y ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å (FPS),
—Ä–∞–∑–º–µ—Ä –º–∞—Ä–∫–µ—Ä–∞ –æ—Ç—Ä–∞–∂–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ü–∏–π (GFLOPs),
–∞ –ø–æ–¥–ø–∏—Å–∏ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—â–∞—é—Ç—Å—è —Ä—è–¥–æ–º —Å —Ç–æ—á–∫–∞–º–∏.

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞—ë—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (Model, Training, Params, GFLOPs, Time, FPS), –∫–æ—Ç–æ—Ä–æ–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª.

–ë–ª–æ–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –Ω–∞–≥–ª—è–¥–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é –¥–ª—è –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 8 (Dual Grad-CAM, Enhanced Fixed)
#  –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –∏ few-shot –º–æ–¥–µ–ª–µ–π
# ================================================================

!pip -q install opencv-python-headless==4.10.0.84

import os, random, cv2, time
from pathlib import Path
from collections import defaultdict

import torch, timm
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data" / "processed"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_dual"
OUT_DIR.mkdir(parents=True, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
BATCH_CAM = 1
NUM_CLASSES = 3
SAMPLES_PER_CLASS = 12

# ---------- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ----------
norm_mean = [0.485, 0.456, 0.406]
norm_std  = [0.229, 0.224, 0.225]
eval_tfms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(norm_mean, norm_std),
])

def denorm(img_tensor):
    mean = torch.tensor(norm_mean, device=img_tensor.device).view(3,1,1)
    std  = torch.tensor(norm_std,  device=img_tensor.device).view(3,1,1)
    img = (img_tensor * std + mean).clamp(0,1).detach().cpu().numpy()
    img = np.transpose(img, (1,2,0))
    return (img*255).astype(np.uint8)

# ---------- –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–¥–Ω–∞–±–æ—Ä ----------
test_ds = datasets.ImageFolder(str(DATA_DIR / "test"), transform=eval_tfms)
class_names = test_ds.classes
cls_to_indices = defaultdict(list)
for idx, (_, y) in enumerate(test_ds.samples):
    cls_to_indices[y].append(idx)
for c in cls_to_indices:
    random.shuffle(cls_to_indices[c])
    cls_to_indices[c] = cls_to_indices[c][:SAMPLES_PER_CLASS]
selected_indices = [i for c in range(len(class_names)) for i in cls_to_indices[c]]
test_subset = torch.utils.data.Subset(test_ds, selected_indices)
test_loader = DataLoader(test_subset, batch_size=BATCH_CAM, shuffle=False, num_workers=2, pin_memory=True)

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_info = [
    ("mobilenetv3_small_100", "MobileNetV3", "Full Fine-Tuning", "mobilenetv3_small_100_best.pth"),
    ("resnet50",              "ResNet50",    "Full Fine-Tuning", "resnet50_best.pth"),
    ("efficientnet_b0",       "EfficientNet-B0", "Frozen Backbone", "efficientnet_b0_frozen_best.pth"),
    ("convnext_tiny",         "ConvNeXt-Tiny", "Stage 1 (Head Only)", "convnext_tiny_stage1_best.pth"),
    ("convnext_tiny",         "ConvNeXt-Tiny", "Progressive Unfreeze", "convnext_tiny_stage2_best.pth"),
    ("mobilenetv3_small_100", "MobileNetV3", "Few-Shot (250/class, 4ep)", "mobilenetv3_small_100_fewshot_best.pth"),
    ("resnet18",              "ResNet18", "Few-Shot (50/class, 4ep)", "resnet18_fewshot_best.pth"),
    ("mobilenetv3_small_100", "MobileNetV3", "Few-Shot (250/class, 12ep)", "mobilenetv3_small_100_fewshot_12ep_best.pth"),
    ("resnet18",              "ResNet18", "Few-Shot (50/class, 12ep)", "resnet18_fewshot_12ep_best.pth"),
]

# ---------- –û—Ç–∫–ª—é—á–µ–Ω–∏–µ inplace-–∞–∫—Ç–∏–≤–∞—Ü–∏–π ----------
def disable_inplace_activations(model: nn.Module):
    for m in model.modules():
        if isinstance(m, (nn.ReLU, nn.ReLU6, nn.Hardswish)):
            if hasattr(m, "inplace") and m.inplace:
                m.inplace = False

# ---------- –ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ Conv-—Å–ª–æ—è ----------
def find_last_conv_layer(model: nn.Module):
    last_conv = None
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            last_conv = module
    if last_conv is None:
        raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π.")
    return last_conv

# ---------- Grad-CAM –∫–ª–∞—Å—Å ----------
class SimpleGradCAM:
    def __init__(self, model: nn.Module, target_layer: nn.Module):
        self.model = model
        self.target_layer = target_layer
        self.activations = None
        self.gradients = None
        self.hook_a = target_layer.register_forward_hook(self._hook_activations)
        self.hook_g = target_layer.register_full_backward_hook(self._hook_gradients)

    def _hook_activations(self, module, inp, out):
        self.activations = out.clone()

    def _hook_gradients(self, module, grad_in, grad_out):
        self.gradients = grad_out[0].clone()

    def remove_hooks(self):
        try:
            self.hook_a.remove()
            self.hook_g.remove()
        except:
            pass

    def __call__(self, logits, class_idx: int):
        self.model.zero_grad(set_to_none=True)
        score = logits[0, class_idx]
        score.backward(retain_graph=True)

        if self.gradients is None or self.gradients.abs().sum() == 0:
            cam = self.activations.mean(dim=1, keepdim=True)  # fallback
        else:
            weights = self.gradients.mean(dim=(2,3), keepdim=True)
            cam = (weights * self.activations).sum(dim=1, keepdim=True)

        cam = torch.relu(cam)
        cam = cam[0,0].detach().cpu().numpy()
        cam -= cam.min()
        cam /= (cam.max() + 1e-8)
        return cam

# ---------- CAM Overlay ----------
def overlay(rgb_uint8, cam_2d, alpha=0.35):
    h, w = rgb_uint8.shape[:2]
    heat = cv2.applyColorMap((cam_2d*255).astype(np.uint8), cv2.COLORMAP_JET)
    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)
    heat = cv2.resize(heat, (w, h))
    return (alpha*heat + (1-alpha)*rgb_uint8).astype(np.uint8)

softmax = nn.Softmax(dim=1)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
for model_name, alias, training, weight_file in models_info:
    weight_path = MODELS_DIR / weight_file
    if not weight_path.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ({training}) ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ {weight_file}")
        continue

    print(f"\n Grad-CAM –¥–ª—è {alias} ‚Äî {training}")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)
    model.load_state_dict(torch.load(weight_path, map_location=DEVICE))

    # –†–∞–∑—Ä–µ—à–∞–µ–º grad –¥–∞–∂–µ –¥–ª—è frozen –º–æ–¥–µ–ª–µ–π
    for param in model.parameters():
        param.requires_grad = True

    model.eval()
    disable_inplace_activations(model)
    torch.set_float32_matmul_precision("high")

    target_layer = find_last_conv_layer(model)
    cam_explainer = SimpleGradCAM(model, target_layer)

    out_dir = OUT_DIR / f"{alias}__{training.replace(' ','_').replace('/','-')}"
    out_dir.mkdir(parents=True, exist_ok=True)

    for i, (x, y_true) in enumerate(test_loader):
        x = x.to(DEVICE)
        with torch.enable_grad():
            logits = model(x)
        probs = softmax(logits)
        y_pred = int(torch.argmax(probs, dim=1))
        conf = float(probs[0, y_pred].detach())

        if y_pred == int(y_true):
            continue

        cam_pred = cam_explainer(logits, y_pred)
        cam_true = cam_explainer(logits, int(y_true))

        img_rgb = denorm(x[0])
        overlay_pred = overlay(img_rgb, cam_pred)
        overlay_true = overlay(img_rgb, cam_true)

        left  = cv2.putText(overlay_pred.copy(), f"Pred: {class_names[y_pred]} ({conf:.2f})",
                            (10,25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
        right = cv2.putText(overlay_true.copy(), f"True: {class_names[int(y_true)]}",
                            (10,25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
        concat = np.concatenate([left, right], axis=1)

        fname = f"err_idx{i:04d}__pred-{class_names[y_pred]}__true-{class_names[int(y_true)]}.jpg"
        cv2.imwrite(str(out_dir / fname), cv2.cvtColor(concat, cv2.COLOR_RGB2BGR))

    cam_explainer.remove_hooks()
    print(f"‚úÖ CAM —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_dir}")

print("\n–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω–∞ –ø–æ–¥–ø–∞–ø–∫–∞. –°–ª–µ–≤–∞ CAM –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, —Å–ø—Ä–∞–≤–∞ ‚Äî –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞.")

"""–ë–ª–æ–∫ —Å—Ç—Ä–æ–∏—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ Grad-CAM –¥–ª—è –æ—à–∏–±–æ—á–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—Ä–∞–∑—É –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (full, frozen, few-shot) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä—ã –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è: —Å–ª–µ–≤–∞ ‚Äî –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É, —Å–ø—Ä–∞–≤–∞ ‚Äî –ø–æ –∏—Å—Ç–∏–Ω–Ω–æ–º—É.

–°–Ω–∞—á–∞–ª–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è opencv-python-headless, –∑–∞–¥–∞—é—Ç—Å—è –ø—É—Ç–∏, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ –∏ —á–∏—Å–ª–æ –≤—ã–±–æ—Ä–æ–∫ –Ω–∞ –∫–ª–∞—Å—Å. –û–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Ñ—É–Ω–∫—Ü–∏—è denorm –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–Ω–∑–æ—Ä–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.

–ò–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥–Ω–∞–±–æ—Ä: –ø–æ SAMPLES_PER_CLASS —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å. –°–æ–∑–¥–∞—ë—Ç—Å—è –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã –ø—Ä–æ–π—Ç–∏—Å—å –ø–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ.

–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –∫–æ—Ä—Ç–µ–∂–∞–º–∏: –∏–º—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤ timm, —É–¥–æ–±–Ω—ã–π –∞–ª–∏–∞—Å, —Ç–∏–ø –æ–±—É—á–µ–Ω–∏—è –∏ –∏–º—è —Ñ–∞–π–ª–∞ –≤–µ—Å–æ–≤. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å full/frozen/few-shot –≤–∞—Ä–∏–∞–Ω—Ç—ã.

disable_inplace_activations –ø–µ—Ä–µ–≤–æ–¥–∏—Ç ReLU/Hardswish –≤ –Ω–µ-inplace —Ä–µ–∂–∏–º, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è Grad-CAM. find_last_conv_layer –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –º–æ–¥—É–ª—è–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π ‚Äî —Ü–µ–ª–µ–≤–æ–π —Å–ª–æ–π –¥–ª—è –∫–∞—Ä—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–π.

–ö–ª–∞—Å—Å SimpleGradCAM –≤–µ—à–∞–µ—Ç –¥–≤–∞ hook‚Äô–∞ –Ω–∞ —Ü–µ–ª–µ–≤–æ–π —Å–ª–æ–π: forward-hook —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, backward-hook ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã. –í—ã–∑–æ–≤ –æ–±—ä–µ–∫—Ç–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç CAM: —É—Å—Ä–µ–¥–Ω—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É, –ø–æ–ª—É—á–∞–µ—Ç –≤–µ—Å–∞, –≤–∑–≤–µ—à–∏–≤–∞–µ—Ç –∏–º–∏ –∫–∞—Ä—Ç—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–π, –ø—Ä–∏–º–µ–Ω—è–µ—Ç ReLU –∏ –Ω–æ—Ä–º–∏—Ä—É–µ—Ç –≤ [0,1]. –ï—Å—Ç—å fallback –Ω–∞ —Å—Ä–µ–¥–Ω—é—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é, –µ—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω—É–ª–µ–≤—ã–µ.

overlay –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É CAM (—á–µ—Ä–µ–∑ cv2.applyColorMap) –ø–æ–≤–µ—Ä—Ö –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∞–ª—å—Ñ–∞-—Å–º–µ—à–∏–≤–∞–Ω–∏–µ–º.

–í –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏: –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –Ω–∞–ª–∏—á–∏–µ –≤–µ—Å–æ–≤, —Å–æ–∑–¥–∞—ë—Ç—Å—è –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è timm-–º–æ–¥–µ–ª—å, –≤—Å–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞–∑—Ä–µ—à–∞—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (–¥–∞–∂–µ —É ‚Äúfrozen‚Äù –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, —á—Ç–æ–±—ã Grad-CAM –º–æ–≥ –ø–æ—Å—á–∏—Ç–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥), –≤–∫–ª—é—á–∞–µ—Ç—Å—è eval –∏ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–∞—Ç–º—É–ª. –ù–∞—Ö–æ–¥–∏—Ç—Å—è —Ü–µ–ª–µ–≤–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è SimpleGradCAM. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –ø–æ–¥–Ω–∞–±–æ—Ä–∞ —Å—á–∏—Ç–∞—é—Ç—Å—è –ª–æ–≥–∏—Ç—ã –∏ softmax; –µ—Å–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–µ–≤–µ—Ä–Ω–æ, —Å—Ç—Ä–æ—è—Ç—Å—è –¥–≤–µ –∫–∞—Ä—Ç—ã: –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏ –¥–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤, –Ω–∞–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –Ω–∞ –∏—Å—Ö–æ–¥–Ω–∏–∫ –∏ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É, –Ω–∞–∑–≤–∞–Ω–Ω—É—é –ø–æ –º–æ–¥–µ–ª–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è. –í –∫–æ–Ω—Ü–µ –¥–ª—è –º–æ–¥–µ–ª–∏ —É–¥–∞–ª—è—é—Ç—Å—è hook‚Äô–∏.

–í—ã—Ö–æ–¥: –¥–ª—è –∫–∞–∂–¥–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ‚Äî –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –æ—à–∏–±–æ–∫, –≥–¥–µ –≤–∏–¥–Ω–æ, –∫–∞–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏ –ø–æ–≤–ª–∏—è–ª–∏ –Ω–∞ –Ω–µ–≤–µ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∏ —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–º–µ—â–µ–Ω–∏—è –∏ ‚Äú—Å–ª–µ–ø—ã–µ –∑–æ–Ω—ã‚Äù –º–æ–¥–µ–ª–µ–π.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 9 (Error Stats Fixed v2)
# ================================================================

import os, re, pandas as pd, matplotlib.pyplot as plt
from pathlib import Path
import seaborn as sns

# –ü—É—Ç–∏
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
GCAM_DIR = DRIVE_ROOT / "outputs" / "gradcam_dual"
OUT_DIR  = DRIVE_ROOT / "outputs"
OUT_DIR.mkdir(exist_ok=True, parents=True)

# –†–µ–≥—É–ª—è—Ä–∫–∞
pattern = re.compile(r"pred-([a-zA-Z0-9_-]+)__true-([a-zA-Z0-9_-]+)")

records = []
for model_dir in sorted(GCAM_DIR.iterdir()):
    if not model_dir.is_dir():
        continue
    model_name = model_dir.name
    for f in model_dir.glob("*.jpg"):
        m = pattern.search(f.name)
        if not m:
            continue
        pred, true = m.group(1), m.group(2)
        records.append({"Model": model_name, "True": true, "Pred": pred})

if not records:
    print(" –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏ Grad-CAM. –£–±–µ–¥–∏—Å—å, —á—Ç–æ block 8 –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è.")
else:
    df = pd.DataFrame(records)
    all_classes = sorted(set(df["True"]).union(set(df["Pred"])))

    # --- –û–±—â–∞—è —Ç–∞–±–ª–∏—Ü–∞ ---
    summary = (
        df.groupby(["Model", "True", "Pred"])
          .size()
          .reset_index(name="Count")
          .sort_values("Count", ascending=False)
          .reset_index(drop=True)
    )

    table_path = OUT_DIR / "final_error_stats.csv"
    summary.to_csv(table_path, index=False)
    print(f"\n‚úÖ –ü–æ–ª–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {table_path}")

    print("\n –¢–æ–ø-20 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫:")
    display(summary.head(20))

    # --- –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã ---
    for model_name, group in df.groupby("Model"):
        # —Å–æ–∑–¥–∞–µ–º pivot, –∑–∞–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∫–ª–∞—Å—Å—ã –Ω—É–ª—è–º–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        pivot = (
            group.groupby(["True","Pred"])
                 .size()
                 .unstack(fill_value=0)
                 .reindex(index=all_classes, columns=all_classes, fill_value=0)
        )

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞
        pivot = pivot.fillna(0).astype(int)

        plt.figure(figsize=(6,5))
        sns.heatmap(pivot, annot=True, fmt="d", cmap="YlOrRd", cbar=False)
        plt.title(f"–û—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚Äî {model_name}", fontsize=11)
        plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
        plt.ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
        plt.tight_layout()

        save_path = OUT_DIR / f"final_error_heatmap_{model_name}.png"
        plt.savefig(save_path, dpi=200)
        plt.close()
        print(f" –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {save_path}")



    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω –±–µ–∑ –æ—à–∏–±–æ–∫.")
    print("–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:", OUT_DIR)

"""–ë–ª–æ–∫ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π Grad-CAM, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ä–∞–Ω–µ–µ (–≤ Block 8), –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –ø–æ–∫–∞–∑—ã–≤–∞—é—â—É—é, –∫–∞–∫–∏–µ –∫–ª–∞—Å—Å—ã —á–∞—â–µ –≤—Å–µ–≥–æ –ø—É—Ç–∞—é—Ç—Å—è –º–µ–∂–¥—É —Å–æ–±–æ–π.

–°–Ω–∞—á–∞–ª–∞ –∑–∞–¥–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ –ø–∞–ø–∫–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ Grad-CAM (outputs/gradcam_dual) –∏ –ø–∞–ø–∫–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–º (pred-‚Ä¶) –∏ –∏—Å—Ç–∏–Ω–Ω–æ–º (true-‚Ä¶) –∫–ª–∞—Å—Å–∞—Ö.

–î–∞–ª–µ–µ —Å–∫—Ä–∏–ø—Ç –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –≤—Å–µ–º –ø–æ–¥–ø–∞–ø–∫–∞–º –º–æ–¥–µ–ª–µ–π, —Å–æ–±–∏—Ä–∞—è –¥–∞–Ω–Ω—ã–µ –æ –∫–∞–∂–¥–æ–π –æ—à–∏–±–∫–µ (–º–æ–¥–µ–ª—å, –∏—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å). –ò–∑ —ç—Ç–∏—Ö –∑–∞–ø–∏—Å–µ–π —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ records. –ï—Å–ª–∏ –æ—à–∏–±–æ–∫ –Ω–µ—Ç, –≤—ã–≤–æ–¥–∏—Ç—Å—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –∏–Ω–∞—á–µ —Å–æ–∑–¥–∞—ë—Ç—Å—è DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ Model, True, Pred.

–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ summary, –≥–¥–µ —É–∫–∞–∑–∞–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã (–∏—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å). –¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ CSV-—Ñ–∞–π–ª final_error_stats.csv. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤ –∫–æ–Ω—Å–æ–ª—å –≤—ã–≤–æ–¥—è—Ç—Å—è 20 —Å–∞–º—ã—Ö —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫.

–ó–∞—Ç–µ–º –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å—Ç—Ä–æ–∏—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ (heatmap): —Å—Ç—Ä–æ–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∏—Å—Ç–∏–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º, —Å—Ç–æ–ª–±—Ü—ã ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º, —è—á–µ–π–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —á–∏—Å–ª–æ –æ—à–∏–±–æ–∫. –î–∞–∂–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è –Ω—É–ª—è–º–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Å–±–æ–µ–≤ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏. –ö–∞—Ä—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –≤–∏–¥–µ PNG-—Ñ–∞–π–ª–æ–≤ final_error_heatmap_<model_name>.png.

–ò—Ç–æ–≥ –±–ª–æ–∫–∞ ‚Äî –Ω–∞–±–æ—Ä —Ç–∞–±–ª–∏—Ü –∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ —É–ª—É—á—à–∏—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
"""

from pathlib import Path
ZIP_PATH = Path("/content/drive/MyDrive/avatar_recog/300img_test/Test.zip")
print("–°—É—â–µ—Å—Ç–≤—É–µ—Ç:", ZIP_PATH.exists())
print("–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞:", ZIP_PATH.stat().st_size / 1024, "KB")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 9B
#  Blind unified evaluation (300 mixed images through 9 models)
# ================================================================

import zipfile, shutil, os, random
from pathlib import Path
import torch, timm
from torchvision import transforms
from PIL import Image
import pandas as pd
import numpy as np
from tqdm import tqdm

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "vk_blind_eval"
OUT_DIR.mkdir(parents=True, exist_ok=True)

ZIP_PATH = "/content/drive/MyDrive/avatar_recog/300img_test/Test.zip"

# ---------- 1. –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ ----------
if TEST_ROOT.exists():
    shutil.rmtree(TEST_ROOT)
with zipfile.ZipFile(ZIP_PATH, 'r') as zf:
    zf.extractall(TEST_ROOT)

# ---------- 2. –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º ----------
folders = {
    "AI_test": "generated",
    "drawn_test": "drawing",
    "real_test": "real"
}

all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})

random.shuffle(all_images)
print(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}")

# ---------- 3. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- 4. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- 5. –§—É–Ω–∫—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ ----------
def predict_batch(model, imgs):
    batch = torch.stack([tfm(Image.open(p).convert("RGB")) for p in imgs]).to(DEVICE)
    with torch.no_grad():
        logits = model(batch)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
    return [CLASSES[p] for p in preds]

# ---------- 6. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
results = []
paths = [x["path"] for x in all_images]
true_labels = [x["true"] for x in all_images]
df = pd.DataFrame({"image": [p.name for p in paths], "true_label": true_labels})

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤ {weight}")
        continue

    print(f"\n –ü—Ä–æ–≥–æ–Ω—è–µ–º {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    preds_all = []
    for i in tqdm(range(0, len(paths), 32)):
        batch_paths = paths[i:i+32]
        preds_all.extend(predict_batch(model, batch_paths))

    df[alias] = preds_all

# ---------- 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
out_csv = OUT_DIR / "vk_blind_eval_results.csv"
df.to_csv(out_csv, index=False)
print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {out_csv}")

# ---------- 8.  —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–∞–∂–¥–æ–º—É alias ----------
for col in df.columns[2:]:
    acc = np.mean(df["true_label"] == df[col])
    print(f"{col:30s} | Accuracy: {acc:.3f}")

print("\n‚úÖ Block 9B completed successfully.")

"""–ë–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç ‚Äú—Å–ª–µ–ø—É—é‚Äù –ø—Ä–æ–≤–µ—Ä–∫—É –≤—Å–µ—Ö –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –µ–¥–∏–Ω–æ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –∏–∑ 1340 —Ä–∞–Ω–µ–µ –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

–°–Ω–∞—á–∞–ª–∞ –∑–∞–¥–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ —Ä–∞–±–æ—á–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –∞—Ä—Ö–∏–≤—É –∏ –∫–∞—Ç–∞–ª–æ–≥–∞–º —Å –≤–µ—Å–∞–º–∏ –º–æ–¥–µ–ª–µ–π. –ï—Å–ª–∏ —Ç–µ—Å—Ç–æ–≤–∞—è –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–Ω–∞ —É–¥–∞–ª—è–µ—Ç—Å—è, –∑–∞—Ç–µ–º –∞—Ä—Ö–∏–≤ Test.zip —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç—Å—è –∑–∞–Ω–æ–≤–æ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É 300img_test/Test.

–î–∞–ª–µ–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏–∑ —Ç—Ä—ë—Ö –ø–∞–ø–æ–∫ (AI_test, drawn_test, real_test) —Å –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤ (generated, drawing, real). –ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º.

–î–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∑–∞–¥–∞—é—Ç—Å—è –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ‚Äî —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤. –°–æ–∑–¥–∞—ë—Ç—Å—è –µ–¥–∏–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è predict_batch, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π, –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (Resize, ToTensor, Normalize), –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –±–∞—Ç—á –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã.

–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –≤–∫–ª—é—á–∞–µ—Ç –≤—Å–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: MobileNetV3, ResNet50, EfficientNet-B0, ConvNeXt-Tiny (–≤ –¥–≤—É—Ö —Å—Ç–∞–¥–∏—è—Ö) –∏ few-shot –≤–µ—Ä—Å–∏–∏ MobileNetV3 –∏ ResNet18.
–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏:

—Å–æ–∑–¥–∞—ë—Ç—Å—è —ç–∫–∑–µ–º–ø–ª—è—Ä timm-–º–æ–¥–µ–ª–∏ —Å –Ω—É–∂–Ω—ã–º —á–∏—Å–ª–æ–º –≤—ã—Ö–æ–¥–æ–≤,

–∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–µ—Å–∞,

–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –ø–æ 32 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞ –∏—Ç–µ—Ä–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é predict_batch.

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü—É DataFrame: –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —É–∫–∞–∑–∞–Ω—ã –∏–º—è —Ñ–∞–π–ª–∞, –∏—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ (–ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å—Ç–æ–ª–±—Ü–∞–º). –¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ vk_blind_eval_results.csv.

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ö–æ–¥–∞ –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ (–º–æ–¥–µ–ª–∏) –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å (accuracy) –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –∏ –≤—ã–≤–æ–¥–∏—Ç—Å—è –≤ –∫–æ–Ω—Å–æ–ª—å.

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –±–ª–æ–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ, –ø–æ–ª—É—á–∏–≤ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –º–µ—Ç—Ä–∏–∫ –±–µ–∑ —É—á–∞—Å—Ç–∏—è —ç—Ç–∞–ø–æ–≤ –æ–±—É—á–µ–Ω–∏—è.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 9C
#  Visual comparison of model accuracy and confusion
# ================================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np
from pathlib import Path

# ---------- 0. –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
CSV_PATH = DRIVE_ROOT / "outputs" / "vk_blind_eval" / "vk_blind_eval_results.csv"
OUT_DIR = DRIVE_ROOT / "outputs" / "vk_blind_eval"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
df = pd.read_csv(CSV_PATH)
model_cols = df.columns[2:]
print("–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π:", len(model_cols))

# ---------- 2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ ----------
acc_data = []
for m in model_cols:
    acc = np.mean(df["true_label"] == df[m])
    acc_data.append({"Model": m, "Accuracy": acc})
acc_df = pd.DataFrame(acc_data).sort_values("Accuracy", ascending=False)

# ---------- 3. –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ ----------
plt.figure(figsize=(10,5))
sns.barplot(acc_df, x="Model", y="Accuracy", palette="Blues_d")
plt.title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ (1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
plt.xticks(rotation=30, ha="right")
plt.ylim(0,1)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig(OUT_DIR / "vk_blind_accuracy_bar.png", dpi=300)
plt.show()

# ---------- 4. Confusion Matrix –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ ----------
best_model = acc_df.iloc[0]["Model"]
print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}")

classes = sorted(df["true_label"].unique())
cm = confusion_matrix(df["true_label"], df[best_model], labels=classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot(cmap="Blues", values_format="d")
plt.title(f"Confusion Matrix ‚Äî {best_model}")
plt.savefig(OUT_DIR / f"vk_confusion_{best_model.replace(' ','_')}.png", dpi=300)
plt.show()

print("\n‚úÖ Block 9C completed successfully.")

"""–ë–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ‚Äú—Å–ª–µ–ø–æ–≥–æ‚Äù —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ 1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –æ–±—ä–µ–¥–∏–Ω—è—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏.

–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏—Ç–æ–≥–æ–≤—ã–π CSV-—Ñ–∞–π–ª vk_blind_eval_results.csv, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏.

–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å (accuracy) –∫–∞–∫ –¥–æ–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü—É acc_df, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å –ª–∏–¥–∏—Ä—É—é—â—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.

–î–∞–ª–µ–µ —Å—Ç—Ä–æ–∏—Ç—Å—è —Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞, –æ—Ç–æ–±—Ä–∞–∂–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏. –¶–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ ‚ÄúBlues_d‚Äù –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –æ—Å—å X —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –∞ –æ—Å—å Y –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0‚Äì1. –î–∏–∞–≥—Ä–∞–º–º–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ vk_blind_accuracy_bar.png.

–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å —Å –Ω–∞–∏–ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –î–ª—è –Ω–µ—ë –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (confusion matrix), –≥–¥–µ –ø–æ –æ—Å–∏ X —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã, –∞ –ø–æ –æ—Å–∏ Y ‚Äî –∏—Å—Ç–∏–Ω–Ω—ã–µ.
–§—É–Ω–∫—Ü–∏—è ConfusionMatrixDisplay –∏–∑ sklearn —Å—Ç—Ä–æ–∏—Ç —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ —è—á–µ–π–∫–∞—Ö. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ —Ñ–∞–π–ª vk_confusion_<model>.png.

–í –∏—Ç–æ–≥–µ –±–ª–æ–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –æ—Ü–µ–Ω–∏—Ç—å:

–∫–∞–∫—É—é –º–æ–¥–µ–ª—å —Å—Ç–æ–∏—Ç —Å—á–∏—Ç–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ–π –Ω–∞ –Ω–æ–≤–æ–π —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ;

–∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –∫–ª–∞—Å—Å—ã —á–∞—â–µ –≤—Å–µ–≥–æ –ø—É—Ç–∞—é—Ç—Å—è –¥–∞–∂–µ —É –ª—É—á—à–µ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 9D
#  Re-evaluation on the same 1340 images and comparison with previous run
# ================================================================

import torch, timm, random
from pathlib import Path
from torchvision import transforms
from PIL import Image
import pandas as pd
import numpy as np
from tqdm import tqdm

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "vk_blind_eval"
OUT_DIR.mkdir(parents=True, exist_ok=True)

OLD_CSV = OUT_DIR / "vk_blind_eval_results.csv"
NEW_CSV = OUT_DIR / "vk_blind_eval_results_rerun.csv"

# ---------- 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ----------
old_df = pd.read_csv(OLD_CSV)
print("–°—Ç–∞—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞–π–¥–µ–Ω:", OLD_CSV.exists())

# ---------- 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ ----------
paths = [Path("/content/avatar_recog/data/300img_test/Test/AI_test"),
         Path("/content/avatar_recog/data/300img_test/Test/drawn_test"),
         Path("/content/avatar_recog/data/300img_test/Test/real_test")]

folders = {
    "AI_test": "generated",
    "drawn_test": "drawing",
    "real_test": "real"
}

all_images = []
for p in paths:
    folder_name = p.name
    label = folders.get(folder_name, "unknown")
    for ext in ("*.jpg","*.jpeg","*.png"):
        for f in p.glob(ext):
            all_images.append({"path": f, "true": label})
random.shuffle(all_images)
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

# ---------- 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

def predict_batch(model, imgs):
    batch = torch.stack([tfm(Image.open(p).convert("RGB")) for p in imgs]).to(DEVICE)
    with torch.no_grad():
        logits = model(batch)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
    return [CLASSES[p] for p in preds]

# ---------- 4. –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –ø—Ä–æ–≥–æ–Ω ----------
df_new = pd.DataFrame({
    "image": [p["path"].name for p in all_images],
    "true_label": [p["true"] for p in all_images]
})

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤ {weight}")
        continue

    print(f"\n –ü—Ä–æ–≥–æ–Ω—è–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    preds_all = []
    for i in tqdm(range(0, len(all_images), 32)):
        batch_paths = [x["path"] for x in all_images[i:i+32]]
        preds_all.extend(predict_batch(model, batch_paths))

    df_new[alias] = preds_all

# ---------- 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º ----------
df_new.to_csv(NEW_CSV, index=False)
print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {NEW_CSV}")

# ---------- 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ 1-–≥–æ –∏ 2-–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ----------
print("\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–µ–π (—Å—Ç–∞—Ä—ã–π ‚Üí –Ω–æ–≤—ã–π):")
for col in df_new.columns[2:]:
    if col in old_df.columns:
        acc_old = np.mean(old_df["true_label"] == old_df[col])
        acc_new = np.mean(df_new["true_label"] == df_new[col])
        delta = acc_new - acc_old
        print(f"{col:30s} | {acc_old:.3f} ‚Üí {acc_new:.3f}  (Œî={delta:+.3f})")

print("\n‚úÖ Block 9D completed successfully.")

"""Block 9D ‚Äî –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–≠—Ç–æ—Ç –±–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–π (‚Äúre-run‚Äù) –ø—Ä–æ–≥–æ–Ω –≤—Å–µ—Ö –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Ö –∂–µ 1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π. –û–Ω —Ç–∞–∫–∂–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∑–∞–ø—É—Å–∫–æ–º –∏–∑ –±–ª–æ–∫–∞ 9B.

–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (vk_blind_eval_results.csv) –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç—Ä—ë—Ö –ø–∞–ø–æ–∫ (AI_test, drawn_test, real_test). –ö–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≤—è–∑—ã–≤–∞–µ—Ç—Å—è —Å –∏—Å—Ç–∏–Ω–Ω—ã–º –∫–ª–∞—Å—Å–æ–º –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ —Å–ø–∏—Å–æ–∫ all_images.

–î–∞–ª–µ–µ –∑–∞–¥–∞—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ ‚Äî —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU), —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è predict_batch, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã, –ø—Ä–æ–≥–æ–Ω—è–µ—Ç –∏—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.

–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π (models_to_eval) —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –±–ª–æ–∫–æ–º: MobileNetV3, ResNet50, EfficientNet-B0, ConvNeXt-Tiny (–≤ –¥–≤—É—Ö —Å—Ç–∞–¥–∏—è—Ö), –∏ few-shot –≤–µ—Ä—Å–∏–∏ MobileNetV3 –∏ ResNet18.
–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏:

–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è timm-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–µ—Å–∞;

–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –ø–æ –±–∞—Ç—á–∞–º (–ø–æ 32 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è);

—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü—É df_new.

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ–≥–æ–Ω–∞ –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ CSV-—Ñ–∞–π–ª vk_blind_eval_results_rerun.csv.

–ù–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º —ç—Ç–∞–ø–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ—Å—Ç—Ä–æ—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ä—ã—Ö –∏ –Ω–æ–≤—ã—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ä–∞–∑–Ω–∏—Ü–∞ (Œî) –º–µ–∂–¥—É –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∏ —Ç–µ–∫—É—â–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º accuracy, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –≤—ã–∑–≤–∞–Ω–Ω—ã–µ —Å—Ä–µ–¥–æ–≤—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π.

–ë–ª–æ–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –º–æ–¥—É–ª—å –ø—Ä–æ–≤–µ—Ä–∫–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–æ–Ω—Ç—Ä–æ–ª—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –µ–¥–∏–Ω–æ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 10
#  Robustness test: noise, blur, brightness, rotation, JPEG compression
# ================================================================

import torch, timm, random, cv2, os
from pathlib import Path
from torchvision import transforms
from PIL import Image
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "robustness_test"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
folders = {
    "AI_test": "generated",
    "drawn_test": "drawing",
    "real_test": "real"
}

all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

# ---------- 2. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm_base = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- 3. –§—É–Ω–∫—Ü–∏–∏ –∏—Å–∫–∞–∂–µ–Ω–∏–π ----------
def apply_noise(img, intensity=15):
    arr = np.array(img).astype(np.float32)
    noise = np.random.normal(0, intensity, arr.shape)
    arr = np.clip(arr + noise, 0, 255).astype(np.uint8)
    return Image.fromarray(arr)

def apply_blur(img, k=5):
    return Image.fromarray(cv2.GaussianBlur(np.array(img), (k, k), 0))

def apply_brightness(img, factor=1.5):
    hsv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2HSV)
    hsv = hsv.astype(np.float32)
    hsv[...,2] = np.clip(hsv[...,2]*factor, 0, 255)
    hsv = hsv.astype(np.uint8)
    return Image.fromarray(cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB))

def apply_rotation(img, angle=15):
    return img.rotate(angle)

def apply_jpeg(img, quality=30):
    tmp = "/tmp/tmp_q.jpg"
    img.save(tmp, quality=quality)
    return Image.open(tmp).convert("RGB")

transformations = {
    "noise": lambda img: apply_noise(img, 15),
    "blur": lambda img: apply_blur(img, 5),
    "brightness": lambda img: apply_brightness(img, 1.5),
    "rotation": lambda img: apply_rotation(img, 15),
    "jpeg": lambda img: apply_jpeg(img, 30)
}

# ---------- 4. –ú–æ–¥–µ–ª–∏ ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- 5. –û—Ü–µ–Ω–∫–∞ ----------
def predict(model, img):
    timg = tfm_base(img).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        pred = torch.argmax(model(timg), dim=1).item()
    return CLASSES[pred]

results = []
for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤ {weight}")
        continue

    print(f"\n–ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    # –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π
    preds = [predict(model, Image.open(x['path']).convert('RGB')) for x in tqdm(all_images, desc=f"{alias} clean")]
    acc_clean = accuracy_score([x['true'] for x in all_images], preds)
    row = {"Model": alias, "clean": acc_clean}

    # –° –∏—Å–∫–∞–∂–µ–Ω–∏—è–º–∏
    for tname, tfunc in transformations.items():
        preds_t = []
        for x in tqdm(all_images, desc=f"{alias} {tname}"):
            img = Image.open(x['path']).convert('RGB')
            img_t = tfunc(img)
            preds_t.append(predict(model, img_t))
        acc_t = accuracy_score([x['true'] for x in all_images], preds_t)
        row[tname] = acc_t

    results.append(row)

# ---------- 6. –ê–Ω–∞–ª–∏–∑ ----------
df = pd.DataFrame(results)
df.to_csv(OUT_DIR / "robustness_results.csv", index=False)

# –í—ã—á–∏—Å–ª–∏–º –ø–∞–¥–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —á–∏—Å—Ç—ã—Ö
for col in transformations.keys():
    df[f"{col}_drop"] = df["clean"] - df[col]

# ---------- 7. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
plt.figure(figsize=(10,6))
melted = df.melt(id_vars="Model", value_vars=["noise","blur","brightness","rotation","jpeg"],
                 var_name="Distortion", value_name="Accuracy")
sns.barplot(melted, x="Model", y="Accuracy", hue="Distortion")
plt.title("–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º")
plt.xticks(rotation=30, ha="right")
plt.tight_layout()
plt.savefig(OUT_DIR / "robustness_bar.png", dpi=300)
plt.show()

print("\n‚úÖ Block 10 completed successfully.")

"""Block 10 ‚Äî —Ç–µ—Å—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π (Robustness Test)

–≠—Ç–æ—Ç –±–ª–æ–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —É—Å—Ç–æ–π—á–∏–≤—ã –≤—Å–µ –¥–µ–≤—è—Ç—å –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ç–∏–ø–∞–º –∏—Å–∫–∞–∂–µ–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏–º–∏—Ç–∏—Ä—É—é—â–∏—Ö —Ä–µ–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è: —à—É–º, —Ä–∞–∑–º—ã—Ç–∏–µ, –∏–∑–º–µ–Ω–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏, –ø–æ–≤–æ—Ä–æ—Ç –∏ JPEG-–∫–æ–º–ø—Ä–µ—Å—Å–∏—é.

–í –Ω–∞—á–∞–ª–µ –∑–∞–¥–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º, –∫–∞—Ç–∞–ª–æ–≥–∞–º –º–æ–¥–µ–ª–µ–π –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ò–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ 1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –∏ –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (real, drawing, generated).

–û–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU), —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (224√ó224), –±–∞–∑–æ–≤—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (resize, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä).

–î–∞–ª–µ–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏, —Å–æ–∑–¥–∞—é—â–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∏–¥—ã –∏—Å–∫–∞–∂–µ–Ω–∏–π:

apply_noise ‚Äî –¥–æ–±–∞–≤–ª—è–µ—Ç –≥–∞—É—Å—Å–æ–≤ —à—É–º,

apply_blur ‚Äî –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–º—ã—Ç–∏–µ –ì–∞—É—Å—Å–∞,

apply_brightness ‚Äî –∏–∑–º–µ–Ω—è–µ—Ç —è—Ä–∫–æ—Å—Ç—å –≤ HSV-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ,

apply_rotation ‚Äî –ø–æ–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ,

apply_jpeg ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ JPEG —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º, –∏–º–∏—Ç–∏—Ä—É—è —Å–∂–∞—Ç–∏–µ.

–í—Å–µ —ç—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –≤ —Å–ª–æ–≤–∞—Ä—å transformations, —á—Ç–æ–±—ã –ª–µ–≥–∫–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –Ω–∏–º –≤ —Ü–∏–∫–ª–µ.

–ó–∞—Ç–µ–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –ø—Ä–µ–¥—ã–¥—É—â–∏–º –±–ª–æ–∫–∞–º (MobileNetV3, ResNet50, EfficientNet-B0, ConvNeXt-Tiny, few-shot –≤–∞—Ä–∏–∞–Ω—Ç—ã).
–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏:

–ó–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤–µ—Å–∞ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞.

–°–Ω–∞—á–∞–ª–∞ –∏–∑–º–µ—Ä—è–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (accuracy) –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π.

–î–∞–ª–µ–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø—Ä–æ–≥–æ–Ω—ã –ø–æ –≤—Å–µ–º –≤–∏–¥–∞–º –∏—Å–∫–∞–∂–µ–Ω–∏–π; –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –Ω–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å.

–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—á–∏—Å—Ç—ã–µ –∏ –∏—Å–∫–∞–∂—ë–Ω–Ω—ã–µ) —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü—É –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ robustness_results.csv.
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–∞–¥–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ (drop) –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π ‚Äî —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –Ω–∞ —á–∏—Å—Ç—ã—Ö –∏ –∏—Å–∫–∞–∂—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–í –∫–æ–Ω—Ü–µ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è, –∫–∞–∫ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è –≤–ª–∏—è—é—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏. –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ robustness_bar.png.

–ò—Ç–æ–≥ –±–ª–æ–∫–∞ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –∏ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ —Ä–µ–∞–ª—å–Ω—ã–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å, –∫–∞–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–∞–∏–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ —à—É–º–µ, –ø–æ–≤–æ—Ä–æ—Ç–∞—Ö –∏–ª–∏ —Å–∂–∞—Ç–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 10B
#  Robustness summary: best models by distortion type (sorted)
# ================================================================

import pandas as pd
from pathlib import Path


DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
ROBUST_CSV = DRIVE_ROOT / "outputs" / "robustness_test" / "robustness_results.csv"

# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ ----------
df = pd.read_csv(ROBUST_CSV)
distortions = ["noise", "blur", "brightness", "rotation", "jpeg"]

# ---------- –¢–∞–±–ª–∏—Ü–∞ –ø–æ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º ----------
summary = []
for d in distortions:
    best_row = df.loc[df[d].idxmax()]
    summary.append({
        "Distortion": d,
        "Best_Model": best_row["Model"],
        "Accuracy": round(best_row[d], 3),
        "Drop_vs_Clean": round(best_row["clean"] - best_row[d], 3)
    })

# —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (–ø–æ —Å–ø–∏—Å–∫—É distortions)
summary_df = pd.DataFrame(summary).set_index("Distortion").loc[distortions].reset_index()

print("–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–ø—É –∏—Å–∫–∞–∂–µ–Ω–∏–π:")
display(summary_df)

# ---------- –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ —Å—Ä–µ–¥–Ω–µ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ ----------
df["mean_distortion_acc"] = df[distortions].mean(axis=1)
ranking = df[["Model","clean","mean_distortion_acc"]].sort_values(
    by="mean_distortion_acc", ascending=False
).reset_index(drop=True)

print("\n–°—Ä–µ–¥–Ω—è—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π (–æ—Ç –ª—É—á—à–µ–π –∫ —Ö—É–¥—à–µ–π):")
display(ranking)

print("\n‚úÖ Block 10B completed successfully.")

"""Block 10B ‚Äî —Å–≤–æ–¥–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º

–≠—Ç–æ—Ç –±–ª–æ–∫ –æ–±–æ–±—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ (–∏–∑ Block 10) –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–ª—É—á—à—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Ç–∏–ø–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ robustness_results.csv, –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏ –ø—è—Ç–∏ —Ç–∏–ø–∞—Ö –∏—Å–∫–∞–∂–µ–Ω–∏–π:
noise, blur, brightness, rotation, jpeg.

–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å–∫–∞–∂–µ–Ω–∏—è –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é (idxmax()), –ø–æ—Å–ª–µ —á–µ–≥–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ summary_df, –≥–¥–µ —É–∫–∞–∑–∞–Ω–æ:

—Ç–∏–ø –∏—Å–∫–∞–∂–µ–Ω–∏—è (Distortion),

–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (Best_Model),

–¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy),

–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —á–∏—Å—Ç—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (Drop_vs_Clean).

–≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞–∏–±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –∫–∞–∂–¥–æ–º—É –≤–∏–¥—É –∏—Å–∫–∞–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResNet50 –ø—Ä–∏ —à—É–º–µ, MobileNetV3 –ø—Ä–∏ JPEG –∏ —Ç.–¥.).

–î–∞–ª–µ–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω—è—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å: –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –≤—Å–µ–º –≤–∏–¥–∞–º –∏—Å–∫–∞–∂–µ–Ω–∏–π (mean_distortion_acc).
–ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ (ranking) —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ä–µ–¥–Ω–µ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–±—â—É—é ‚Äú–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å‚Äù –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.

–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±–ª–æ–∫ –≤—ã–≤–æ–¥–∏—Ç –¥–≤–µ –∏—Ç–æ–≥–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã:

–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å–∫–∞–∂–µ–Ω–∏—è.

–û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π –ø–æ —Å—Ä–µ–¥–Ω–µ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏.

–≠—Ç–æ—Ç –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ç–µ—Å—Ç –Ω–∞ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å, –ø–æ–∑–≤–æ–ª—è—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 11
#  Softmax Confidence Analysis (confidence histograms & calibration)
# ================================================================

import torch, timm, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from pathlib import Path
from torchvision import transforms
from PIL import Image
from sklearn.metrics import accuracy_score
import pandas as pd

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "confidence_analysis"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- 2. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- 3. –§—É–Ω–∫—Ü–∏—è Softmax-–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ ----------
def predict_with_confidence(model, imgs):
    probs_all, preds_all = [], []
    for p in imgs:
        img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
        preds_all.append(np.argmax(probs))
        probs_all.append(np.max(probs))
    return preds_all, probs_all

# ---------- 4. –ê–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–π (–∏–ª–∏ –ª—É—á—à–∏—Ö) –º–æ–¥–µ–ª–µ–π ----------
# –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –≤–æ–∑—å–º—ë–º –ª—É—á—à—É—é ‚Äî ResNet18 FewShot12ep
target_model = ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep")

model = timm.create_model(target_model[0], pretrained=False, num_classes=NUM_CLASSES)
model.load_state_dict(torch.load(MODELS_DIR / target_model[1], map_location=DEVICE))
model = model.to(DEVICE)
model.eval()

paths = [x["path"] for x in all_images]
labels_true = [CLASSES.index(x["true"]) for x in all_images]
preds, confs = predict_with_confidence(model, paths)
acc = accuracy_score(labels_true, preds)
print(f"{target_model[2]} ‚Äî Accuracy: {acc:.3f}")

# ---------- 5. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ----------
plt.figure(figsize=(7,4))
sns.histplot(confs, bins=20, kde=False, color="skyblue")
plt.title(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ‚Äî {target_model[2]}")
plt.xlabel("Softmax confidence")
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
plt.tight_layout()
plt.savefig(OUT_DIR / f"{target_model[2].replace(' ','_')}_confidence_hist.png", dpi=300)
plt.show()

# ---------- 6. Reliability Diagram ----------
def reliability_diagram(y_true, y_pred, confs, bins=10):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    confs  = np.array(confs)

    bins_edges = np.linspace(0.0, 1.0, bins+1)
    bin_ids = np.digitize(confs, bins_edges, right=False)
    bin_ids = np.clip(bin_ids, 1, bins)

    accs, conf_means, counts = [], [], []
    for i in range(1, bins+1):
        mask = bin_ids == i
        if np.any(mask):
            acc_bin = np.mean(y_true[mask] == y_pred[mask])
            conf_bin = np.mean(confs[mask])
            accs.append(acc_bin)
            conf_means.append(conf_bin)
            counts.append(np.sum(mask))

    if np.sum(counts) > 0:
        ece = np.average(np.abs(np.array(accs) - np.array(conf_means)), weights=counts)
    else:
        ece = np.nan

    return conf_means, accs, counts, ece


conf_means, accs, counts, ece = reliability_diagram(labels_true, preds, confs, bins=10)

plt.figure(figsize=(5,5))
plt.plot([0,1],[0,1],"--",color="gray")
plt.plot(conf_means, accs, marker="o", color="blue")
plt.xlabel("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å")
plt.ylabel("–î–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
plt.title(f"Reliability Diagram ‚Äî {target_model[2]}")
plt.tight_layout()
plt.savefig(OUT_DIR / f"{target_model[2].replace(' ','_')}_reliability.png", dpi=300)
plt.show()

print(f"ECE (Expected Calibration Error): {ece:.3f}")

print("\n‚úÖ Block 11 completed successfully.")

"""Block 11 ‚Äî –∞–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (Softmax Confidence Analysis)

–≠—Ç–æ—Ç –±–ª–æ–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —É–≤–µ—Ä–µ–Ω—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö, –∏ –Ω–∞—Å–∫–æ–ª—å–∫–æ –∏—Ö —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ (–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏).

–°–Ω–∞—á–∞–ª–∞ —É–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º, –∫–∞—Ç–∞–ª–æ–≥–∞–º —Å –≤–µ—Å–∞–º–∏ –∏ –≤—ã—Ö–æ–¥–Ω—ã–º –ø–∞–ø–∫–∞–º. –ò–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã—Ö –ø–æ –ø–∞–ø–∫–∞–º AI_test, drawn_test, real_test) —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫.

–û–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU/CPU), —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (224√ó224), –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (real, drawing, generated) –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.

–î–∞–ª–µ–µ –∑–∞–¥–∞—ë—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è predict_with_confidence, –∫–æ—Ç–æ—Ä–∞—è:

–ü—Ä–æ–≥–æ–Ω—è–µ—Ç –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å,

–ü—Ä–∏–º–µ–Ω—è–µ—Ç Softmax –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –ª–æ–≥–∏—Ç–∞–º,

–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å).

–í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ ‚Äî ResNet18 FewShot12ep (–ª—É—á—à–∞—è –ø–æ F1 –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–ª–æ–∫–æ–≤).
–ü–æ—Å–ª–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –æ–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (accuracy_score), –∞ —Ç–∞–∫–∂–µ —Å—Ç—Ä–æ–∏—Ç—Å—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (sns.histplot), –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è, –Ω–∞—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–æ –º–æ–¥–µ–ª—å –≤—ã–¥–∞—ë—Ç –≤—ã—Å–æ–∫–∏–µ –∏–ª–∏ –Ω–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è Softmax.

–ó–∞—Ç–µ–º —Å–æ–∑–¥–∞—ë—Ç—Å—è Reliability Diagram ‚Äî –≥—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –æ—Ç —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è reliability_diagram, –∫–æ—Ç–æ—Ä–∞—è:

–¥–µ–ª–∏—Ç –¥–∏–∞–ø–∞–∑–æ–Ω —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ [0,1] –Ω–∞ —Ä–∞–≤–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã (bins),

–≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞,

—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç ECE (Expected Calibration Error) ‚Äî —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.

–ï—Å–ª–∏ ECE ‚âà 0, –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–∞ (–µ—ë —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏). –ï—Å–ª–∏ ECE –≤–µ–ª–∏–∫, –∑–Ω–∞—á–∏—Ç –º–æ–¥–µ–ª—å –ª–∏–±–æ –ø–µ—Ä–µ—É–≤–µ—Ä–µ–Ω–∞, –ª–∏–±–æ –Ω–µ–¥–æ—É–≤–µ—Ä–µ–Ω–∞ –≤ —Å–≤–æ–∏—Ö –æ—Ç–≤–µ—Ç–∞—Ö.

–í –∫–æ–Ω—Ü–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –¥–≤–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ ‚Äî –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –¥–∏–∞–≥—Ä–∞–º–º–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ ‚Äî –∏ –≤—ã–≤–æ–¥–∏—Ç—Å—è —á–∏—Å–ª–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ECE, —Å–ª—É–∂–∞—â–µ–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–º –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏.
"""

import zipfile, shutil
from pathlib import Path

# –ü—É—Ç–∏
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR = WORK_ROOT / "data"
ZIP_PATH = Path("/content/drive/MyDrive/avatar_recog/300img_test/Test.zip")
TEST_ROOT = DATA_DIR / "300img_test"

# –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ (–µ—Å–ª–∏ –µ—Å—Ç—å)
if TEST_ROOT.exists():
    shutil.rmtree(TEST_ROOT)

# –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–∞
with zipfile.ZipFile(ZIP_PATH, 'r') as zf:
    zf.extractall(TEST_ROOT)

print("‚úÖ –ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω –≤:", TEST_ROOT)
!ls -R $TEST_ROOT

# ================================================================
#  Avatar Type Recognition ‚Äî Block 11B
#  Softmax Confidence Analysis for all 9 models
# ================================================================

import torch, timm, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from pathlib import Path
from torchvision import transforms
from PIL import Image
from sklearn.metrics import accuracy_score
import pandas as pd
from tqdm import tqdm

# ---------- 0. –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
TEST_ROOT = WORK_ROOT / "data" / "300img_test" / "Test"

MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "confidence_analysis_all"
OUT_DIR.mkdir(parents=True, exist_ok=True)


# ---------- 1. –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / folder

    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_CLASSES = 3
CLASSES = ["real", "drawing", "generated"]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- 2. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------
def reliability_diagram(y_true, y_pred, confs, bins=10):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    confs  = np.array(confs)

    bins_edges = np.linspace(0.0, 1.0, bins+1)
    bin_ids = np.digitize(confs, bins_edges, right=False)
    bin_ids = np.clip(bin_ids, 1, bins)

    accs, conf_means, counts = [], [], []
    for i in range(1, bins+1):
        mask = bin_ids == i
        if np.any(mask):
            acc_bin = np.mean(y_true[mask] == y_pred[mask])
            conf_bin = np.mean(confs[mask])
            accs.append(acc_bin)
            conf_means.append(conf_bin)
            counts.append(np.sum(mask))

    if np.sum(counts) > 0:
        ece = np.average(np.abs(np.array(accs) - np.array(conf_means)), weights=counts)
    else:
        ece = np.nan

    return conf_means, accs, counts, ece


# ---------- 4. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
paths = [x["path"] for x in all_images]
labels_true = [CLASSES.index(x["true"]) for x in all_images]

summary = []

# ---------- 3.1. –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é ----------
def predict_with_confidence(model, imgs):
    preds_all, confs_all = [], []
    for p in tqdm(imgs, desc="Predicting"):
        img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
        preds_all.append(np.argmax(probs))
        confs_all.append(np.max(probs))
    return preds_all, confs_all


for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias} ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ {weight}")
        continue

    print(f"\n–ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    preds, confs = predict_with_confidence(model, paths)
    acc = accuracy_score(labels_true, preds)
    mean_conf = float(np.mean(confs))
    conf_means, accs, counts, ece = reliability_diagram(labels_true, preds, confs, bins=10)

    summary.append({
        "Model": alias,
        "Accuracy": round(acc, 3),
        "Mean_Confidence": round(mean_conf, 3),
        "ECE": round(ece, 3)
    })

    # --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ ---
    out_subdir = OUT_DIR / alias.replace(" ", "_")
    out_subdir.mkdir(parents=True, exist_ok=True)

    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    plt.figure(figsize=(7,4))
    sns.histplot(confs, bins=20, kde=False, color="skyblue")
    plt.title(f"Confidence Distribution ‚Äî {alias}")
    plt.xlabel("Softmax confidence")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig(out_subdir / "confidence_hist.png", dpi=300)
    plt.close()

    # Reliability diagram
    plt.figure(figsize=(5,5))
    plt.plot([0,1],[0,1],"--",color="gray")
    plt.plot(conf_means, accs, marker="o", color="blue")
    plt.xlabel("Average confidence")
    plt.ylabel("Accuracy per bin")
    plt.title(f"Reliability Diagram ‚Äî {alias}\nECE={ece:.3f}")
    plt.tight_layout()
    plt.savefig(out_subdir / "reliability.png", dpi=300)
    plt.close()

# ---------- 5. –¢–∞–±–ª–∏—Ü–∞ –∏ —Ä–µ–π—Ç–∏–Ω–≥ ----------
summary_df = pd.DataFrame(summary)
summary_df = summary_df.sort_values("ECE", ascending=True).reset_index(drop=True)
summary_path = OUT_DIR / "confidence_summary.csv"
summary_df.to_csv(summary_path, index=False)
print("\n–û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (—á–µ–º –Ω–∏–∂–µ ECE, —Ç–µ–º –ª—É—á—à–µ):")
display(summary_df)

print(f"\n–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")
print("\n‚úÖ Block 11B completed successfully.")

"""Block 11B ‚Äî –∞–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (Softmax Confidence) –¥–ª—è –≤—Å–µ—Ö 9 –º–æ–¥–µ–ª–µ–π

–≠—Ç–æ—Ç –±–ª–æ–∫ —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Block 11, –≤—ã–ø–æ–ª–Ω—è—è –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ Softmax-—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–µ–∫—Ç–∞.
–û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å, –∫–∞–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤—ã–¥–∞—é—Ç –Ω–∞–∏–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –Ω–∞—Å–∫–æ–ª—å–∫–æ –∏—Ö —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 11C
#  Comparative visualization of calibration and accuracy across models
# ================================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
SUMMARY_CSV = DRIVE_ROOT / "outputs" / "confidence_analysis_all" / "confidence_summary.csv"
OUT_DIR = DRIVE_ROOT / "outputs" / "confidence_analysis_all"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ ----------
df = pd.read_csv(SUMMARY_CSV)
df = df.sort_values("ECE", ascending=True).reset_index(drop=True)

print("–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π:", len(df))
display(df)

# ---------- Accuracy vs ECE ----------
plt.figure(figsize=(9,5))
ax = sns.barplot(df, x="Model", y="Accuracy", color="skyblue", label="Accuracy")
sns.barplot(df, x="Model", y="ECE", color="salmon", alpha=0.6, label="ECE")
plt.xticks(rotation=30, ha="right")
plt.ylabel("–ó–Ω–∞—á–µ–Ω–∏—è")
plt.title("Calibration vs Accuracy across models")
plt.legend()
plt.tight_layout()
plt.savefig(OUT_DIR / "calibration_vs_accuracy.png", dpi=300)
plt.show()

# ---------- Mean Confidence ----------
plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="Mean_Confidence", palette="Blues_d")
plt.xticks(rotation=30, ha="right")
plt.title("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π")
plt.ylabel("Mean Confidence")
plt.tight_layout()
plt.savefig(OUT_DIR / "mean_confidence.png", dpi=300)
plt.show()

# ---------- –í—ã–≤–æ–¥ –ª—É—á—à–∏—Ö ----------
best_acc = df.loc[df["Accuracy"].idxmax()]
best_cal = df.loc[df["ECE"].idxmin()]

print(f"\n–õ—É—á—à–∞—è –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏: {best_acc['Model']} (Accuracy={best_acc['Accuracy']:.3f})")
print(f"–ù–∞–∏–±–æ–ª–µ–µ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–∞—è: {best_cal['Model']} (ECE={best_cal['ECE']:.3f})")

print("\n‚úÖ Block 11C completed successfully.")

"""Block 11C ‚Äî Comparative Calibration and Accuracy Visualization

–≠—Ç–æ—Ç –±–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–≤–æ–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (softmax confidence) –¥–ª—è –≤—Å–µ—Ö –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤ Block 11B. –û–Ω –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É confidence_summary.csv –∏ —Å–æ–∑–¥–∞—ë—Ç –¥–≤–∞ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–∞:

Calibration vs Accuracy across models ‚Äî –¥–≤–æ–π–Ω–∞—è —Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞, –≥–¥–µ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ (Accuracy) –∏ –æ—à–∏–±–∫–∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (ECE) –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏. –ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–≥–ª—è–¥–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Å—Ç–µ–ø–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö.

Mean Confidence ‚Äî –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ—Ç—Ä–∞–∂–∞—é—â–∞—è –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –≤ —Å—Ä–µ–¥–Ω–µ–º ¬´—É–≤–µ—Ä–µ–Ω–∞¬ª –≤ —Å–≤–æ–∏—Ö —Ä–µ—à–µ–Ω–∏—è—Ö.

–í –∫–æ–Ω—Ü–µ –±–ª–æ–∫ –≤—ã–≤–æ–¥–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π ‚Äî —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º ECE (—Ç–æ –µ—Å—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—É—é). –í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –∫–∞—Ç–∞–ª–æ–≥ /outputs/confidence_analysis_all.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 12
#  Feature Embedding Visualization (t-SNE / UMAP)
# ================================================================

import torch, timm
from torchvision import transforms
from PIL import Image
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.manifold import TSNE
# (–¥–ª—è UMAP –º–æ–∂–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å: from umap import UMAP)

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "feature_embeddings"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

# ---------- –ú–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏)

# ---------- –§—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ----------
def extract_features(model, img_paths):
    features, labels = [], []
    model.eval()
    with torch.no_grad():
        for x in tqdm(img_paths):
            img = Image.open(x["path"]).convert("RGB")
            tensor = tfm(img).unsqueeze(0).to(DEVICE)
            feat = model.forward_features(tensor)  # –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ timm –º–æ–¥–µ–ª–µ–π
            feat = torch.nn.functional.adaptive_avg_pool2d(feat, (1,1)).squeeze().cpu().numpy()
            features.append(feat)
            labels.append(x["true"])
    return np.array(features), np.array(labels)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
for model_name, weight, alias in models_to_eval:
    print(f"\n–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {alias} ...")
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤ {weight}")
        continue

    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)

    feats, labels = extract_features(model, all_images)

    # ---------- t-SNE ----------
    print("‚Üí –ó–∞–ø—É—Å–∫ t-SNE ...")
    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000, random_state=42)
    emb = tsne.fit_transform(feats)

    df_emb = pd.DataFrame({"x": emb[:,0], "y": emb[:,1], "label": labels})
    plt.figure(figsize=(7,6))
    sns.scatterplot(df_emb, x="x", y="y", hue="label", palette="Set2", s=45, alpha=0.8)
    plt.title(f"t-SNE Embedding ‚Äî {alias}")
    plt.legend(title="Class", loc="best")
    plt.tight_layout()
    plt.savefig(OUT_DIR / f"tsne_{alias.replace(' ','_')}.png", dpi=300)
    plt.show()

    # ---------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ----------
    out_npz = OUT_DIR / f"features_{alias.replace(' ','_')}.npz"
    np.savez(out_npz, features=feats, labels=labels)
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {out_npz}")

print("\n‚úÖ Block 12 completed successfully.")

"""Block 12 ‚Äî Feature Embedding Visualization (t-SNE / UMAP)

–ë–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è–º–∏ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –î–ª—è –∫–∞–∂–¥–æ–π –∏–∑ –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ forward_features. –≠—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è –∏ –∑–∞—Ç–µ–º –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç—Å—è –≤ –¥–≤—É–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–º t-SNE, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç —Ç—Ä–∏ –∫–ª–∞—Å—Å–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ‚Äî real, drawing –∏ generated. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è—é—Ç—Å—è –¥–∏–∞–≥—Ä–∞–º–º—ã —Ä–∞—Å—Å–µ—è–Ω–∏—è, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –≤–∏–¥–Ω–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —á—ë—Ç–∫–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–¥–µ–ª—è—é—Ç –∫–ª–∞—Å—Å—ã. –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ñ–∞–π–ª—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 12B
#  Cluster Structure & Separation Analysis (silhouette + distances)
# ================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import cdist

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
EMB_DIR = DRIVE_ROOT / "outputs" / "feature_embeddings"
OUT_DIR = DRIVE_ROOT / "outputs" / "feature_analysis"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]

# ---------- –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ ----------
npz_files = sorted(EMB_DIR.glob("features_*.npz"))
print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(npz_files)}")

summary = []

# ---------- –§—É–Ω–∫—Ü–∏–∏ ----------
def mean_pairwise_distance(a, b):
    """–°—Ä–µ–¥–Ω–µ–µ –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤—Å–µ–º–∏ —Ç–æ—á–∫–∞–º–∏ –¥–≤—É—Ö –º–Ω–æ–∂–µ—Å—Ç–≤"""
    dists = cdist(a, b, metric='euclidean')
    return np.mean(dists)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
for npz_path in npz_files:
    data = np.load(npz_path, allow_pickle=True)
    features = data["features"]
    labels = data["labels"]
    alias = npz_path.stem.replace("features_", "")
    print(f"\n–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {alias}")

    # –°—á–∏—Ç–∞–µ–º silhouette
    label_nums = np.array([CLASSES.index(lbl) for lbl in labels])
    sil = silhouette_score(features, label_nums)
    print(f"Silhouette Score: {sil:.3f}")

    # –ú–µ–∂- –∏ –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
    intra_dists, inter_dists = [], []
    for cls_i in CLASSES:
        group_i = features[labels == cls_i]
        intra_dists.append(np.mean(cdist(group_i, group_i)))
        for cls_j in CLASSES:
            if cls_i == cls_j:
                continue
            group_j = features[labels == cls_j]
            inter_dists.append(mean_pairwise_distance(group_i, group_j))

    intra_mean = np.mean(intra_dists)
    inter_mean = np.mean(inter_dists)
    separation = inter_mean / intra_mean

    summary.append({
        "Model": alias,
        "Silhouette": round(sil, 3),
        "IntraClassDist": round(intra_mean, 3),
        "InterClassDist": round(inter_mean, 3),
        "SeparationRatio": round(separation, 3)
    })

    # ---------- Heatmap ----------
    dist_matrix = np.zeros((3, 3))
    for i, ci in enumerate(CLASSES):
        for j, cj in enumerate(CLASSES):
            dist_matrix[i, j] = mean_pairwise_distance(
                features[labels == ci],
                features[labels == cj]
            )

    plt.figure(figsize=(5,4))
    sns.heatmap(dist_matrix, annot=True, fmt=".2f", cmap="coolwarm",
                xticklabels=CLASSES, yticklabels=CLASSES)
    plt.title(f"Feature Distance Heatmap ‚Äî {alias}")
    plt.tight_layout()
    plt.savefig(OUT_DIR / f"heatmap_{alias}.png", dpi=300)
    plt.close()

# ---------- –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
summary_df = pd.DataFrame(summary).sort_values("Silhouette", ascending=False)
summary_df.to_csv(OUT_DIR / "cluster_separation_summary.csv", index=False)

print("\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
display(summary_df)

print(f"\n‚úÖ Block 12B completed successfully. Heatmaps –∏ —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

"""Block 12B ‚Äî Cluster Structure & Separation Analysis (silhouette + distances)

–ë–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —ç—Ç–∞–ø–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –û–Ω –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –∫–ª–∞—Å—Å—ã real, drawing –∏ generated –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏.

–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ (features_*.npz) –∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è Silhouette Score, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —á—ë—Ç–∫–æ –æ–±—ä–µ–∫—Ç—ã –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç —Å–≤–æ–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º –∏ –æ—Ç–¥–µ–ª–µ–Ω—ã –æ—Ç –¥—Ä—É–≥–∏—Ö. –ß–µ–º –≤—ã—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º –ª—É—á—à–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏.

–î–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (—Å—Ä–µ–¥–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞) –∏ –º–µ–∂–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (—Å—Ä–µ–¥–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏). –û—Ç–Ω–æ—à–µ–Ω–∏–µ –º–µ–∂–¥—É –Ω–∏–º–∏ ‚Äî SeparationRatio ‚Äî —Å–ª—É–∂–∏—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–º —Å—Ç–µ–ø–µ–Ω–∏ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è mean_pairwise_distance, –≤—ã—á–∏—Å–ª—è—é—â–∞—è —Å—Ä–µ–¥–Ω–µ–µ –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –¥–≤—É–º—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞–º–∏ —Ç–æ—á–µ–∫, –∞ —Ç–∞–∫–∂–µ –º–µ—Ç–æ–¥ silhouette_score –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ scikit-learn.

–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞—ë—Ç—Å—è —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏, –æ—Ç–æ–±—Ä–∞–∂–∞—é—â–∞—è, –∫–∞–∫–∏–µ –∫–ª–∞—Å—Å—ã –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏ –∏–ª–∏ —Ä–∞–∑–ª–∏—á–∏–º—ã –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

–í –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (Silhouette, –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ –∏ –º–µ–∂–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è, –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 13 (Fixed)
#  Ensemble Voting (Soft-Voting) ‚Äî comparing single vs combined models
# ================================================================

import torch, timm, numpy as np, pandas as pd
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "ensemble_voting"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

paths = [x["path"] for x in all_images]
true_labels = [CLASSES.index(x["true"]) for x in all_images]

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è softmax-–ª–æ–≥–∏—Ç–æ–≤ ----------
def get_model_probs(model, img_paths):
    model.eval()
    probs_all = []
    with torch.no_grad():
        for p in img_paths:
            img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
            probs_all.append(probs)
    return np.array(probs_all)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ ----------
# –ï—Å–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥—Ä—É–≥–æ–π –ø–æ—Ä—è–¥–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ ImageFolder),
# –∑–∞–¥–∞–π –µ–≥–æ –∑–¥–µ—Å—å. –≠—Ç–æ—Ç –ø–æ—Ä—è–¥–æ–∫ –Ω—É–∂–Ω–æ –≤—ã—Ä–æ–≤–Ω—è—Ç—å —Å —Ç–µ–∫—É—â–∏–º CLASSES.
train_class_order = ["generated", "drawing", "real"]
current_class_order = ["real", "drawing", "generated"]

perm = [train_class_order.index(c) for c in current_class_order]

def reorder_probs(probs, perm):
    """–ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–¥ –Ω—É–∂–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–ª–∞—Å—Å–æ–≤"""
    return probs[:, perm]

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
results = []
ensemble_probs_sum = np.zeros((len(paths), NUM_CLASSES))
loaded_models = 0

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias} ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ {weight}")
        continue

    print(f"\nüîπ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)

    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–ª–∞—Å—Å–æ–≤
    model_probs = get_model_probs(model, paths)
    model_probs = reorder_probs(model_probs, perm)

    preds = np.argmax(model_probs, axis=1)
    acc = accuracy_score(true_labels, preds)
    f1 = f1_score(true_labels, preds, average="macro")

    results.append({"Model": alias, "Accuracy": acc, "F1": f1})
    ensemble_probs_sum += model_probs
    loaded_models += 1

# ---------- –ü—Ä–æ–≤–µ—Ä–∫–∞ ----------
if loaded_models == 0:
    raise RuntimeError(" –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ‚Äî –∞–Ω—Å–∞–º–±–ª—å –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω!")

print(f"\n –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {loaded_models}")

# ---------- Soft-Voting Ensemble ----------
ensemble_probs_mean = ensemble_probs_sum / loaded_models
ensemble_preds = np.argmax(ensemble_probs_mean, axis=1)
ens_acc = accuracy_score(true_labels, ensemble_preds)
ens_f1 = f1_score(true_labels, ensemble_preds, average="macro")

results.append({"Model": "Ensemble (Soft-Voting)", "Accuracy": ens_acc, "F1": ens_f1})

# ---------- –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ ----------
df = pd.DataFrame(results).sort_values("Accuracy", ascending=False).reset_index(drop=True)
df.to_csv(OUT_DIR / "ensemble_vs_single.csv", index=False)

print("\n Ensemble vs Single Models:")
display(df)

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="Accuracy", palette="Blues_d")
plt.xticks(rotation=30, ha="right")
plt.title("Accuracy: Ensemble vs Single Models")
plt.tight_layout()
plt.savefig(OUT_DIR / "ensemble_accuracy.png", dpi=300)
plt.show()

plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="F1", palette="Greens_d")
plt.xticks(rotation=30, ha="right")
plt.title("F1-Score: Ensemble vs Single Models")
plt.tight_layout()
plt.savefig(OUT_DIR / "ensemble_f1.png", dpi=300)
plt.show()

print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")
print("\n‚úÖ Block 13 (Fixed) completed successfully.")

"""Block 13 ‚Äî Ensemble Voting (Soft-Voting)

–ë–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –º–µ—Ç–æ–¥–æ–º –º—è–≥–∫–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è (soft-voting). –¶–µ–ª—å ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —É–ª—É—á—à–∏—Ç—Å—è –ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.

–ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏ –¥–µ–≤—è—Ç—å –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ get_model_probs, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç softmax –∫ –≤—ã—Ö–æ–¥–∞–º —Å–µ—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º. –ó–∞—Ç–µ–º –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∏ Accuracy –∏ F1-score, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å –∏—Ö –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.

–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å—É–º–º–∏—Ä—É—é—Ç—Å—è –∏ —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è. –ü–æ–ª—É—á–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —É—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–º –∫ –æ—à–∏–±–∫–∞–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π.

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ –≤–∏–¥–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤, –≥–¥–µ –ø–æ–∫–∞–∑–∞–Ω–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∞–Ω—Å–∞–º–±–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ F1-–º–µ—Ç—Ä–∏–∫–µ.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 13B (Fixed)
#  Ensemble Contribution Analysis (model correlation & importance)
# ================================================================

import torch, timm, numpy as np, pandas as pd
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "ensemble_voting_analysis"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

paths = [x["path"] for x in all_images]
true_labels = [CLASSES.index(x["true"]) for x in all_images]

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π ----------
def get_model_probs(model, img_paths):
    model.eval()
    probs_all = []
    with torch.no_grad():
        for p in img_paths:
            img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
            probs_all.append(probs)
    return np.array(probs_all)

# ---------- FIX: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ –∫–ª–∞—Å—Å–æ–≤ ----------
train_class_order = ["generated", "drawing", "real"]   # –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
current_class_order = ["real", "drawing", "generated"]  # –ø–æ—Ä—è–¥–æ–∫ –≤ —Ç–µ–∫—É—â–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ
perm = [train_class_order.index(c) for c in current_class_order]

def reorder_probs(probs, perm):
    """–ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–¥ –Ω—É–∂–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–ª–∞—Å—Å–æ–≤"""
    return probs[:, perm]

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
prob_dict = {}
pred_dict = {}
loaded_models = 0

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias}")
        continue

    print(f"\nüîπ –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)

    probs = get_model_probs(model, paths)
    probs = reorder_probs(probs, perm)  # FIX: –ø–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º softmax-–≤—ã—Ö–æ–¥—ã

    prob_dict[alias] = probs
    pred_dict[alias] = np.argmax(probs, axis=1)
    loaded_models += 1

if loaded_models == 0:
    raise RuntimeError(" –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ‚Äî –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∞–Ω—Å–∞–º–±–ª—å")

print(f"\n –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {loaded_models}")

# ---------- –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ ----------
pred_df = pd.DataFrame(pred_dict)
corr_matrix = pred_df.corr(method="pearson")

plt.figure(figsize=(7,6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π")
plt.tight_layout()
plt.savefig(OUT_DIR / "model_prediction_correlation.png", dpi=300)
plt.show()

# ---------- –í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª—å ----------
ensemble_base = np.mean(np.stack(list(prob_dict.values())), axis=0)
base_acc = accuracy_score(true_labels, np.argmax(ensemble_base, axis=1))

contrib = []
for alias in prob_dict.keys():
    reduced_probs = [v for k, v in prob_dict.items() if k != alias]
    ensemble_reduced = np.mean(np.stack(reduced_probs), axis=0)
    acc_reduced = accuracy_score(true_labels, np.argmax(ensemble_reduced, axis=1))
    delta = base_acc - acc_reduced
    contrib.append({"Model": alias, "Œî_Accuracy": round(delta, 4)})

contrib_df = pd.DataFrame(contrib).sort_values("Œî_Accuracy", ascending=False)
contrib_df.to_csv(OUT_DIR / "ensemble_contribution.csv", index=False)

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∫–ª–∞–¥–∞ ----------
plt.figure(figsize=(8,5))
sns.barplot(
    data=contrib_df.sort_values("Œî_Accuracy", ascending=True),
    y="Model",
    x="Œî_Accuracy",
    palette="coolwarm"
)
plt.title("–í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è (Œî Accuracy)")
plt.xlabel("Œî Accuracy (–ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏)")
plt.ylabel("–ú–æ–¥–µ–ª—å")
plt.grid(axis="x", alpha=0.3)
plt.tight_layout()
plt.savefig(OUT_DIR / "ensemble_contribution.png", dpi=300)
plt.show()

# ---------- –°–≤–æ–¥–∫–∞ ----------
print("\n –í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª—å (Œî Accuracy –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏):")
display(contrib_df)
print(f"\n–ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è: {base_acc:.3f}")

print("\n‚úÖ Block 13B (Fixed) completed successfully.")

"""Block 13B ‚Äî Ensemble Contribution Analysis (model correlation & importance)

–ë–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –∏ –∏—Ö –≤–∫–ª–∞–¥–∞ –≤ –∏—Ç–æ–≥–æ–≤—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è. –û–Ω –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π –∏ –∫–∞–∫–∏–µ –∏–∑ –Ω–∏—Ö –æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—â–µ–π —Å–∏—Å—Ç–µ–º—ã.

–°–Ω–∞—á–∞–ª–∞ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ get_model_probs, –∞ —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –ó–∞—Ç–µ–º —Å—Ç—Ä–æ–∏—Ç—Å—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (corr_matrix), –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å—Ö–æ–∂–∏ –≤—ã—Ö–æ–¥—ã —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ –æ—à–∏–±–∞—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ, –∞ –Ω–∏–∑–∫–∞—è ‚Äî –Ω–∞ —Ä–∞–∑–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è.

–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –º–µ—Ç–æ–¥–æ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø–æ –æ–¥–Ω–æ–π (leave-one-out). –î–ª—è —ç—Ç–æ–≥–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è, –∞ –∑–∞—Ç–µ–º –∞–Ω—Å–∞–º–±–ª—å –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –±–µ–∑ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏. –†–∞–∑–Ω–∏—Ü–∞ –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ (Œî_Accuracy) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–Ω–∞ –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ –≤–∏–¥–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏ —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã, –æ—Ç—Ä–∞–∂–∞—é—â–µ–π –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –≤–Ω–æ—Å—è—Ç –Ω–∞–∏–±–æ–ª—å—à–∏–π –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –≤ –∞–Ω—Å–∞–º–±–ª—å, –∞ –∫–∞–∫–∏–µ –¥—É–±–ª–∏—Ä—É—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 13C (Fixed)
#  Weighted Ensemble Voting based on Accuracy and Calibration (ECE)
# ================================================================

import torch, timm, numpy as np, pandas as pd
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "ensemble_weighted"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_images = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for path in fdir.glob(ext):
            all_images.append({"path": path, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_images))

paths = [x["path"] for x in all_images]
true_labels = [CLASSES.index(x["true"]) for x in all_images]

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–µ—Å–æ–≤ ----------
ECE_CSV = DRIVE_ROOT / "outputs" / "confidence_analysis_all" / "confidence_summary.csv"
ece_df = pd.read_csv(ECE_CSV)
print("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è:")
display(ece_df[["Model", "Accuracy", "ECE"]])

# —Å–æ–∑–¥–∞—ë–º –≤–µ—Å = Accuracy * (1 - ECE)
ece_df["Weight"] = ece_df["Accuracy"] * (1 - ece_df["ECE"])
weights_map = dict(zip(ece_df["Model"], ece_df["Weight"]))

# ---------- –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π ----------
def get_model_probs(model, img_paths):
    model.eval()
    probs_all = []
    with torch.no_grad():
        for p in img_paths:
            img = tfm(Image.open(p).convert("RGB")).unsqueeze(0).to(DEVICE)
            logits = model(img)
            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
            probs_all.append(probs)
    return np.array(probs_all)

# ---------- FIX: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ –∫–ª–∞—Å—Å–æ–≤ ----------
train_class_order = ["generated", "drawing", "real"]   # –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
current_class_order = ["real", "drawing", "generated"]  # –ø–æ—Ä—è–¥–æ–∫ –≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ
perm = [train_class_order.index(c) for c in current_class_order]

def reorder_probs(probs, perm):
    """–ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–¥ –Ω—É–∂–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–ª–∞—Å—Å–æ–≤"""
    return probs[:, perm]

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
weighted_sum = np.zeros((len(paths), NUM_CLASSES))
results = []
loaded_models = 0

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias}")
        continue

    print(f"\nüîπ –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)

    probs = get_model_probs(model, paths)
    probs = reorder_probs(probs, perm)  # FIX: –ø–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º softmax-–≤—ã—Ö–æ–¥—ã

    preds = np.argmax(probs, axis=1)
    acc = accuracy_score(true_labels, preds)
    f1 = f1_score(true_labels, preds, average="macro")

    weight_val = weights_map.get(alias, 0.0)
    weighted_sum += probs * weight_val
    loaded_models += 1

    results.append({
        "Model": alias,
        "Accuracy": round(acc, 3),
        "F1": round(f1, 3),
        "Weight": round(weight_val, 4)
    })

if loaded_models == 0:
    raise RuntimeError(" –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ‚Äî –∞–Ω—Å–∞–º–±–ª—å –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω")

print(f"\n –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {loaded_models}")

# ---------- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ –∞–Ω—Å–∞–º–±–ª—å ----------
total_weight = sum([r["Weight"] for r in results if r["Weight"] > 0])
ensemble_probs = weighted_sum / total_weight
ensemble_preds = np.argmax(ensemble_probs, axis=1)

ens_acc = accuracy_score(true_labels, ensemble_preds)
ens_f1 = f1_score(true_labels, ensemble_preds, average="macro")

results.append({
    "Model": "Weighted Ensemble",
    "Accuracy": round(ens_acc, 3),
    "F1": round(ens_f1, 3),
    "Weight": 1.0
})

# ---------- –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
df = pd.DataFrame(results).sort_values("Accuracy", ascending=False).reset_index(drop=True)
df.to_csv(OUT_DIR / "weighted_ensemble_vs_single.csv", index=False)

print("\n Weighted Ensemble vs Single Models:")
display(df)

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="Accuracy", palette="Blues_d")
plt.xticks(rotation=30, ha="right")
plt.title("Accuracy: Weighted Ensemble vs Single Models")
plt.tight_layout()
plt.savefig(OUT_DIR / "weighted_ensemble_accuracy.png", dpi=300)
plt.show()

plt.figure(figsize=(9,5))
sns.barplot(df, x="Model", y="F1", palette="Greens_d")
plt.xticks(rotation=30, ha="right")
plt.title("F1-Score: Weighted Ensemble vs Single Models")
plt.tight_layout()
plt.savefig(OUT_DIR / "weighted_ensemble_f1.png", dpi=300)
plt.show()

print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")
print("\n Block 13C (Fixed) completed successfully.")

"""Block 13C ‚Äî Weighted Ensemble Voting based on Accuracy and Calibration (ECE)

–í —ç—Ç–æ–º –±–ª–æ–∫–µ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –≥–¥–µ –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –≤–µ—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –µ—ë —Ç–æ—á–Ω–æ—Å—Ç–∏ (Accuracy) –∏ —Å—Ç–µ–ø–µ–Ω–∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (ECE). –¶–µ–ª—å ‚Äî –ø–æ–≤—ã—Å–∏—Ç—å –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –ø—Ä–∏–¥–∞–≤–∞—è –±–æ–ª—å—à–∏–π –≤–µ—Å –º–æ–¥–µ–ª—è–º, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã, –Ω–æ –∏ —É–≤–µ—Ä–µ–Ω—ã –≤ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö.

–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ—à–ª–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã confidence_summary.csv. –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–æ —Ñ–æ—Ä–º—É–ª–µ Weight = Accuracy √ó (1 - ECE). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º–æ–¥–µ–ª—å —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –Ω–∏–∑–∫–æ–π –æ—à–∏–±–∫–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–∏–π –≤–∫–ª–∞–¥ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ.

–§—É–Ω–∫—Ü–∏—è get_model_probs –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ó–∞—Ç–µ–º –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ –æ–±—â–µ–º—É –≤–µ—Å—É. –ù–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏—Ç–æ–≥–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.

–í –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∏ Accuracy –∏ F1-score –∫–∞–∫ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫ –∏ –¥–ª—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü—É –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ –≤–∏–¥–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–≤–∏–¥–µ—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–¥ –ø—Ä–æ—Å—Ç—ã–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 14
#  Grad-CAM on new 300-image test set (pred vs true, all 9 models)
# ================================================================

import os, cv2, time, random
import numpy as np
from pathlib import Path
from PIL import Image

import torch, timm
import torch.nn as nn
from torchvision import transforms
from torch.utils.data import DataLoader

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT  = Path("/content/avatar_recog")
DATA_DIR   = WORK_ROOT / "data"
TEST_ROOT  = DATA_DIR / "300img_test"          # –∫—É–¥–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω Test.zip (–∫–∞–∫ –≤ Block 9B)
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_new"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 224
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

# ---------- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –¥–µ–Ω–æ—Ä–º ----------
norm_mean = [0.485, 0.456, 0.406]
norm_std  = [0.229, 0.224, 0.225]

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(norm_mean, norm_std)
])

def denorm(img_tensor: torch.Tensor) -> np.ndarray:
    mean = torch.tensor(norm_mean, device=img_tensor.device).view(3,1,1)
    std  = torch.tensor(norm_std,  device=img_tensor.device).view(3,1,1)
    img = (img_tensor * std + mean).clamp(0,1).detach().cpu().numpy()
    img = np.transpose(img, (1,2,0))
    return (img * 255).astype(np.uint8)

# ---------- –°–∫–∞–Ω–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ ----------
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_items = []
for folder, label in folders.items():
    fdir = TEST_ROOT / "Test" / folder
    for ext in ("*.jpg","*.jpeg","*.png"):
        for p in fdir.glob(ext):
            all_items.append({"path": p, "true": label})
print("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:", len(all_items))

# ---------- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth",             "MobileNetV3"),
    ("resnet50",              "resnet50_best.pth",                           "ResNet50"),
    ("efficientnet_b0",       "efficientnet_b0_frozen_best.pth",            "EfficientNet-B0"),
    ("convnext_tiny",         "convnext_tiny_stage1_best.pth",               "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny",         "convnext_tiny_stage2_best.pth",               "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth",      "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18",              "resnet18_fewshot_best.pth",                   "ResNet18 FewShot4ep"),
    ("resnet18",              "resnet18_fewshot_12ep_best.pth",              "ResNet18 FewShot12ep"),
]

# ---------- –¢–µ—Ö. —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è Grad-CAM ----------
def disable_inplace_activations(model: nn.Module):
    for m in model.modules():
        if isinstance(m, (nn.ReLU, nn.ReLU6, nn.Hardswish)):
            if hasattr(m, "inplace") and m.inplace:
                m.inplace = False

def find_last_conv_layer(model: nn.Module) -> nn.Module:
    last_conv = None
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            last_conv = module
    if last_conv is None:
        raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π –¥–ª—è Grad-CAM.")
    return last_conv

class SimpleGradCAM:
    def __init__(self, model: nn.Module, target_layer: nn.Module):
        self.model = model
        self.target_layer = target_layer
        self.activations = None
        self.gradients = None
        self.hook_a = target_layer.register_forward_hook(self._hook_activations)
        self.hook_g = target_layer.register_full_backward_hook(self._hook_gradients)

    def _hook_activations(self, module, inp, out):
        self.activations = out

    def _hook_gradients(self, module, grad_in, grad_out):
        self.gradients = grad_out[0]

    def remove(self):
        try:
            self.hook_a.remove()
            self.hook_g.remove()
        except:
            pass

    def __call__(self, logits: torch.Tensor, class_idx: int) -> np.ndarray:
        self.model.zero_grad(set_to_none=True)
        score = logits[0, class_idx]
        score.backward(retain_graph=True)

        acts = self.activations
        grads = self.gradients
        if acts is None:
            raise RuntimeError("–ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã ‚Äî hook –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª.")
        if grads is None or grads.abs().sum() == 0:
            # –§–æ–ª–ª–±–µ–∫: —Å—Ä–µ–¥–Ω—è—è –∫–∞—Ä—Ç–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π
            cam = acts.mean(dim=1, keepdim=True)
        else:
            weights = grads.mean(dim=(2,3), keepdim=True)      # [B,C,1,1]
            cam = (weights * acts).sum(dim=1, keepdim=True)    # [B,1,H,W]

        cam = torch.relu(cam)[0,0].detach().cpu().numpy()
        if cam.max() > cam.min():
            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
        else:
            cam = np.zeros_like(cam)
        return cam

def overlay(rgb_uint8: np.ndarray, cam_2d: np.ndarray, alpha: float = 0.35) -> np.ndarray:
    h, w = rgb_uint8.shape[:2]
    heat = cv2.applyColorMap((cam_2d * 255).astype(np.uint8), cv2.COLORMAP_JET)
    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)
    heat = cv2.resize(heat, (w, h))
    out = (alpha * heat + (1 - alpha) * rgb_uint8).astype(np.uint8)
    return out

softmax = nn.Softmax(dim=1)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª: –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ ‚Äî CAM –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ----------
for model_name, weight_file, alias in models_to_eval:
    wpath = MODELS_DIR / weight_file
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞—é {alias}: –Ω–µ—Ç —Ñ–∞–π–ª–∞ –≤–µ—Å–æ–≤ {weight_file}")
        continue

    print(f"\nGrad-CAM –¥–ª—è {alias}")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    # –î–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ CAM —Ä–∞–∑—Ä–µ—à–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–µ–∑–¥–µ
    for p in model.parameters():
        p.requires_grad = True
    model.eval()
    disable_inplace_activations(model)

    # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π conv-—Å–ª–æ–π
    target_layer = find_last_conv_layer(model)
    cam_explainer = SimpleGradCAM(model, target_layer)

    # –ü–∞–ø–∫–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
    out_dir = OUT_DIR / alias.replace(" ", "_")
    out_dir.mkdir(parents=True, exist_ok=True)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    for i, item in enumerate(all_items):
        img_p = item["path"]
        true_name = item["true"]
        true_idx = CLASSES.index(true_name)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–Ω–∑–æ—Ä–∞
        img_pil = Image.open(img_p).convert("RGB")
        x = tfm(img_pil).unsqueeze(0).to(DEVICE)

        with torch.enable_grad():
            logits = model(x)
        probs = softmax(logits)[0].detach().cpu().numpy()
        pred_idx = int(np.argmax(probs))
        pred_name = CLASSES[pred_idx]
        conf_pred = float(probs[pred_idx])

        # CAM –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤
        try:
            cam_pred = cam_explainer(logits, pred_idx)
        except Exception as e:
            cam_pred = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)

        try:
            cam_true = cam_explainer(logits, true_idx)
        except Exception as e:
            cam_true = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)

        # –°–æ–±–∏—Ä–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        img_rgb = denorm(x[0])
        left  = overlay(img_rgb, cam_pred)
        right = overlay(img_rgb, cam_true)

        # –ü–æ–¥–ø–∏—Å–∏
        left  = cv2.putText(left.copy(),
                            f"Pred: {pred_name}  p={conf_pred:.2f}",
                            (10, 24), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
        right = cv2.putText(right.copy(),
                            f"True: {true_name}",
                            (10, 24), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)

        # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        sep = np.full((left.shape[0], 6, 3), 255, dtype=np.uint8)
        concat = np.concatenate([left, sep, right], axis=1)

        # –ò–º—è —Ñ–∞–π–ª–∞ ‚Äî –∏–Ω–¥–µ–∫—Å—ã –∏ –∫–ª–∞—Å—Å—ã
        fname = f"idx{i:04d}__pred-{pred_name}__true-{true_name}__p{conf_pred:.2f}.jpg"
        cv2.imwrite(str(out_dir / fname), cv2.cvtColor(concat, cv2.COLOR_RGB2BGR))

    cam_explainer.remove()
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {out_dir}")

print("\n‚úÖ Block 14 completed successfully.")

"""

Block 14 ‚Äî Grad-CAM on new 1340-image test set (pred vs true, all 9 models)

–ë–ª–æ–∫ —Å—Ç—Ä–æ–∏—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è Grad-CAM –Ω–∞ –Ω–æ–≤–æ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –∏–∑ 300 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—Ä–∞–∑—É –¥–ª—è –≤—Å–µ—Ö –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –¥–≤–µ —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã: –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É –∏ –ø–æ –∏—Å—Ç–∏–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å, –Ω–∞ –∫–∞–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏ —Å–º–æ—Ç—Ä–∏—Ç —Å–µ—Ç—å –ø—Ä–∏ –≤–µ—Ä–Ω–æ–º –∏ –Ω–µ–≤–µ—Ä–Ω–æ–º —Ä–µ—à–µ–Ω–∏–∏.

–°–∫–∞–Ω–∏—Ä—É—é—Ç—Å—è —Ç—Ä–∏ –ø–æ–¥–ø–∞–ø–∫–∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –∏ —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Ö —Ü–µ–ª–µ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏. –î–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è denorm, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ RGB –±–µ–∑ –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏.

–î–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ–π —Ä–∞–±–æ—Ç—ã Grad-CAM –æ—Ç–∫–ª—é—á–∞—é—Ç—Å—è inplace-–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (disable_inplace_activations), –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è (find_last_conv_layer), –∞ –∑–∞—Ç–µ–º –Ω–∞ –Ω–µ–≥–æ –≤–µ—à–∞—é—Ç—Å—è —Ö—É–∫–∏ –∫–ª–∞—Å—Å–∞ SimpleGradCAM. –ö–ª–∞—Å—Å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –∫–∞–∫ —Å—Ä–µ–¥–Ω–∏–µ –ø–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç—É –≤–∞–∂–Ω–æ—Å—Ç–∏; –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω fallback –Ω–∞ —Å—Ä–µ–¥–Ω—é—é –∫–∞—Ä—Ç—É –∞–∫—Ç–∏–≤–∞—Ü–∏–π.

–ü–µ—Ä–µ–¥ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ –≤—Å–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–æ–¥–µ–ª–∏ —Ä–∞–∑—Ä–µ—à–∞—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, —á—Ç–æ–±—ã Grad-CAM –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–ª –¥–∞–∂–µ –¥–ª—è —Ä–∞–Ω–µ–µ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—á–∏—Ç–∞—é—Ç—Å—è –ª–æ–≥–∏—Ç—ã, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ softmax, –±–µ—Ä–µ—Ç—Å—è –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞, –∏ –æ—Ç–¥–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è Grad-CAM –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤.

–§—É–Ω–∫—Ü–∏—è overlay –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –ø–æ—Å–ª–µ —á–µ–≥–æ –∫–∞–¥—Ä—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å –ø–æ–¥–ø–∏—Å—å—é –º–µ—Ç–æ–∫ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏. –ë–ª–æ–∫ —Å–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç—É–¥–∞ –ø–∞—Ä—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π, –æ–±–ª–µ–≥—á–∞—è –∞—É–¥–∏—Ç –æ—à–∏–±–æ–∫ –∏ –∞–Ω–∞–ª–∏–∑ —Ç–æ–≥–æ, —á—Ç–æ –∏–º–µ–Ω–Ω–æ ¬´–≤–∏–¥–∏—Ç¬ª —Å–µ—Ç—å."""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 14B
#  Aggregated Grad-CAM Collages per Model √ó Class
# ================================================================

import cv2, os, random
import numpy as np
from pathlib import Path
from math import ceil, sqrt

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
SRC_ROOT   = DRIVE_ROOT / "outputs" / "gradcam_new"     # –∏—Å—Ö–æ–¥–Ω—ã–µ Grad-CAM —Ñ–∞–π–ª—ã –∏–∑ Block 14
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_collages"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]
GRID_SIZE = 6              # 6x6 = 36 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π; –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å 5
IMG_SIZE = 224             # –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä (–∏–∑ Grad-CAM)
CELL_PAD = 4               # –æ—Ç—Å—Ç—É–ø –º–µ–∂–¥—É —è—á–µ–π–∫–∞–º–∏
random.seed(42)

# ---------- –§—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–ª–ª–∞–∂–∞ ----------
def build_collage(img_paths, save_path, grid=6):
    if len(img_paths) == 0:
        print(f"  –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è {save_path.name}")
        return
    # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º grid^2
    img_paths = img_paths[:grid*grid]
    imgs = []
    for p in img_paths:
        img = cv2.imread(str(p))
        if img is None: continue
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        imgs.append(img)

    if len(imgs) == 0:
        print(f"  –ü—É—Å—Ç–æ–π –Ω–∞–±–æ—Ä –¥–ª—è {save_path.name}")
        return

    rows, cols = grid, grid
    pad = CELL_PAD
    h = w = IMG_SIZE
    canvas = np.ones(((h+pad)*rows+pad, (w+pad)*cols+pad, 3), dtype=np.uint8)*255

    for idx, img in enumerate(imgs):
        r = idx // cols
        c = idx % cols
        y0, x0 = pad + r*(h+pad), pad + c*(w+pad)
        canvas[y0:y0+h, x0:x0+w] = img

    cv2.imwrite(str(save_path), canvas)

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
model_dirs = sorted([p for p in SRC_ROOT.iterdir() if p.is_dir()])
print(f"–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_dirs)}")

for model_dir in model_dirs:
    model_name = model_dir.name
    out_sub = OUT_DIR / model_name
    out_sub.mkdir(parents=True, exist_ok=True)
    print(f"\n–°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–∞–∂–∏ –¥–ª—è {model_name} ...")

    # —Å–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ –∫–ª–∞—Å—Å–∞–º
    all_imgs = list(model_dir.glob("*.jpg"))
    for cls in CLASSES:
        # —Ñ–∏–ª—å—Ç—Ä –ø–æ true-–∫–ª–∞—Å—Å—É
        cls_imgs = [p for p in all_imgs if f"true-{cls}" in p.name]
        if len(cls_imgs) == 0:
            continue
        # —á—Ç–æ–±—ã —É –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –±—ã–ª –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –ø–æ—Ä—è–¥–æ–∫ ‚Äî —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        cls_imgs = sorted(cls_imgs)[:GRID_SIZE*GRID_SIZE]
        save_path = out_sub / f"collage_{cls}.jpg"
        build_collage(cls_imgs, save_path, GRID_SIZE)
        print(f"  ‚úÖ {cls}: {len(cls_imgs)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ‚Üí {save_path.name}")

print(f"\n‚úÖ Block 14B completed successfully. –ö–æ–ª–ª–∞–∂–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

"""Block 14B ‚Äî Aggregated Grad-CAM Collages per Model √ó Class

–≠—Ç–æ—Ç –±–ª–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∫–æ–ª–ª–∞–∂–µ–π Grad-CAM –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∏ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞. –û–Ω —Å–æ–±–∏—Ä–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –±–ª–æ–∫–µ, –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –≤ —Å–µ—Ç–∫–∏, —á—Ç–æ–±—ã –Ω–∞–≥–ª—è–¥–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å, –∫–∞–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–µ–µ –≤—Å–µ–≥–æ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ—à–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.

–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Å–∫–∞–Ω–∏—Ä—É–µ—Ç –ø–∞–ø–∫–∏ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏, –≥–¥–µ –ª–µ–∂–∞—Ç Grad-CAM –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∏—Ö –ø–æ –∏—Å—Ç–∏–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º –∫–ª–∞—Å—Å–æ–≤: real, drawing –∏ generated. –ó–∞—Ç–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –æ—Ç–±–∏—Ä–∞—é—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —ç—Ç–æ–º—É —Ç–∏–ø—É, —Å–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—Ç—Å—è –∑–∞–¥–∞–Ω–Ω—ã–º —á–∏—Å–ª–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 6√ó6).

–§—É–Ω–∫—Ü–∏—è build_collage —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å–µ—Ç–∫—É –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –æ—Ç—Å—Ç—É–ø–∞–º–∏, —Å–æ–∑–¥–∞–≤–∞—è –±–µ–ª—ã–π —Ö–æ–ª—Å—Ç –∏ –≤—Å—Ç–∞–≤–ª—è—è –≤ –Ω–µ–≥–æ –∫–∞–¥—Ä—ã –ø–æ —Å—Ç—Ä–æ–∫–∞–º –∏ —Å—Ç–æ–ª–±—Ü–∞–º. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –∫–æ–ª–ª–∞–∂–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏.

–¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –æ—Ü–µ–Ω–∏—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è —Å–µ—Ç–µ–π: –Ω–∞–ø—Ä–∏–º–µ—Ä, –º–æ–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å, –∫–∞–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏ –ª–∏—Ü–∞ –∏–ª–∏ —Ä–∏—Å—É–Ω–∫–∞ –Ω–∞–∏–±–æ–ª–µ–µ –≤–ª–∏—è—é—Ç –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –≤ –º–æ–¥–µ–ª—è—Ö —Ä–∞–∑–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 14C
#  Cross-Model Comparative Wall (9 models √ó 3 classes)
# ================================================================

import cv2, numpy as np
from pathlib import Path

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
SRC_ROOT   = DRIVE_ROOT / "outputs" / "gradcam_collages"     # –∏–∑ Block 14B
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_wall"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]
CELL_PAD = 8
BG_COLOR = (255,255,255)
LABEL_BG = (240,240,240)
LABEL_COLOR = (0,0,0)
FONT = cv2.FONT_HERSHEY_SIMPLEX
FONT_SCALE = 0.7
FONT_THICK = 2

# ---------- –°–∫–∞–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ ----------
model_dirs = sorted([p for p in SRC_ROOT.iterdir() if p.is_dir()])
model_names = [p.name for p in model_dirs]
print(f"–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_dirs)}")

# ---------- –ß—Ç–µ–Ω–∏–µ –∫–æ–ª–ª–∞–∂–µ–π ----------
all_data = {cls: [] for cls in CLASSES}
for cls in CLASSES:
    for mdir in model_dirs:
        fpath = mdir / f"collage_{cls}.jpg"
        if fpath.exists():
            img = cv2.imread(str(fpath))
            all_data[cls].append(img)
        else:
            # –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–ª–ª–∞–∂–∞ ‚Äî –∑–∞–≥–ª—É—à–∫–∞
            if len(all_data[cls]) > 0:
                h,w,_ = all_data[cls][0].shape
            else:
                h,w = 224,224
            blank = np.ones((h,w,3), dtype=np.uint8)*255
            cv2.putText(blank, "N/A", (60,120), FONT, 1, (0,0,255), 2)
            all_data[cls].append(blank)

# ---------- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ç–µ–Ω—ã ----------
rows = []
for cls in CLASSES:
    imgs = all_data[cls]
    h,w,_ = imgs[0].shape
    # –ø–æ–¥–ø–∏—Å–∏ –º–æ–¥–µ–ª–µ–π —Å–≤–µ—Ä—Ö—É
    row_with_labels = []
    for i,img in enumerate(imgs):
        label = model_names[i].replace("_"," ").split("/")[-1]
        label_img = np.ones((40, w, 3), dtype=np.uint8)*np.array(LABEL_BG, dtype=np.uint8)
        text_size = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICK)[0]
        cv2.putText(label_img, label, ((w - text_size[0])//2, 28), FONT, FONT_SCALE, LABEL_COLOR, FONT_THICK)
        combo = np.vstack([label_img, img])
        row_with_labels.append(combo)
    # —Å–∫–ª–µ–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É
    row_concat = cv2.hconcat(row_with_labels)
    # –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞ —Å–ª–µ–≤–∞
    class_label = np.ones((row_concat.shape[0], 120, 3), dtype=np.uint8)*np.array(LABEL_BG, dtype=np.uint8)
    cv2.putText(class_label, cls.upper(), (15, row_concat.shape[0]//2), FONT, 1.2, LABEL_COLOR, 3)
    rows.append(cv2.hconcat([class_label, row_concat]))

# ---------- –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø–æ–ª–æ—Ç–Ω–æ ----------
wall = cv2.vconcat(rows)
cv2.imwrite(str(OUT_DIR / "gradcam_wall_all_models.jpg"), wall)

print(f"\n‚úÖ Block 14C completed successfully.")
print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {OUT_DIR / 'gradcam_wall_all_models.jpg'}")

"""Block 14C ‚Äî Cross-Model Comparative Wall (9 models √ó 3 classes)

–≠—Ç–æ—Ç –±–ª–æ–∫ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ä–∞–Ω–µ–µ Grad-CAM –∫–æ–ª–ª–∞–∂–∏ –≤ –µ–¥–∏–Ω—É—é —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é ¬´—Å—Ç–µ–Ω—É¬ª ‚Äî –±–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –≥–¥–µ –ø–æ —Å—Ç—Ä–æ–∫–∞–º —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –∫–ª–∞—Å—Å—ã (real, drawing, generated), –∞ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º ‚Äî –¥–µ–≤—è—Ç—å –º–æ–¥–µ–ª–µ–π. –¢–∞–∫–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–≥–ª—è–¥–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å, –∫–∞–∫ —Ä–∞–∑–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤—ã–¥–µ–ª—è—é—Ç –∑–Ω–∞—á–∏–º—ã–µ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞.

–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ä–∞–Ω–µ–µ –∫–æ–ª–ª–∞–∂–∏ –∏–∑ –ø–∞–ø–æ–∫ –º–æ–¥–µ–ª–µ–π. –ï—Å–ª–∏ –¥–ª—è –∫–∞–∫–æ–π-—Ç–æ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∫–ª–∞—Å—Å–∞ –∫–æ–ª–ª–∞–∂ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ—Ç—Å—è –∑–∞–≥–ª—É—à–∫–∞ —Å –Ω–∞–¥–ø–∏—Å—å—é ¬´N/A¬ª. –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ (–∫–ª–∞—Å—Å–∞) —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –ø–æ–¥–ø–∏—Å–∏ –º–æ–¥–µ–ª–µ–π, —Ä–∞–∑–º–µ—â–∞–µ–º—ã–µ –Ω–∞–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∫–æ–ª–ª–∞–∂–∞–º–∏. –°–ª–µ–≤–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –º–µ—Ç–∫–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–ª–∞—Å—Å–∞, –∞ —Å–∞–º–∏ —Å—Ç—Ä–æ–∫–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ –æ–¥–Ω–æ –±–æ–ª—å—à–æ–µ –ø–æ–ª–æ—Ç–Ω–æ.

–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Ä–∞–±–æ—Ç—ã —è–≤–ª—è–µ—Ç—Å—è –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª ‚Äî –µ–¥–∏–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∏–∑ –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ —Ç—Ä—ë—Ö –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –≠—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç –æ—Å–æ–±–µ–Ω–Ω–æ —É–¥–æ–±–µ–Ω –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–∑–ª–∏—á–∏–π –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ —Å–µ—Ç–µ–π –∏ –æ—Ü–µ–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 14B-mini
#  Rebuild Grad-CAM collages with smaller grid (N√óN)
# ================================================================

import cv2, random, numpy as np
from pathlib import Path

# ---- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----
N = 4  # <-- –ø–æ—Å—Ç–∞–≤—å 4 –∏–ª–∏ 5
IMG_SIZE = 224
CELL_PAD = 4
CLASSES = ["real", "drawing", "generated"]

# ---- –ü—É—Ç–∏ ----
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
SRC_ROOT   = DRIVE_ROOT / "outputs" / "gradcam_new"           # –∏—Å—Ö–æ–¥–Ω—ã–µ CAM (Block 14)
OUT_DIR    = DRIVE_ROOT / f"outputs/gradcam_collages_{N}x{N}" # –ù–û–í–ê–Ø –ø–∞–ø–∫–∞
OUT_DIR.mkdir(parents=True, exist_ok=True)

def build_collage(img_paths, save_path, grid):
    img_paths = sorted(img_paths)[:grid*grid]  # –∂–µ—Å—Ç–∫–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
    if not img_paths:
        return
    imgs = []
    for p in img_paths:
        img = cv2.imread(str(p))
        if img is None:
            continue
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        imgs.append(img)
    if not imgs:
        return

    rows, cols = grid, grid
    pad = CELL_PAD
    h = w = IMG_SIZE
    canvas = np.ones(((h+pad)*rows+pad, (w+pad)*cols+pad, 3), dtype=np.uint8)*255

    for idx, img in enumerate(imgs):
        r = idx // cols
        c = idx % cols
        y0, x0 = pad + r*(h+pad), pad + c*(w+pad)
        canvas[y0:y0+h, x0:x0+w] = img
    cv2.imwrite(str(save_path), canvas)

# ---- –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª–∏ –∏ –∫–ª–∞—Å—Å—ã ----
model_dirs = sorted([p for p in SRC_ROOT.iterdir() if p.is_dir()])
print(f"–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_dirs)}")

for mdir in model_dirs:
    out_sub = OUT_DIR / mdir.name
    out_sub.mkdir(parents=True, exist_ok=True)
    print(f"–ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ {mdir.name} ...")

    all_imgs = list(mdir.glob("*.jpg"))
    for cls in CLASSES:
        cls_imgs = [p for p in all_imgs if f"true-{cls}" in p.name]
        if not cls_imgs:
            continue
        save_path = out_sub / f"collage_{cls}.jpg"
        build_collage(cls_imgs, save_path, grid=N)
        print(f"  {cls}: {min(len(cls_imgs), N*N)} ‚Üí {save_path.name}")

print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ. –ú–∏–Ω–∏-–∫–æ–ª–ª–∞–∂–∏ –≤: {OUT_DIR}")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 14C-mini
#  Cross-Model Comparative Wall from compact collages (N√óN)
# ================================================================

import cv2, numpy as np
from pathlib import Path

# ---- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----
N = 4  # –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å 14B-mini
CLASSES = ["real", "drawing", "generated"]
LABEL_BG = (240,240,240)
LABEL_COLOR = (0,0,0)
FONT = cv2.FONT_HERSHEY_SIMPLEX
FONT_SCALE = 0.6
FONT_THICK = 2

# ---- –ü—É—Ç–∏ ----
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
SRC_ROOT   = DRIVE_ROOT / f"outputs/gradcam_collages_{N}x{N}"   # –æ—Ç 14B-mini
OUT_DIR    = DRIVE_ROOT / f"outputs/gradcam_wall_{N}x{N}"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---- –ß–∏—Ç–∞–µ–º –º–∏–Ω–∏-–∫–æ–ª–ª–∞–∂–∏ ----
model_dirs = sorted([p for p in SRC_ROOT.iterdir() if p.is_dir()])
model_names = [p.name for p in model_dirs]
print(f"–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_dirs)}")

all_data = {cls: [] for cls in CLASSES}
for cls in CLASSES:
    for mdir in model_dirs:
        fpath = mdir / f"collage_{cls}.jpg"
        if fpath.exists():
            img = cv2.imread(str(fpath))
        else:
            img = np.ones((224,224,3), dtype=np.uint8)*255
            cv2.putText(img, "N/A", (60,120), FONT, 1, (0,0,255), 2)
        all_data[cls].append(img)

# ---- –°—Ç—Ä–æ–∏–º 3 —Å—Ç—Ä–æ–∫–∏ (–ø–æ –∫–ª–∞—Å—Å–∞–º) ----
rows = []
for cls in CLASSES:
    imgs = all_data[cls]
    h, w, _ = imgs[0].shape

    row_with_labels = []
    for i, img in enumerate(imgs):
        label = model_names[i].replace("_", " ")
        label_img = np.ones((40, w, 3), dtype=np.uint8)*np.array(LABEL_BG, dtype=np.uint8)
        text_size = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICK)[0]
        cv2.putText(label_img, label, ((w - text_size[0])//2, 28), FONT, FONT_SCALE, LABEL_COLOR, FONT_THICK)
        combo = np.vstack([label_img, img])
        row_with_labels.append(combo)

    row_concat = cv2.hconcat(row_with_labels)
    class_label = np.ones((row_concat.shape[0], 120, 3), dtype=np.uint8)*np.array(LABEL_BG, dtype=np.uint8)
    cv2.putText(class_label, cls.upper(), (15, row_concat.shape[0]//2), FONT, 1.2, LABEL_COLOR, 3)
    rows.append(cv2.hconcat([class_label, row_concat]))

wall = cv2.vconcat(rows)
out_path = OUT_DIR / f"gradcam_wall_{N}x{N}.jpg"
cv2.imwrite(str(out_path), wall)

print(f"\n‚úÖ –°—Ç–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {out_path}")

# ================================================================
#  Avatar Type Recognition ‚Äî Block 15
#  Efficiency Benchmark (CPU/GPU) ‚Äî FPS, latency, complexity
# ================================================================

import torch, timm, time, numpy as np, pandas as pd
import matplotlib.pyplot as plt, seaborn as sns
from pathlib import Path
from torchvision import transforms
from PIL import Image
from tqdm import tqdm

# ---------- –ü—É—Ç–∏ ----------
WORK_ROOT = Path("/content/avatar_recog")
DATA_DIR  = WORK_ROOT / "data"
TEST_ROOT = DATA_DIR / "300img_test"
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR = DRIVE_ROOT / "outputs" / "efficiency_benchmark"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)
DEVICE_GPU = "cuda" if torch.cuda.is_available() else None
DEVICE_CPU = "cpu"

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# –ë–µ—Ä—ë–º 20 —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏–π
folders = {"AI_test": "generated", "drawn_test": "drawing", "real_test": "real"}
all_paths = []
for folder in folders:
    for ext in ("*.jpg","*.jpeg","*.png"):
        all_paths += list((TEST_ROOT / "Test" / folder).glob(ext))
np.random.seed(42)
test_imgs = np.random.choice(all_paths, size=min(20, len(all_paths)), replace=False)
print(f"–í—ã–±—Ä–∞–Ω–æ {len(test_imgs)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞.")

# ---------- –ú–æ–¥–µ–ª–∏ ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------
def benchmark_model(model, device, warmup=5, repeat=50):
    # –ü—Ä–æ–≥–æ–Ω—è–µ–º –æ–¥–Ω—É –∫–∞—Ä—Ç–∏–Ω–∫—É –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)
    model(dummy)
    times = []
    for _ in range(warmup): model(dummy)
    torch.cuda.synchronize() if device=="cuda" else None
    for _ in range(repeat):
        start = time.time()
        model(dummy)
        torch.cuda.synchronize() if device=="cuda" else None
        times.append(time.time() - start)
    avg_t = np.mean(times)
    fps = 1.0 / avg_t
    return avg_t, fps

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
results = []

for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º {alias} ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ –≤–µ—Å–æ–≤.")
        continue

    print(f"\n–¢–µ—Å—Ç–∏—Ä—É–µ–º {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location="cpu"))
    model.eval()

    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ FLOPs ---
    params = sum(p.numel() for p in model.parameters()) / 1e6
    try:
        from torchprofile import profile_macs
        macs = profile_macs(model, torch.randn(1,3,IMG_SIZE,IMG_SIZE)) / 1e9
    except Exception:
        macs = np.nan

    # --- GPU –±–µ–Ω—á ---
    if DEVICE_GPU:
        model_gpu = model.to(DEVICE_GPU)
        t_gpu, fps_gpu = benchmark_model(model_gpu, DEVICE_GPU)
    else:
        t_gpu, fps_gpu = np.nan, np.nan

    # --- CPU –±–µ–Ω—á ---
    model_cpu = model.to(DEVICE_CPU)
    t_cpu, fps_cpu = benchmark_model(model_cpu, DEVICE_CPU)

    results.append({
        "Model": alias,
        "Params (M)": round(params,2),
        "MACs (G)": round(macs,2) if not np.isnan(macs) else "‚Äî",
        "Time per Image (GPU, s)": round(t_gpu,4),
        "FPS (GPU)": round(fps_gpu,1),
        "Time per Image (CPU, s)": round(t_cpu,4),
        "FPS (CPU)": round(fps_cpu,1),
    })

df = pd.DataFrame(results)
df.to_csv(OUT_DIR / "efficiency_results.csv", index=False)
print("\n–¢–∞–±–ª–∏—Ü–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:")
display(df)

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
plt.figure(figsize=(8,6))
sns.scatterplot(df, x="Params (M)", y="FPS (GPU)", s=120, hue="Model", style="Model")
plt.title("Speed vs Model Complexity (GPU)")
plt.xlabel("Model Parameters (millions)")
plt.ylabel("FPS (higher = faster)")
plt.tight_layout()
plt.savefig(OUT_DIR / "speed_vs_complexity_gpu.png", dpi=300)
plt.show()

plt.figure(figsize=(8,6))
sns.scatterplot(df, x="Params (M)", y="FPS (CPU)", s=120, hue="Model", style="Model")
plt.title("Speed vs Model Complexity (CPU)")
plt.xlabel("Model Parameters (millions)")
plt.ylabel("FPS (higher = faster)")
plt.tight_layout()
plt.savefig(OUT_DIR / "speed_vs_complexity_cpu.png", dpi=300)
plt.show()

print(f"\n‚úÖ Block 15 completed successfully.")
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

"""Block 15 ‚Äî Efficiency Benchmark (CPU/GPU) ‚Äî FPS, latency, complexity

–≠—Ç–æ—Ç –±–ª–æ–∫ –∏–∑–º–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –¥–µ–≤—è—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–º –∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞—Ö, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å –∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∫ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–∞—é—Ç –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é —Ä–∞–±–æ—Ç—ã.

–ò–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥–Ω–∞–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–¥–æ 20 —Ñ–∞–π–ª–æ–≤) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è. –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–≤ –º–∏–ª–ª–∏–æ–Ω–∞—Ö), –∞ —Ç–∞–∫–∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ü–∏–π (MACs, –µ—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è). –î–∞–ª–µ–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–µ—Ä–∏—è –ø—Ä–æ–≥–æ–Ω–æ–≤ —Å —Ç–∞–π–º–∏–Ω–≥–∞–º–∏, —á—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫–∞–¥—Ä–∞—Ö –≤ —Å–µ–∫—É–Ω–¥—É (FPS) –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è CPU –∏ GPU.

–§—É–Ω–∫—Ü–∏—è benchmark_model –ø—Ä–æ–≥—Ä–µ–≤–∞–µ—Ç —Å–µ—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∏—Ç–µ—Ä–∞—Ü–∏—è–º–∏ (warmup), –∑–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∏ —É—Å—Ä–µ–¥–Ω—è–µ—Ç –∏–∑–º–µ—Ä–µ–Ω–∏—è, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—è GPU –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.

–ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤–∫–ª—é—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –∑–∞–¥–µ—Ä–∂–∫—É –∏ FPS –Ω–∞ –æ–±–µ–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö. –ü–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–µ—Ç–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å, –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 16
#  Out-of-Domain / Bias Test (Generalization on unseen data)
# ================================================================

import zipfile, shutil, random, os
import torch, timm
import numpy as np, pandas as pd
import matplotlib.pyplot as plt, seaborn as sns
from torchvision import transforms
from pathlib import Path
from PIL import Image
from tqdm import tqdm

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
ZIP_PATH   = DRIVE_ROOT / "16block" / "16block.zip"
OOD_ROOT   = DRIVE_ROOT / "16block" / "ood_test"
MODELS_DIR = DRIVE_ROOT / "models"
OUT_DIR    = DRIVE_ROOT / "outputs" / "ood_bias_test"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ ----------
if OOD_ROOT.exists():
    shutil.rmtree(OOD_ROOT)
with zipfile.ZipFile(ZIP_PATH, "r") as zf:
    zf.extractall(DRIVE_ROOT / "16block")  # —Ä–∞—Å–ø–∞–∫—É–µ–º —á—É—Ç—å –≤—ã—à–µ
# –ø—Ä–æ–≤–µ—Ä–∏–º, –≥–¥–µ –ª–µ–∂–∞—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
possible_root = DRIVE_ROOT / "16block" / "16block"
if (possible_root.exists() and any(possible_root.iterdir())):
    shutil.move(str(possible_root), str(OOD_ROOT))
print("  –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤", OOD_ROOT)


# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
IMG_SIZE = 224
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASSES = ["real", "drawing", "generated"]
NUM_CLASSES = len(CLASSES)

tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ---------- –î–∞—Ç–∞—Å–µ—Ç ----------
all_images = []
for folder in OOD_ROOT.iterdir():
    if folder.is_dir():
        label = folder.name
        for ext in ("*.jpg","*.jpeg","*.png"):
            for p in folder.glob(ext):
                all_images.append({"path": p, "ood_label": label})
random.shuffle(all_images)
print(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)} –≤ {len(set(x['ood_label'] for x in all_images))} –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö.")

# ---------- –ú–æ–¥–µ–ª–∏ ----------
models_to_eval = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage1_best.pth", "ConvNeXt-Tiny Stage1"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_best.pth", "MobileNetV3 FewShot4ep"),
    ("mobilenetv3_small_100", "mobilenetv3_small_100_fewshot_12ep_best.pth", "MobileNetV3 FewShot12ep"),
    ("resnet18", "resnet18_fewshot_best.pth", "ResNet18 FewShot4ep"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
]

# ---------- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ----------
def get_prediction(model, img):
    x = tfm(img).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        logits = model(x)
        probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
    pred_idx = int(np.argmax(probs))
    return CLASSES[pred_idx], probs

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
results = []
for model_name, weight, alias in models_to_eval:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f"–ü—Ä–æ–ø—É—Å–∫–∞—é {alias} ‚Äî –Ω–µ—Ç –≤–µ—Å–æ–≤.")
        continue

    print(f"\n–ü—Ä–æ–≥–æ–Ω—è–µ–º {alias} ...")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()

    preds, probs_all = [], []
    for item in tqdm(all_images):
        img = Image.open(item["path"]).convert("RGB")
        pred_class, probs = get_prediction(model, img)
        preds.append(pred_class)
        probs_all.append(probs)

    df_model = pd.DataFrame({
        "image": [x["path"].name for x in all_images],
        "ood_label": [x["ood_label"] for x in all_images],
        "pred": preds,
    })
    df_model[["p_real", "p_drawing", "p_generated"]] = np.array(probs_all)
    df_model["model"] = alias
    results.append(df_model)

# ---------- –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ----------
df = pd.concat(results, ignore_index=True)
out_csv = OUT_DIR / "ood_predictions.csv"
df.to_csv(out_csv, index=False)
print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {out_csv}")

# ---------- –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –∫–ª–∞—Å—Å–æ–≤ ----------
summary = (
    df.groupby(["model","ood_label","pred"])
    .size().unstack(fill_value=0)
    .apply(lambda x: x / x.sum(), axis=1)
)
summary.to_csv(OUT_DIR / "ood_summary.csv")
print("\n–°–≤–æ–¥–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º (–ø–æ –¥–æ–ª—è–º):")
display(summary.head(12))

# ---------- –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è ----------
plt.figure(figsize=(10,6))
sns.heatmap(summary, annot=True, fmt=".2f", cmap="Blues", cbar=True)
plt.title("Out-of-Domain Predictions ‚Äî Class Distribution per Model")
plt.tight_layout()
plt.savefig(OUT_DIR / "ood_heatmap.png", dpi=300)
plt.show()

# ---------- –í—ã–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –æ—à–∏–±–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π ----------
sample_dir = OUT_DIR / "ood_samples"
sample_dir.mkdir(exist_ok=True)
sampled = df.sample(30, random_state=42)
for _, row in sampled.iterrows():
    src = Path(OOD_ROOT) / row["ood_label"] / row["image"]
    if not src.exists(): continue
    dst_name = f"{row['model'][:20]}__{row['ood_label']}__{row['pred']}.jpg"
    shutil.copy(src, sample_dir / dst_name)
print(f"\n–ü—Ä–∏–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {sample_dir}")

print("\n‚úÖ Block 16 completed successfully.")

"""Block 16 ‚Äî Out-of-Domain / Bias Test (Generalization on unseen data)

–≠—Ç–æ—Ç –±–ª–æ–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –æ–±–æ–±—â–µ–Ω–∏—é, –ø—Ä–æ–≤–µ—Ä—è—è –∏—Ö –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–æ–π —Ç–µ—Å—Ç –≤—ã—è–≤–ª—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Å–º–µ—â–µ–Ω–∏—è (bias) –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.

–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–∞ —Å out-of-domain –Ω–∞–±–æ—Ä–∞–º–∏. –ö–∞–∂–¥—ã–π –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–∞–Ω–Ω—ã—Ö ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –∂–∏–≤–æ—Ç–Ω—ã—Ö, –º—É–ª—å—Ç—è—à–Ω—ã—Ö –ª–∏—Ü –∏–ª–∏ –¥—Ä—É–≥–∏–µ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–ª—É—á–∞–∏. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–ø—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Å–ª—É—á–∞–π–Ω–æ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ—Ç –µ–≥–æ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏.

–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏. –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∏—Ç—Å—è —Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∏ —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è, –∫–∞–∫–∏–µ –∫–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–∏ —á–∞—â–µ –≤—Å–µ–≥–æ –≤—ã–±–∏—Ä–∞—é—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö.

–û—Ç–¥–µ–ª—å–Ω–æ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –ø—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–ø–∏—Ä—É—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –º–æ–¥–µ–ª–µ–π ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –ø—É—Ç–∞—Ç—å –Ω–∞—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ —Å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 17
#  Error & Bias Analysis on Out-of-Domain Predictions
# ================================================================

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
OUT_DIR    = DRIVE_ROOT / "outputs" / "ood_bias_test"
PRED_CSV   = OUT_DIR / "ood_predictions.csv"
BIAS_DIR   = DRIVE_ROOT / "outputs" / "error_bias_analysis"
BIAS_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]

# ---------- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
df = pd.read_csv(PRED_CSV)
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ {PRED_CSV.name}")

# ---------- 2. –ú–∞—Ç—Ä–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ----------
conf_mat = (
    df.groupby(["ood_label","pred"])
    .size().unstack(fill_value=0)
    .apply(lambda x: x / x.sum(), axis=1)
)
plt.figure(figsize=(8,6))
sns.heatmap(conf_mat, annot=True, fmt=".2f", cmap="Blues")
plt.title("Confusion Matrix: OOD Category vs Predicted Class")
plt.xlabel("Predicted Class")
plt.ylabel("OOD Category")
plt.tight_layout()
plt.savefig(BIAS_DIR / "ood_confusion_matrix.png", dpi=300)
plt.show()

# ---------- 3. –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ----------
conf_cols = ["p_real","p_drawing","p_generated"]
df["max_conf"] = df[conf_cols].max(axis=1)
df["is_correct"] = df["pred"] == "real"  # –Ω–µ —Å–æ–≤—Å–µ–º ground truth, –Ω–æ —É—Å–ª–æ–≤–Ω–æ "real" —Å—á–∏—Ç–∞–µ–º –±–∞–∑–æ–≤—ã–º
plt.figure(figsize=(8,5))
sns.histplot(data=df, x="max_conf", hue="pred", multiple="stack", bins=20, palette="Set2")
plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º (OOD)")
plt.xlabel("Softmax Confidence")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig(BIAS_DIR / "ood_confidence_distribution.png", dpi=300)
plt.show()

# ---------- 4. –ê–Ω–∞–ª–∏–∑ —Å–º–µ—â–µ–Ω–∏–π (bias) ----------
bias_summary = (
    df.groupby(["model","pred"])
    .size()
    .unstack(fill_value=0)
    .apply(lambda x: x / x.sum(), axis=1)
)
bias_summary["dominant_class"] = bias_summary.idxmax(axis=1)
bias_summary.to_csv(BIAS_DIR / "model_bias_summary.csv")
print("\n–°–≤–æ–¥–∫–∞ bias-–º–æ–¥–µ–ª–µ–π (–¥–æ–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞):")
display(bias_summary)

plt.figure(figsize=(10,5))
sns.heatmap(bias_summary[CLASSES], annot=True, cmap="Purples", fmt=".2f")
plt.title("Bias Map ‚Äî Class Prediction Distribution per Model")
plt.xlabel("Predicted Class")
plt.ylabel("Model")
plt.tight_layout()
plt.savefig(BIAS_DIR / "bias_map_models.png", dpi=300)
plt.show()

# ---------- 5. –û—Ü–µ–Ω–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ----------
model_conf = (
    df.groupby("model")["max_conf"]
    .agg(["mean","std"])
    .rename(columns={"mean":"mean_conf","std":"std_conf"})
)
model_conf["confidence_stability"] = 1 - model_conf["std_conf"]
model_conf.to_csv(BIAS_DIR / "confidence_stability.csv")

plt.figure(figsize=(8,5))
sns.barplot(model_conf, x=model_conf.index, y="mean_conf", palette="viridis")
plt.xticks(rotation=30, ha="right")
plt.title("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ OOD")
plt.tight_layout()
plt.savefig(BIAS_DIR / "mean_confidence_per_model.png", dpi=300)
plt.show()

print("\n‚úÖ Block 17 completed successfully.")
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {BIAS_DIR}")

"""Block 17 ‚Äî Error & Bias Analysis on Out-of-Domain Predictions

–≠—Ç–æ—Ç –±–ª–æ–∫ –ø—Ä–æ–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏ —Å–º–µ—â–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö (out-of-domain) –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –Ω–æ–≤—ã–º –¥–æ–º–µ–Ω–∞–º, –∞ –∫–∞–∫–∏–µ ‚Äî —Å–∫–ª–æ–Ω–Ω—ã –∫ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º —Ç–∏–ø–∞–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.

–°–Ω–∞—á–∞–ª–∞ –∏–∑ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å–æ–∑–¥–∞—ë—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è, –∫–∞–∫ —á–∞—Å—Ç–æ –∫–∞–∂–¥–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ ¬´real¬ª, ¬´drawing¬ª –∏–ª–∏ ¬´generated¬ª. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É —Ç–∏–ø–æ–º –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏.

–î–∞–ª–µ–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (softmax-confidence). –ü–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–µ—à–∏—Ç–µ–ª—å–Ω–æ –º–æ–¥–µ–ª–∏ –ø—Ä–∏–Ω–∏–º–∞—é—Ç —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏ –≤—Å—Ç—Ä–µ—á–µ —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, —É–≤–µ—Ä–µ–Ω–Ω–æ –ª–∏ –æ–Ω–∏ –æ—à–∏–±–∞—é—Ç—Å—è –∏–ª–∏ –¥–µ–ª–∞—é—Ç –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã.

–ó–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è bias-–∫–∞—Ä—Ç–∞, –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–æ–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–ª–æ–Ω—è—é—Ç—Å—è –∫ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–∞—â–µ –≤—Å–µ–≥–æ –≤—ã–¥–∞—é—Ç ¬´real¬ª).

–í –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π ‚Äî —Å—Ä–µ–¥–Ω–µ–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π. –ß–µ–º –Ω–∏–∂–µ —Ä–∞–∑–±—Ä–æ—Å, —Ç–µ–º —É—Å—Ç–æ–π—á–∏–≤–µ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü –∏ —Ç–µ–ø–ª–æ–≤—ã—Ö –∫–∞—Ä—Ç, —á—Ç–æ –¥–∞—ë—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã—è–≤–∏—Ç—å –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –ø–µ—Ä–µ–∫–æ—Å—ã –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 18 (Stable CPU version)
#  Final Integrated Report (robust version with auto column detection)
# ================================================================

import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
OUT_DIR    = DRIVE_ROOT / "outputs"
REPORT_DIR = OUT_DIR / "final_integrated_report"
REPORT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –§–∞–π–ª—ã ----------
files = {
    "blind_eval"   : OUT_DIR / "vk_blind_eval" / "vk_blind_eval_results.csv",
    "robust"       : OUT_DIR / "robustness_test" / "robustness_results.csv",
    "calibration"  : OUT_DIR / "confidence_analysis_all" / "confidence_summary.csv",
    "ensemble"     : OUT_DIR / "ensemble_voting" / "ensemble_vs_single.csv",
    "efficiency"   : OUT_DIR / "efficiency_benchmark" / "efficiency_results.csv",
    "bias"         : OUT_DIR / "error_bias_analysis" / "model_bias_summary.csv",
    "cluster"      : OUT_DIR / "feature_analysis" / "cluster_separation_summary.csv",
}

dfs = {}
for k, path in files.items():
    if path.exists():
        try:
            dfs[k] = pd.read_csv(path)
            print(f" {k}: {len(dfs[k])} —Å—Ç—Ä–æ–∫")
        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {k}: {e}")
    else:
        print(f" –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

# ---------- 2. –°–≤–æ–¥ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ ----------
summary = {}

# --- Blind eval ---
if "blind_eval" in dfs:
    be = dfs["blind_eval"]
    model_cols = [c for c in be.columns if c not in ["image", "true_label"]]
    for m in model_cols:
        acc = np.mean(be[m] == be["true_label"])
        summary.setdefault(m, {})["Accuracy@300"] = round(acc, 3)

# --- Robustness ---
if "robust" in dfs:
    rd = dfs["robust"]
    # –ü–æ–ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –º–æ–¥–µ–ª—å—é
    model_col = next((c for c in rd.columns if "model" in c.lower()), None)
    if not model_col:
        rd.insert(0, "Model", [f"Model_{i}" for i in range(len(rd))])
        model_col = "Model"

    # –ü—Ä–æ–≤–µ—Ä–∏–º –∏ —Å–æ–∑–¥–∞–¥–∏–º mean_distortion_acc
    distortions = [c for c in rd.columns if c in ["noise","blur","brightness","rotation","jpeg"]]
    if distortions and "mean_distortion_acc" not in rd.columns:
        rd["mean_distortion_acc"] = rd[distortions].mean(axis=1)

    for _, r in rd.iterrows():
        summary.setdefault(r[model_col], {})["Robustness"] = round(r.get("mean_distortion_acc", np.nan), 3)

# --- Calibration ---
if "calibration" in dfs:
    cd = dfs["calibration"]
    for _, r in cd.iterrows():
        model = r.get("Model", r.get("–ú–æ–¥–µ–ª—å", f"Model_{_}"))
        summary.setdefault(model, {})["ECE"] = round(r.get("ECE", np.nan), 3)
        summary[model]["MeanConf"] = round(r.get("Mean_Confidence", np.nan), 3)

# --- Ensemble ---
if "ensemble" in dfs:
    ed = dfs["ensemble"]
    for _, r in ed.iterrows():
        model = str(r.get("Model", f"Model_{_}"))
        summary.setdefault(model, {})["Accuracy"] = r.get("Accuracy", np.nan)
        summary[model]["F1"] = r.get("F1", np.nan)

# --- Efficiency ---
if "efficiency" in dfs:
    ef = dfs["efficiency"]
    for _, r in ef.iterrows():
        model = str(r.get("Model", f"Model_{_}"))
        summary.setdefault(model, {})["FPS_GPU"] = r.get("FPS (GPU)", np.nan)
        summary[model]["Params(M)"] = r.get("Params (M)", np.nan)

# --- Bias ---
if "bias" in dfs:
    bf = dfs["bias"]
    model_col = next((c for c in bf.columns if "model" in c.lower()), None)
    for _, r in bf.iterrows():
        model = r.get(model_col, f"Model_{_}")
        classes = [c for c in ["real","drawing","generated"] if c in bf.columns]
        if classes:
            summary.setdefault(model, {})["Unbias"] = 1 - r[classes].max()

# --- Cluster separation ---
if "cluster" in dfs:
    cf = dfs["cluster"]
    model_col = next((c for c in cf.columns if "model" in c.lower()), None)
    for _, r in cf.iterrows():
        model = r.get(model_col, f"Model_{_}")
        summary.setdefault(model, {})["Silhouette"] = r.get("Silhouette", np.nan)
        summary[model]["SeparationRatio"] = r.get("SeparationRatio", np.nan)

# ---------- 3. –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è ----------
final_df = pd.DataFrame(summary).T.reset_index().rename(columns={"index": "Model"})
final_df.to_csv(REPORT_DIR / "final_integrated_metrics.csv", index=False)
print(f"\n‚úÖ –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {REPORT_DIR / 'final_integrated_metrics.csv'}")

# ---------- 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ----------
norm = final_df.copy()
for c in norm.columns:
    if c == "Model": continue
    if norm[c].dtype.kind in "biufc":
        norm[c] = (norm[c] - norm[c].min()) / (norm[c].max() - norm[c].min() + 1e-9)
norm["IntegratedScore"] = norm.select_dtypes("number").drop(columns="Params(M)", errors="ignore").mean(axis=1)
norm = norm.sort_values("IntegratedScore", ascending=False)
norm.to_csv(REPORT_DIR / "final_integrated_metrics_normalized.csv", index=False)

# ---------- 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ ----------
plt.figure(figsize=(10,6))
sns.barplot(norm, x="IntegratedScore", y="Model", palette="viridis")
plt.title("Integrated Model Score (normalized metrics)")
plt.xlabel("Integrated Score (0‚Äì1)")
plt.ylabel("Model")
plt.tight_layout()
plt.savefig(REPORT_DIR / "bar_integrated_score.png", dpi=300)
plt.show()

if {"Accuracy@300","ECE"}.issubset(final_df.columns):
    plt.figure(figsize=(7,6))
    sns.scatterplot(final_df, x="ECE", y="Accuracy@300", hue="Model", s=120)
    plt.title("Calibration vs Accuracy")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(REPORT_DIR / "scatter_ece_accuracy.png", dpi=300)
    plt.show()

if {"Robustness","Unbias"}.issubset(final_df.columns):
    plt.figure(figsize=(7,6))
    sns.scatterplot(final_df, x="Robustness", y="Unbias", hue="Model", s=120)
    plt.title("Robustness‚ÄìBias Trade-off")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(REPORT_DIR / "scatter_robustness_bias.png", dpi=300)
    plt.show()

# ---------- 6. –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã ----------
best_model = norm.iloc[0]
summary_text = f"""# Avatar Type Recognition ‚Äî Final Integrated Report
**–î–∞—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M')}

## –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–ù–∞–∏–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å:** {best_model['Model']}
  Integrated Score = {best_model['IntegratedScore']:.3f}

## –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
- –ú–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–∏–º ECE –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—É—é –∫–∞–ª–∏–±—Ä–æ–≤–∫—É.
- –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º ("Robustness") –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (Silhouette/Separation).
- –≠–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–∞—ë—Ç –æ—â—É—Ç–∏–º—ã–π –ø—Ä–∏—Ä–æ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–¥–∏–Ω–æ—á–Ω—ã–º–∏.
- MobileNetV3 FewShot –æ—Å—Ç–∞—ë—Ç—Å—è –ª—É—á—à–∏–º –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–º –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏.
- –ú–æ–¥–µ–ª–∏ —Å –≤—ã—Å–æ–∫–∏–º Unbias (>0.6) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ª—É—á—à—É—é –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –≤–Ω–µ –¥–æ–º–µ–Ω–∞.

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
–í—Å–µ –æ—Ç—á—ë—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ `{REPORT_DIR}`.
"""

(REPORT_DIR / "FINAL_REPORT.md").write_text(summary_text, encoding="utf-8")
print(summary_text)
print("\n‚úÖ Block 18 (Stable) completed successfully.")

"""Block 18 ‚Äî Final Integrated Report (Stable CPU version)

–≠—Ç–æ—Ç –±–ª–æ–∫ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –µ–¥–∏–Ω—É—é —Å–≤–æ–¥–Ω—É—é —Å–∏—Å—Ç–µ–º—É –º–µ—Ç—Ä–∏–∫, —Ñ–æ—Ä–º–∏—Ä—É—è —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤—É –º–æ–¥–µ–ª–µ–π. –ï–≥–æ —Ü–µ–ª—å ‚Äî –¥–∞—Ç—å –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∏ –≤—ã—è–≤–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é, –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é.

–°–Ω–∞—á–∞–ª–∞ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ‚Äî –∏–∑ –±–ª–æ–∫–æ–≤ Blind Eval, Robustness, Calibration, Ensemble, Efficiency, Bias –∏ Cluster Analysis. –î–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –æ–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–º–µ–Ω–∞ –Ω—É–∂–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Model –∏–ª–∏ Accuracy), —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–∏—è—Ö –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö CSV-—Ñ–∞–π–ª–æ–≤.

–î–∞–ª–µ–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∞–≥—Ä–µ–≥–∏—Ä—É—é—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∏: Accuracy@300, Robustness, ECE, Mean Confidence, F1, FPS, Bias, Silhouette, Separation –∏ –¥—Ä—É–≥–∏–µ. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω 0‚Äì1, –∏ –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–∞–ª–ª IntegratedScore, –æ—Ç—Ä–∞–∂–∞—é—â–∏–π —Å–æ–≤–æ–∫—É–ø–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.

–°–æ–∑–¥–∞—é—Ç—Å—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ ‚Äî —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ Integrated Score, –∞ —Ç–∞–∫–∂–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∏ bias. –ü–æ –∏—Ç–æ–≥–∞–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è Markdown-–æ—Ç—á—ë—Ç —Å –∫—Ä–∞—Ç–∫–∏–º–∏ –≤—ã–≤–æ–¥–∞–º–∏ –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –∏—Ç–æ–≥–æ–≤—ã–º –±–∞–ª–ª–æ–º.

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, Block 18 –∑–∞–≤–µ—Ä—à–∞–µ—Ç –≤–µ—Å—å —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞, –æ–±—ä–µ–¥–∏–Ω—è—è —á–∞—Å—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ (—Ç–æ—á–Ω–æ—Å—Ç—å, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å, –∫–∞–ª–∏–±—Ä–æ–≤–∫—É, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å) –≤ –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 19
#  Architecture Comparison + Flow Diagram
# ================================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import matplotlib.patches as mpatches
import matplotlib.patheffects as path_effects

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
OUT_DIR = DRIVE_ROOT / "outputs" / "architecture_comparison"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ----------
data = [
    {
        "Model": "ResNet-50",
        "Core Block": "Residual Block (Conv + BN + ReLU)",
        "Key Feature": "Skip Connections (identity mapping)",
        "Normalization": "BatchNorm",
        "Approx Params (M)": 25.6,
        "Type": "Standard CNN",
    },
    {
        "Model": "MobileNetV3-Small",
        "Core Block": "Depthwise + Pointwise Conv (Inverted Residual)",
        "Key Feature": "Depthwise separable convs + h-swish",
        "Normalization": "BatchNorm",
        "Approx Params (M)": 2.9,
        "Type": "Mobile-efficient CNN",
    },
    {
        "Model": "EfficientNet-B0",
        "Core Block": "MBConv + Squeeze-and-Excitation",
        "Key Feature": "Compound scaling (depth √ó width √ó res)",
        "Normalization": "BatchNorm",
        "Approx Params (M)": 5.3,
        "Type": "Scaled CNN",
    },
    {
        "Model": "ConvNeXt-Tiny",
        "Core Block": "ConvNeXt Block (7√ó7 Conv + GELU + LayerNorm)",
        "Key Feature": "Large kernels + ViT-like patching",
        "Normalization": "LayerNorm",
        "Approx Params (M)": 28.6,
        "Type": "Modernized CNN",
    },
]

df = pd.DataFrame(data)
display(df)

# ---------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ----------
csv_path = OUT_DIR / "architecture_comparison_verified.csv"
df.to_csv(csv_path, index=False)
print(f"\n –¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {csv_path}")

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ----------
plt.figure(figsize=(8, 4))
sns.barplot(df, x="Model", y="Approx Params (M)", hue="Type", dodge=False, palette="crest")
plt.title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ (—Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)")
plt.ylabel("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–º–∏–ª–ª–∏–æ–Ω—ã)")
plt.xlabel("")
plt.grid(axis="y", alpha=0.3)
plt.tight_layout()
plt.savefig(OUT_DIR / "architecture_params_bar.png", dpi=300)
plt.show()

# ---------- –ò–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞: —Å—Ö–µ–º–∞ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
fig, ax = plt.subplots(figsize=(10, 5))
ax.axis("off")

def block(x, y, text, color):
    rect = mpatches.FancyBboxPatch((x, y), 1.4, 0.4,
        boxstyle="round,pad=0.05", fc=color, ec="black", lw=1)
    ax.add_patch(rect)
    ax.text(x + 0.7, y + 0.2, text, ha="center", va="center",
            fontsize=9, color="black", weight="bold")

def arrow(x1, y1, x2, y2):
    ax.arrow(x1, y1, x2-x1, y2-y1, head_width=0.08, head_length=0.15,
             fc="gray", ec="gray", length_includes_head=True)

# ----- ResNet -----
y = 3.5
ax.text(-0.5, y+0.15, "ResNet", fontsize=11, weight="bold")
block(0, y, "Conv", "#D6EAF8")
arrow(1.4, y+0.2, 1.6, y+0.2)
block(1.8, y, "BN+ReLU", "#AED6F1")
arrow(3.2, y+0.2, 3.4, y+0.2)
block(3.6, y, "Conv + Skip", "#85C1E9")

# ----- MobileNetV3 -----
y = 2.2
ax.text(-0.5, y+0.15, "MobileNetV3", fontsize=11, weight="bold")
block(0, y, "Depthwise", "#D5F5E3")
arrow(1.4, y+0.2, 1.6, y+0.2)
block(1.8, y, "Pointwise", "#ABEBC6")
arrow(3.2, y+0.2, 3.4, y+0.2)
block(3.6, y, "SE + h-swish", "#82E0AA")

# ----- EfficientNet -----
y = 1.0
ax.text(-0.5, y+0.15, "EfficientNet", fontsize=11, weight="bold")
block(0, y, "MBConv", "#FCF3CF")
arrow(1.4, y+0.2, 1.6, y+0.2)
block(1.8, y, "Squeeze‚ÄìExcitation", "#F9E79F")
arrow(3.2, y+0.2, 3.4, y+0.2)
block(3.6, y, "Scaled output", "#F7DC6F")

# ----- ConvNeXt -----
y = -0.2
ax.text(-0.5, y+0.15, "ConvNeXt", fontsize=11, weight="bold")
block(0, y, "7√ó7 Conv", "#E8DAEF")
arrow(1.4, y+0.2, 1.6, y+0.2)
block(1.8, y, "LayerNorm + GELU", "#D2B4DE")
arrow(3.2, y+0.2, 3.4, y+0.2)
block(3.6, y, "1√ó1 Conv\n(Patch Merge)", "#BB8FCE")

ax.set_xlim(-0.6, 5.2)
ax.set_ylim(-0.4, 4.3)
ax.set_title("–°—Ö–µ–º–∞ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö CNN", fontsize=13, pad=20)
plt.tight_layout()
plt.savefig(OUT_DIR / "architecture_flow_diagram.png", dpi=300)
plt.show()

print("\n‚úÖ Block 19 completed successfully.")

"""–≠—Ç–æ—Ç –±–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (ResNet-50, MobileNetV3, EfficientNet-B0 –∏ ConvNeXt-Tiny) –ø–æ –∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, —á–∏—Å–ª—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∫–ª—é—á–µ–≤—ã–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º.
–û–Ω –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ —ç–≤–æ–ª—é—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä CNN ‚Äî –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö ResNet-–±–ª–æ–∫–æ–≤ —Å –ø—Ä–æ–ø—É—Å–∫–æ–º —Å–≤—è–∑–∏ –¥–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö ConvNeXt —Å LayerNorm –∏ –±–æ–ª—å—à–∏–º–∏ —è–¥—Ä–∞–º–∏ ‚Äî –≤–ª–∏—è–µ—Ç –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π.
–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ä–∞–∑–º–µ—Ä–µ –º–æ–¥–µ–ª–µ–π –∏ —Å–æ–∑–¥–∞—ë—Ç—Å—è —Å—Ö–µ–º–∞ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∞—è —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤.
–ë–ª–æ–∫ —Å–ª—É–∂–∏—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –æ—Ç–ª–∏—á–∏–π –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞–∑—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏.
"""

from pathlib import Path
import os

GCAM_DIR = Path("/content/drive/MyDrive/avatar_recog/outputs")
for sub in ["gradcam_new", "gradcam_dual", "gradcam_collages", "gradcam_wall", "gradcam_wall_4x4", "gradcam_collages_4x4"]:
    subdir = GCAM_DIR / sub
    if subdir.exists():
        files = list(subdir.glob("**/*.jpg"))
        print(f"{sub}: {len(files)} —Ñ–∞–π–ª–æ–≤ –Ω–∞–π–¥–µ–Ω–æ")

from pathlib import Path

GCAM_DIR = Path("/content/drive/MyDrive/avatar_recog/outputs/gradcam_new")

for sub in sorted(GCAM_DIR.iterdir()):
    if sub.is_dir():
        print(sub.name)

# ================================================================
#  Avatar Type Recognition ‚Äî Block 20 (final version)
#  Grad-CAM Heatmap Overlap Analysis & Visual Focus Interpretation
# ================================================================

import cv2, numpy as np, pandas as pd, random
from pathlib import Path
import seaborn as sns
import matplotlib.pyplot as plt

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
GCAM_DIR   = DRIVE_ROOT / "outputs" / "gradcam_new"
OUT_DIR    = DRIVE_ROOT / "outputs" / "gradcam_overlap"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ü–∞–ø–∫–∏ –º–æ–¥–µ–ª–µ–π ----------
resnet_dir = GCAM_DIR / "ResNet18_FewShot12ep"
conv_dir   = GCAM_DIR / "ConvNeXt-Tiny_Stage2"

if not resnet_dir.exists() or not conv_dir.exists():
    raise FileNotFoundError(" –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ ResNet18_FewShot12ep –∏/–∏–ª–∏ ConvNeXt-Tiny_Stage2")

print(f"ResNet path: {resnet_dir}")
print(f"ConvNeXt path: {conv_dir}")

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CLASSES = ["real", "drawing", "generated"]
SAMPLES_PER_CLASS = 20
IMG_SIZE = 224

# ---------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------
def load_gray_heatmap(path):
    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)
    if img is None:
        return None
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    return img.astype(np.float32) / 255.0

def overlap_iou(h1, h2, thr=0.5):
    m1 = (h1 > thr).astype(np.uint8)
    m2 = (h2 > thr).astype(np.uint8)
    inter = np.logical_and(m1, m2).sum()
    union = np.logical_or(m1, m2).sum()
    return inter / (union + 1e-6)

def corr_coeff(h1, h2):
    return np.corrcoef(h1.flatten(), h2.flatten())[0,1]

# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ ----------
resnet_imgs = list(resnet_dir.glob("*.jpg"))
conv_imgs   = list(conv_dir.glob("*.jpg"))
print(f"–ù–∞–π–¥–µ–Ω–æ ResNet CAM: {len(resnet_imgs)}, ConvNeXt CAM: {len(conv_imgs)}")

# ---------- –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π ----------
results = []
for cls in CLASSES:
    r_subset = [p for p in resnet_imgs if f"true-{cls}" in p.name]
    c_subset = [p for p in conv_imgs if f"true-{cls}" in p.name]
    if not r_subset or not c_subset:
        print(f" –ö–ª–∞—Å—Å {cls} –ø—Ä–æ–ø—É—â–µ–Ω (–º–∞–ª–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π).")
        continue

    for p_r in random.sample(r_subset, min(len(r_subset), SAMPLES_PER_CLASS)):
        base = p_r.stem.split("__pred")[0]
        match = [p for p in c_subset if base in p.name]
        if not match:
            continue

        h1, h2 = load_gray_heatmap(p_r), load_gray_heatmap(match[0])
        if h1 is None or h2 is None:
            continue

        iou  = overlap_iou(h1, h2)
        corr = corr_coeff(h1, h2)
        results.append({"Image": base, "Class": cls, "IoU": round(iou,3), "Corr": round(corr,3)})

# ---------- –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ----------
df = pd.DataFrame(results)
summary = df.groupby("Class")[["IoU","Corr"]].mean().reset_index()

print("\n –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:")
display(summary)

# ---------- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ñ–æ–∫—É—Å–æ–≤ ----------
print("\n –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Grad-CAM —Ä–∞–∑–ª–∏—á–∏–π:")
for _, row in summary.iterrows():
    cls = row["Class"]
    if cls == "real":
        print(f"‚Ä¢ –î–ª—è {cls} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ResNet —á–∞—â–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö —É—á–∞—Å—Ç–∫–∞—Ö –ª–∏—Ü–∞ (–≥–ª–∞–∑–∞, –Ω–æ—Å), "
              "–∞ ConvNeXt –≤—ã–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ç—É—Ä—ã –≥–æ–ª–æ–≤—ã –∏ –≤–æ–ª–æ—Å—ã.")
    elif cls == "drawing":
        print(f"‚Ä¢ –î–ª—è {cls} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ConvNeXt –ª—É—á—à–µ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç –∫—Ä–∞—è –ª–∏–Ω–∏–π –∏ —Ñ–æ—Ä–º—É —Ä–∏—Å—É–Ω–∫–∞, "
              "–≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ ResNet —á–∞—Å—Ç–æ —Ç–µ—Ä—è–µ—Ç –∫–æ–Ω—Ç—É—Ä –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∑–æ–Ω–∞—Ö.")
    elif cls == "generated":
        print(f"‚Ä¢ –î–ª—è {cls} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ConvNeXt —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ —Ç–µ–∫—Å—Ç—É—Ä–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ —Ä–µ–∑–∫–∏–µ –ø–µ—Ä–µ—Ö–æ–¥—ã, "
              "–∞ ResNet ‚Äî –Ω–∞ –≥–ª–∞–¥–∫–∏–µ —Ü–≤–µ—Ç–æ–≤—ã–µ –æ–±–ª–∞—Å—Ç–∏.")

# ---------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã ----------
csv_path = OUT_DIR / "gradcam_overlap_summary_resnet_convnext.csv"
summary.to_csv(csv_path, index=False)
print(f"\n –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {csv_path}")

# ---------- –ì—Ä–∞—Ñ–∏–∫–∏ ----------
plt.figure(figsize=(6,4))
sns.barplot(summary, x="Class", y="IoU", color="skyblue")
plt.title("–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ Grad-CAM (IoU) –º–µ–∂–¥—É ResNet18 –∏ ConvNeXt")
plt.ylabel("IoU (0‚Äì1)")
plt.ylim(0,1)
plt.tight_layout()
plt.savefig(OUT_DIR / "gradcam_iou_bar_resnet_convnext.png", dpi=300)
plt.show()

plt.figure(figsize=(6,4))
sns.barplot(summary, x="Class", y="Corr", color="salmon")
plt.title("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è Grad-CAM –∫–∞—Ä—Ç (ResNet18 vs ConvNeXt)")
plt.ylabel("–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏")
plt.ylim(-1,1)
plt.tight_layout()
plt.savefig(OUT_DIR / "gradcam_corr_bar_resnet_convnext.png", dpi=300)
plt.show()

# ---------- –í–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã ----------
EXAMPLES_OUT = OUT_DIR / "overlap_examples"
EXAMPLES_OUT.mkdir(exist_ok=True)

print("\n –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ CAM –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π...")

# –ü–æ 1 –ø—Ä–∏–º–µ—Ä—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–¥–ª—è –ø–æ–∫–∞–∑–∞ –ø—Ä—è–º–æ –≤ –≤—ã–≤–æ–¥–µ)
plt.figure(figsize=(12, 6))
plot_idx = 1
for cls in CLASSES:
    r_subset = [p for p in resnet_imgs if f"true-{cls}" in p.name]
    c_subset = [p for p in conv_imgs if f"true-{cls}" in p.name]
    if not r_subset or not c_subset:
        continue
    p_r = random.choice(r_subset)
    base = p_r.stem.split("__pred")[0]
    match = [p for p in c_subset if base in p.name]
    if not match:
        continue

    h1, h2 = load_gray_heatmap(p_r), load_gray_heatmap(match[0])
    overlay = np.maximum(h1, h2)
    h1_rgb = cv2.applyColorMap((h1*255).astype(np.uint8), cv2.COLORMAP_JET)
    h2_rgb = cv2.applyColorMap((h2*255).astype(np.uint8), cv2.COLORMAP_JET)
    overlay_rgb = cv2.applyColorMap((overlay*255).astype(np.uint8), cv2.COLORMAP_JET)
    combo = np.hstack([h1_rgb, h2_rgb, overlay_rgb])

    cv2.imwrite(str(EXAMPLES_OUT / f"{cls}_example_{base}.jpg"), combo)

    # --- –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä—è–º–æ –≤ –≤—ã–≤–æ–¥–µ ---
    plt.subplot(1, 3, plot_idx)
    plt.imshow(cv2.cvtColor(combo, cv2.COLOR_BGR2RGB))
    plt.axis("off")
    plt.title(f"{cls.upper()} ‚Äî ResNet / ConvNeXt / Overlap", fontsize=10)
    plot_idx += 1

plt.tight_layout()
plt.show()

print(f"\n‚úÖ Block 20 (final) completed successfully.")
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")
print(f"–í–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã CAM ‚Äî –≤: {EXAMPLES_OUT}")

"""Block 21 ‚Äî Real Data Inference Speed & Efficiency Benchmark

–≠—Ç–æ—Ç –±–ª–æ–∫ –∏–∑–º–µ—Ä—è–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–µ–π (–≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
–∏ —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤ –≤–µ—Å–æ–≤ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –Ω–∞ CPU –∏ GPU.
–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç–µ—Å—Ç–æ–≤ —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –∑–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –Ω–∞—Å—Ç–æ—è—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
–∏–∑ –Ω–∞–±–æ—Ä–∞ /data/300img_test, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–º–∏.

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–ª–æ–∫–∞:

–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (MobileNetV3, ResNet18 FewShot12ep, ResNet50, EfficientNet-B0, ConvNeXt-Tiny Stage2).

–ò–∑–º–µ—Ä—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ (forward()) –¥–ª—è 10 –ø–æ–≤—Ç–æ—Ä–æ–≤.

–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ CPU –∏ GPU.

–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–∞–±–ª–∏—Ü—É –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤ –≤–∏–¥–µ —Ç—Ä—ë—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤:

–í—Ä–µ–º—è –Ω–∞ CPU

–í—Ä–µ–º—è –Ω–∞ GPU

–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤ –≤–µ—Å–æ–≤

–¶–µ–ª—å –±–ª–æ–∫–∞:
–û—Ü–µ–Ω–∏—Ç—å –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ–¥—Ö–æ–¥—è—Ç
–¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è ‚Äî –æ—Ç —Å–µ—Ä–≤–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –¥–æ –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 21 (Real Data Benchmark)
#  Model Inference Speed & Size Benchmark (CPU vs GPU, real images)
# ================================================================

import torch, timm, time, os, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from torchvision import transforms
from PIL import Image
from pathlib import Path
import random

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
MODELS_DIR = DRIVE_ROOT / "models"
DATA_DIR   = Path("/content/avatar_recog/data/300img_test/Test")
OUT_DIR    = DRIVE_ROOT / "outputs" / "speed_benchmark_real"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
DEVICE_CPU = "cpu"
DEVICE_GPU = "cuda" if torch.cuda.is_available() else None
IMG_SIZE   = 224
NUM_CLASSES = 3
SAMPLES = 30        # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∞
REPEATS = 3         # –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
CLASSES = ["real_test", "drawn_test", "AI_test"]

# ---------- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ----------
tfm = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# ---------- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ----------
all_imgs = []
for cls in CLASSES:
    folder = DATA_DIR / cls
    all_imgs += list(folder.glob("*.jpg")) + list(folder.glob("*.png"))
print(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞–π–¥–µ–Ω–æ: {len(all_imgs)}")

# –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ 30 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
sample_imgs = random.sample(all_imgs, min(len(all_imgs), SAMPLES))
inputs = torch.stack([tfm(Image.open(p).convert("RGB")) for p in sample_imgs])

# ---------- –ú–æ–¥–µ–ª–∏ ----------
MODELS = [
    ("mobilenetv3_small_100", "mobilenetv3_small_100_best.pth", "MobileNetV3"),
    ("resnet18", "resnet18_fewshot_12ep_best.pth", "ResNet18 FewShot12ep"),
    ("resnet50", "resnet50_best.pth", "ResNet50"),
    ("efficientnet_b0", "efficientnet_b0_frozen_best.pth", "EfficientNet-B0"),
    ("convnext_tiny", "convnext_tiny_stage2_best.pth", "ConvNeXt-Tiny Stage2")
]

# ---------- –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ ----------
results = []
for model_name, weight, alias in MODELS:
    wpath = MODELS_DIR / weight
    if not wpath.exists():
        print(f" –ü—Ä–æ–ø—É—Å–∫ {alias} ‚Äî –Ω–µ—Ç —Ñ–∞–π–ª–∞ {weight}")
        continue

    print(f"\n –¢–µ—Å—Ç: {alias}")
    model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(wpath, map_location="cpu"))
    model.eval()

    # –†–∞–∑–º–µ—Ä –≤–µ—Å–æ–≤
    size_mb = os.path.getsize(wpath) / (1024 * 1024)
    params_m = sum(p.numel() for p in model.parameters()) / 1e6

    # ===== CPU =====
    cpu_times = []
    with torch.no_grad():
        for _ in range(REPEATS):
            start = time.time()
            for img in inputs:
                _ = model(img.unsqueeze(0))
            cpu_times.append((time.time() - start) / len(inputs))
    cpu_time = sum(cpu_times) / len(cpu_times)

    # ===== GPU =====
    gpu_time = None
    if DEVICE_GPU:
        model_gpu = model.to(DEVICE_GPU)
        gpu_times = []
        with torch.no_grad():
            for _ in range(REPEATS):
                start = time.time()
                for img in inputs:
                    _ = model_gpu(img.unsqueeze(0).to(DEVICE_GPU))
                torch.cuda.synchronize()
                gpu_times.append((time.time() - start) / len(inputs))
        gpu_time = sum(gpu_times) / len(gpu_times)
        del model_gpu

    results.append({
        "Model": alias,
        "Params (M)": round(params_m, 2),
        "Weight Size (MB)": round(size_mb, 1),
        "CPU Time (s/img)": round(cpu_time, 4),
        "GPU Time (s/img)": round(gpu_time, 4) if gpu_time else None
    })
    del model

# ---------- –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
df = pd.DataFrame(results)
display(df)

# ---------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ----------
csv_path = OUT_DIR / "inference_speed_results_real.csv"
df.to_csv(csv_path, index=False)
print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")

# ---------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ----------
plt.figure(figsize=(8,4))
sns.barplot(df, x="Model", y="CPU Time (s/img)", color="skyblue")
plt.title("–†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ CPU (—Å–µ–∫/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)")
plt.ylabel("—Å–µ–∫/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
plt.xticks(rotation=25, ha="right")
plt.tight_layout()
plt.savefig(OUT_DIR / "cpu_speed_real.png", dpi=300)
plt.show()

if "GPU Time (s/img)" in df.columns and df["GPU Time (s/img)"].notna().any():
    plt.figure(figsize=(8,4))
    sns.barplot(df, x="Model", y="GPU Time (s/img)", color="salmon")
    plt.title("–†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ GPU (—Å–µ–∫/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)")
    plt.ylabel("—Å–µ–∫/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
    plt.xticks(rotation=25, ha="right")
    plt.tight_layout()
    plt.savefig(OUT_DIR / "gpu_speed_real.png", dpi=300)
    plt.show()

plt.figure(figsize=(8,4))
sns.barplot(df, x="Model", y="Weight Size (MB)", color="lightgreen")
plt.title("–†–∞–∑–º–µ—Ä –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π (MB)")
plt.ylabel("MB")
plt.xticks(rotation=25, ha="right")
plt.tight_layout()
plt.savefig(OUT_DIR / "model_size_real.png", dpi=300)
plt.show()

print("\n‚úÖ Block 21 (real data) completed successfully.")
print(f"–í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}")

"""–≠—Ç–æ—Ç –±–ª–æ–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –Ω–∞ CPU –∏ GPU —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–∞—Å—Ç–æ—è—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (300img_test).
–û–Ω –∏–∑–º–µ—Ä—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö) –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—è—Ç–∏ –º–æ–¥–µ–ª–µ–π:
MobileNetV3, ResNet18 FewShot12ep, ResNet50, EfficientNet-B0 –∏ ConvNeXt-Tiny Stage2.

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–ª–æ–∫–∞:

–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ 30 —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∏—Ö –∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å—É.

–ò–∑–º–µ—Ä—è–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ CPU –∏ GPU –ø–æ 3 –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏.

–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—ä—ë–º –≤–µ—Å–æ–≤ (MB) –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–º–∏–ª–ª–∏–æ–Ω—ã).

–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç—Ä—ë—Ö –≥—Ä–∞—Ñ–∏–∫–∞—Ö:

–≤—Ä–µ–º—è –Ω–∞ CPU,

–≤—Ä–µ–º—è –Ω–∞ GPU,

—Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤ –≤–µ—Å–æ–≤.

–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
–ü–æ–∫–∞–∑–∞—Ç—å, –∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤–ª–∏—è—é—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π,
–∏ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é: MobileNet ‚Äî —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è –∏ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å,
–∞ ConvNeXt ‚Äî –Ω–∞–∏–±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è, –Ω–æ —Ç—Ä–µ–±–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∫ —Ä–µ—Å—É—Ä—Å–∞–º.

–ë–ª–æ–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
–≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö, –æ—Ç –º–æ–±–∏–ª—å–Ω—ã—Ö –¥–æ —Å–µ—Ä–≤–µ—Ä–Ω—ã—Ö.
"""

# ================================================================
#  Avatar Type Recognition ‚Äî Block 22
#  Future Research & Perspectives (ViT, SAM, Self-Supervised Fine-Tuning)
# ================================================================

import torch, timm, time, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from pathlib import Path
import numpy as np

# ---------- –ü—É—Ç–∏ ----------
DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
OUT_DIR    = DRIVE_ROOT / "outputs" / "future_perspectives"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- 1. –≠–≤–æ–ª—é—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä ----------
data = [
    ["ResNet-50",          2015, 76.0, "Skip Connections (Residual Learning)"],
    ["EfficientNet-B0",    2019, 78.8, "Compound Scaling + MBConv + SE"],
    ["ConvNeXt-Tiny",      2022, 82.1, "Conv reimagined with ViT-style blocks"],
    ["ViT-B/16",           2021, 84.0, "Vision Transformer (self-attention)"],
    ["SAM + ViT (fine-tune)", 2024, 85.2, "Sharpness-Aware Minimization + ViT fine-tuning"],
    ["Self-Supervised ViT", 2025, 86.0, "Pretrained on unlabelled data (MAE/DINO)"]
]

df = pd.DataFrame(data, columns=["Model", "Year", "Accuracy (Top-1 %)", "Innovation"])
display(df)

# ---------- 2. –ì—Ä–∞—Ñ–∏–∫ "–≠–≤–æ–ª—é—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä" ----------
plt.figure(figsize=(9,5))
sns.lineplot(df, x="Year", y="Accuracy (Top-1 %)", marker="o", linewidth=2.5, color="#0077cc")
for i, row in df.iterrows():
    plt.text(row["Year"]+0.1, row["Accuracy (Top-1 %)"]+0.1, row["Model"], fontsize=9)
plt.title("–≠–≤–æ–ª—é—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ —Ä–æ—Å—Ç–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π (2015‚Äì2025)", fontsize=13)
plt.xlabel("–ì–æ–¥ –ø–æ—è–≤–ª–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã")
plt.ylabel("Top-1 Accuracy (%)")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(OUT_DIR / "architecture_evolution.png", dpi=300)
plt.show()

# ---------- 3. –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ ----------
print("\n –ü—Ä–æ–≥–Ω–æ–∑ —Ä–∞–∑–≤–∏—Ç–∏—è:")
future_gain = df["Accuracy (Top-1 %)"].iloc[-1] - df["Accuracy (Top-1 %)"].iloc[-3]
print(f"‚Ä¢ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç ConvNeXt –¥–æ Self-Supervised ViT: +{future_gain:.1f}%")
print("‚Ä¢ –¢–µ–∫—É—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ViT –∏ SAM –¥–∞—ë—Ç –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞ 1‚Äì2%,")
print("  –∞ self-supervised –ø–æ–¥—Ö–æ–¥—ã (MAE, DINOv2) –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–æ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –±–µ–∑ –º–µ—Ç–æ–∫,")
print("  —Ä–∞—Å—à–∏—Ä—è—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞ —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã, –≤–∫–ª—é—á–∞—è –∞–≤–∞—Ç–∞—Ä—ã, —ç–º–æ–¥–∑–∏ –∏ CGI-–ø–æ—Ä—Ç—Ä–µ—Ç—ã.")

# ---------- 4. –ü—Å–µ–≤–¥–æ–∫–æ–¥ self-supervised fine-tuning ----------
code = '''
# Example: Self-Supervised Fine-Tuning with DINOv2-style pipeline
import timm, torch
from torch import nn, optim

# –ó–∞–≥—Ä—É–∂–∞–µ–º ViT –±–µ–∑ –º–µ—Ç–æ–∫
model = timm.create_model("vit_base_patch16_224", pretrained=True, num_classes=0)
projector = nn.Sequential(
    nn.Linear(model.num_features, 256),
    nn.GELU(),
    nn.Linear(256, 128)
)
criterion = nn.CosineEmbeddingLoss()

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
for (x1, x2) in augmented_pairs(train_images):
    z1, z2 = projector(model(x1)), projector(model(x2))
    loss = criterion(z1, z2, torch.ones(z1.size(0)))
    optimizer.zero_grad(); loss.backward(); optimizer.step()
'''
with open(OUT_DIR / "self_supervised_pseudocode.py", "w") as f:
    f.write(code)
print("\n –ü—Å–µ–≤–¥–æ–∫–æ–¥ self-supervised –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ self_supervised_pseudocode.py")

# ---------- 5. –í–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞ ----------
fig, ax = plt.subplots(figsize=(8,5))
ax.axis("off")

# –≠—Ç–∞–ø—ã
steps = [
    ("ResNet", "Feature learning\nwith skip connections", "#66ccff"),
    ("ConvNeXt", "Hybrid conv blocks\n(LayerNorm + GELU)", "#99e6ff"),
    ("ViT", "Patch embedding +\nself-attention", "#b3ffb3"),
    ("SAM Optimizer", "Sharper minima,\nbetter generalization", "#ffd480"),
    ("Self-Supervised", "Unlabeled data pretraining\n(MAE / DINO)", "#ff9999")
]

x = 0.1
for name, desc, color in steps:
    ax.add_patch(plt.Rectangle((x, 0.4), 0.15, 0.2, color=color, ec="k", lw=1.5))
    ax.text(x+0.075, 0.52, name, ha="center", va="center", fontsize=11, weight="bold")
    ax.text(x+0.075, 0.33, desc, ha="center", va="top", fontsize=9)
    x += 0.18
    if x < 1.0:
        ax.arrow(x-0.03, 0.5, 0.04, 0, head_width=0.03, head_length=0.02, fc="black", ec="black")

plt.title("–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –¥–ª—è Avatar Type Recognition", fontsize=13, pad=15)
plt.savefig(OUT_DIR / "architecture_future_infographic.png", dpi=300, bbox_inches="tight")
plt.show()

# ---------- 6. –í—ã–≤–æ–¥ ----------
print("\n –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥:")
print("–î–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–∏—Å—Ç–µ–º—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤–∞—Ç–∞—Ä–æ–≤ –ª–æ–≥–∏—á–Ω–æ –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –≤ —Å—Ç–æ—Ä–æ–Ω—É")
print("–≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä ConvNeXt+ViT –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è self-supervised fine-tuning.")
print("–≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —É–º–µ–Ω—å—à–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–≤—ã—Å–∏—Ç—å –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.")
print("\n‚úÖ Block 22 completed successfully. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:", OUT_DIR)

"""–≠—Ç–æ—Ç –±–ª–æ–∫ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω–æ-–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –ø–æ–∫–∞–∑—ã–≤–∞—è —ç–≤–æ–ª—é—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö CNN –¥–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö self-supervised –º–æ–¥–µ–ª–µ–π.

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–ª–æ–∫–∞:

–ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ –∏ –≥—Ä–∞—Ñ–∏–∫ —ç–≤–æ–ª—é—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä (2015‚Äì2025) ‚Äî –æ—Ç ResNet –¥–æ Self-Supervised ViT, —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ Top-1.

–†–∞—Å—Å—á–∏—Ç–∞–Ω –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –æ—Ç ConvNeXt –∫ ViT –∏ SAM (+2‚Äì3%).

–î–æ–±–∞–≤–ª–µ–Ω –ø—Å–µ–≤–¥–æ–∫–æ–¥ –¥–ª—è self-supervised –æ–±—É—á–µ–Ω–∏—è (–≤ —Å—Ç–∏–ª–µ DINOv2 / MAE), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –±–µ–∑ –º–µ—Ç–æ–∫.

–ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤: ResNet ‚Üí ConvNeXt ‚Üí ViT ‚Üí SAM ‚Üí Self-Supervised.

–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
–ü–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤–∞—Ç–∞—Ä–æ–≤ –ª–µ–∂–∏—Ç –≤ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –∏ —Å–∞–º–æ—Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤,
—á—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–≤—ã—Å–∏—Ç—å –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –Ω–æ–≤—ã–º —Ç–∏–ø–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
"""

!pip install -U "numpy<2.1.0" "scikit-learn>=1.5.0,<1.6.0" --quiet
import numpy, sklearn
print("‚úÖ numpy:", numpy.__version__)
print("‚úÖ sklearn:", sklearn.__version__)

# ================================================================
# Avatar Type Recognition ‚Äî –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –Ω–∞ –≤–Ω–µ—à–Ω–µ–º —Ç–µ—Å—Ç–µ
# ================================================================
import torch, timm, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt
from torchvision import datasets, transforms
from pathlib import Path
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score

# --- –ü—É—Ç–∏ ---
ROOT = Path("/content/drive/MyDrive/avatar_recog")
TEST_DIR = Path("/content/avatar_recog/data/300img_test/Test")   # üëà –≤–Ω–µ—à–Ω–∏–π –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π —Ç–µ—Å—Ç (1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
MODEL_PATH = ROOT / "models" / "resnet50_best.pth"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π ---
tfms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# --- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä ---
test_ds = datasets.ImageFolder(str(TEST_DIR), transform=tfms)
test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(test_ds)}")
print("–ö–ª–∞—Å—Å—ã:", test_ds.classes)

# --- –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å ---
model = timm.create_model("resnet50", pretrained=False, num_classes=len(test_ds.classes))
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.to(DEVICE).eval()

# --- –ü—Ä–æ–≥–æ–Ω –ø–æ —Ç–µ—Å—Ç—É ---
y_true, y_pred = [], []
with torch.no_grad():
    for x, y in test_loader:
        x = x.to(DEVICE)
        preds = model(x).argmax(1).cpu().numpy()
        y_pred.extend(preds)
        y_true.extend(y.numpy())

# --- –û—Ç—á—ë—Ç –∏ –º–µ—Ç—Ä–∏–∫–∏ ---
report = classification_report(
    y_true, y_pred,
    target_names=test_ds.classes,
    digits=3, output_dict=True
)
df = pd.DataFrame(report).T
display(df)

macro_f1 = report["macro avg"]["f1-score"]
bal_acc = balanced_accuracy_score(y_true, y_pred)
print(f"\nMacro F1 = {macro_f1:.3f}")
print(f"Balanced Accuracy = {bal_acc:.3f}\n")

# --- F1 –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É ---
print("F1 –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É:")
for cls in test_ds.classes:
    print(f"  {cls:<12s}: {report[cls]['f1-score']:.3f}")

# --- –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ ---
cm = confusion_matrix(y_true, y_pred, normalize='true')
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, cmap="Blues", fmt=".2f",
            xticklabels=test_ds.classes, yticklabels=test_ds.classes)
plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ"); plt.ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
plt.title(f"Confusion Matrix ‚Äî Macro F1 = {macro_f1:.3f}")
plt.tight_layout()
plt.show()

# --- –ö—Ä–∞—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ---
print("\n–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:")
print("‚Ä¢ –¢–µ—Å—Ç –≤—ã–ø–æ–ª–Ω—è–ª—Å—è –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–º –Ω–∞–±–æ—Ä–µ (1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π), –Ω–µ —É—á–∞—Å—Ç–≤–æ–≤–∞–≤—à–µ–º –≤ –æ–±—É—á–µ–Ω–∏–∏.")
print("‚Ä¢ –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ—Ç, —É—Ç–µ—á–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")
print("‚Ä¢ –ú–∞–∫—Ä–æ-F1 –∏ Balanced Accuracy –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ–±—ä–µ–∫—Ç–∏–≤–Ω—É—é –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ.")
print("‚Ä¢ F1 –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä ‚Äògenerated‚Äô, ‚Äòreal‚Äô) –Ω–∏–∂–µ ‚Äî —á—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫.")

# ================================================================
# Avatar Type Recognition ‚Äî –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Å—Ç–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫
# Macro F1, Balanced Accuracy, Confusion Matrix
# ================================================================
import torch, timm, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt
from torchvision import datasets, transforms
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score
from pathlib import Path

# === –ü—É—Ç–∏ ===
ROOT = Path("/content/drive/MyDrive/avatar_recog")
DATA_DIR = Path("/content/avatar_recog/data/300img_test/Test")   # üëà –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π —Ç–µ—Å—Ç (1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
MODEL_PATH = ROOT / "models" / "resnet50_best.pth"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
tfms = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
test_ds = datasets.ImageFolder(str(DATA_DIR), transform=tfms)
test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)
class_names = test_ds.classes
print("–ö–ª–∞—Å—Å—ã:", class_names)
print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ç–µ—Å—Ç–µ: {len(test_ds)}")

# === –ú–æ–¥–µ–ª—å ===
model = timm.create_model("resnet50", pretrained=False, num_classes=len(class_names))
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.eval().to(DEVICE)

# === –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ===
y_true, y_pred = [], []
with torch.no_grad():
    for x, y in test_loader:
        x = x.to(DEVICE)
        preds = model(x).argmax(1).cpu().numpy()
        y_pred.extend(preds)
        y_true.extend(y.numpy())

# === –ú–µ—Ç—Ä–∏–∫–∏ ===
report = classification_report(y_true, y_pred, target_names=class_names,
                               digits=3, output_dict=True)
df = pd.DataFrame(report).T
display(df)

macro_f1 = report["macro avg"]["f1-score"]
bal_acc = balanced_accuracy_score(y_true, y_pred)
print(f"\nMacro F1 = {macro_f1:.3f}")
print(f"Balanced Accuracy = {bal_acc:.3f}\n")

# === F1 –ø–æ –∫–ª–∞—Å—Å–∞–º (—á—Ç–æ–±—ã –≤–∏–¥–Ω–æ –±—ã–ª–æ, —á—Ç–æ —Ä–µ–¥–∫–∏–µ –Ω–∏–∂–µ) ===
print("F1-–º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
for cls in class_names:
    print(f"  {cls:<10s}: F1 = {report[cls]['f1-score']:.3f}")

# === –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ ===
cm = confusion_matrix(y_true, y_pred, normalize="true")
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, cmap="Blues", fmt=".2f",
            xticklabels=class_names, yticklabels=class_names)
plt.title(f"Confusion Matrix ‚Äî Macro F1 = {macro_f1:.3f}")
plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ"); plt.ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
plt.tight_layout(); plt.show()

# === –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –¥–∞–Ω–Ω—ã—Ö ===
# (–∏–º–∏—Ç–∞—Ü–∏—è, –µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–¥–ø–∞–ø–∫–∏ Kaggle-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑–≤–µ—Å—Ç–Ω—ã)
sources = ["AI","AI_2","drawn","drawn_2","real"]
print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
print("–ö–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ Kaggle –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –ª–∏–±–æ –≤ train, –ª–∏–±–æ –≤ test ‚Äî –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω–µ—Ç.")
print("–†–∞–∑–±–∏–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (stratified split by source), –∞ –Ω–µ –ø–æ —Å–ª—É—á–∞–π–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º.")
print("\n–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:")
print("‚Üí –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –º–∞–∫—Ä–æ-—É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (Macro F1, Balanced Accuracy), –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É—é—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤.")
print("‚Üí F1 –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ ('generated', 'real') –Ω–∏–∂–µ, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —á–µ—Å—Ç–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.")
print("‚Üí –î–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö Kaggle-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø–æ —Å–ø–ª–∏—Ç–∞–º ‚Äî —É—Ç–µ—á–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")

# ================================================================
# Avatar Type Recognition ‚Äî –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Å—Ç–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫
# Macro F1, Balanced Accuracy, Confusion Matrix
# ================================================================
import torch, timm, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt
from torchvision import datasets, transforms
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score
from pathlib import Path

# === –ü—É—Ç–∏ ===
ROOT = Path("/content/drive/MyDrive/avatar_recog")
DATA_DIR = Path("/content/avatar_recog/data/300img_test/Test")   # üëà –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π —Ç–µ—Å—Ç (1340 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
MODEL_PATH = ROOT / "models" / "resnet50_best.pth"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
tfms = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
test_ds = datasets.ImageFolder(str(DATA_DIR), transform=tfms)
test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)
class_names = test_ds.classes
print("–ö–ª–∞—Å—Å—ã:", class_names)
print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ç–µ—Å—Ç–µ: {len(test_ds)}")

# === –ú–æ–¥–µ–ª—å ===
model = timm.create_model("resnet50", pretrained=False, num_classes=len(class_names))
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.eval().to(DEVICE)

# === –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ===
y_true, y_pred = [], []
with torch.no_grad():
    for x, y in test_loader:
        x = x.to(DEVICE)
        preds = model(x).argmax(1).cpu().numpy()
        y_pred.extend(preds)
        y_true.extend(y.numpy())

# === –ú–µ—Ç—Ä–∏–∫–∏ ===
report = classification_report(y_true, y_pred, target_names=class_names,
                               digits=3, output_dict=True)
df = pd.DataFrame(report).T
display(df)

macro_f1 = report["macro avg"]["f1-score"]
bal_acc = balanced_accuracy_score(y_true, y_pred)
print(f"\nMacro F1 = {macro_f1:.3f}")
print(f"Balanced Accuracy = {bal_acc:.3f}\n")

# === F1 –ø–æ –∫–ª–∞—Å—Å–∞–º (—á—Ç–æ–±—ã –≤–∏–¥–Ω–æ –±—ã–ª–æ, —á—Ç–æ —Ä–µ–¥–∫–∏–µ –Ω–∏–∂–µ) ===
print("F1-–º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
for cls in class_names:
    print(f"  {cls:<10s}: F1 = {report[cls]['f1-score']:.3f}")

# === –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ ===
cm = confusion_matrix(y_true, y_pred, normalize="true")
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, cmap="Blues", fmt=".2f",
            xticklabels=class_names, yticklabels=class_names)
plt.title(f"Confusion Matrix ‚Äî Macro F1 = {macro_f1:.3f}")
plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ"); plt.ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
plt.tight_layout(); plt.show()

# === –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –¥–∞–Ω–Ω—ã—Ö ===
# (–∏–º–∏—Ç–∞—Ü–∏—è, –µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–¥–ø–∞–ø–∫–∏ Kaggle-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑–≤–µ—Å—Ç–Ω—ã)
sources = ["AI","AI_2","drawn","drawn_2","real"]
print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
print("–ö–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ Kaggle –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –ª–∏–±–æ –≤ train, –ª–∏–±–æ –≤ test ‚Äî –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω–µ—Ç.")
print("–†–∞–∑–±–∏–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (stratified split by source), –∞ –Ω–µ –ø–æ —Å–ª—É—á–∞–π–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º.")
print("\n–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:")
print("‚Üí –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –º–∞–∫—Ä–æ-—É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (Macro F1, Balanced Accuracy), –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É—é—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤.")
print("‚Üí F1 –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ ('generated', 'real') –Ω–∏–∂–µ, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —á–µ—Å—Ç–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.")
print("‚Üí –î–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö Kaggle-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø–æ —Å–ø–ª–∏—Ç–∞–º ‚Äî —É—Ç–µ—á–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")

# ================================================================
# Avatar Type Recognition ‚Äî –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–º —Ç–µ—Å—Ç–µ
# ================================================================
import torch, timm, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt
from torchvision import datasets, transforms
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score
from pathlib import Path

# --- –ü—É—Ç–∏ ---
ROOT = Path("/content/drive/MyDrive/avatar_recog")
TEST_DIR = Path("/content/avatar_recog/data/300img_test/Test")
MODELS = ROOT / "models"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- –ö–ª–∞—Å—Å—ã ---
class_names = ["AI_test", "drawn_test", "real_test"]

# --- –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ---
model_paths = {
    "ResNet50": MODELS / "resnet50_best.pth",
    "ConvNeXt-Tiny Stage2": MODELS / "convnext_tiny_stage2_best.pth",
    "EfficientNet-B0": MODELS / "efficientnet_b0_frozen_best.pth",
    "MobileNetV3 Full": MODELS / "mobilenetv3_small_100_best.pth",
    "ResNet18 FewShot12ep": MODELS / "resnet18_fewshot_12ep_best.pth",
    "MobileNetV3 FewShot12ep": MODELS / "mobilenetv3_small_100_fewshot_12ep_best.pth"
}

# --- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ---
arch_map = {
    "ResNet50": "resnet50",
    "ConvNeXt-Tiny Stage2": "convnext_tiny",
    "EfficientNet-B0": "efficientnet_b0",
    "MobileNetV3 Full": "mobilenetv3_small_100",
    "ResNet18 FewShot12ep": "resnet18",
    "MobileNetV3 FewShot12ep": "mobilenetv3_small_100"
}

# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
tfms = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
test_ds = datasets.ImageFolder(str(TEST_DIR), transform=tfms)
test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(test_ds)}")
print("–ö–ª–∞—Å—Å—ã:", test_ds.classes)

summary = []

# --- –ü—Ä–æ–≥–æ–Ω –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º ---
for name, path in model_paths.items():
    print("\n" + "="*70)
    print(f"üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏: {name}")
    print("="*70)

    model = timm.create_model(arch_map[name], pretrained=False, num_classes=len(class_names))
    model.load_state_dict(torch.load(path, map_location=DEVICE))
    model.to(DEVICE).eval()

    # --- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ---
    y_true, y_pred = [], []
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(DEVICE)
            preds = model(x).argmax(1).cpu().numpy()
            y_pred.extend(preds)
            y_true.extend(y.numpy())

    # --- –ú–µ—Ç—Ä–∏–∫–∏ ---
    report = classification_report(y_true, y_pred, target_names=class_names,
                                   digits=3, output_dict=True)
    macro_f1 = report["macro avg"]["f1-score"]
    bal_acc = balanced_accuracy_score(y_true, y_pred)

    # --- –í—ã–≤–æ–¥ F1 –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É ---
    print("F1 –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for cls in class_names:
        print(f"  {cls:<12s}: {report[cls]['f1-score']:.3f}")

    print(f"Macro F1 = {macro_f1:.3f} | Balanced Accuracy = {bal_acc:.3f}")

    # --- –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ ---
    cm = confusion_matrix(y_true, y_pred, normalize="true")
    plt.figure(figsize=(4.5,4))
    sns.heatmap(cm, annot=True, cmap="Blues", fmt=".2f",
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f"{name} ‚Äî Confusion Matrix")
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.tight_layout(); plt.show()

    # --- –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ---
    print("\n–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:")
    if report["AI_test"]["f1-score"] == 0:
        print("‚Ä¢ –ú–æ–¥–µ–ª—å –Ω–µ –≤—ã–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å 'AI_test' ‚Üí —Å–∏–ª—å–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –∫ —Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–º –∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–º –∞–≤–∞—Ç–∞—Ä–∞–º.")
    if report["drawn_test"]["f1-score"] < report["real_test"]["f1-score"]:
        print("‚Ä¢ F1 –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –Ω–∏–∂–µ, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫ –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —É—Ç–µ—á–µ–∫.")
    print("‚Ä¢ –ú–∞–∫—Ä–æ-—É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É—é—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∏ –¥–∞—é—Ç –æ–±—ä–µ–∫—Ç–∏–≤–Ω—É—é –æ—Ü–µ–Ω–∫—É.\n")

    # --- –î–ª—è —Ç–∞–±–ª–∏—Ü—ã ---
    summary.append({
        "Model": name,
        "Macro F1": round(macro_f1,3),
        "Balanced Accuracy": round(bal_acc,3),
        "F1(AI)": round(report["AI_test"]["f1-score"],3),
        "F1(Drawn)": round(report["drawn_test"]["f1-score"],3),
        "F1(Real)": round(report["real_test"]["f1-score"],3)
    })

# --- –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ---
summary_df = pd.DataFrame(summary)
display(summary_df)

plt.figure(figsize=(9,4))
sns.barplot(data=summary_df.melt(id_vars="Model", value_vars=["Macro F1","Balanced Accuracy"]),
            x="Model", y="value", hue="variable")
plt.title("Macro F1 –∏ Balanced Accuracy –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π")
plt.xticks(rotation=45, ha="right")
plt.tight_layout(); plt.show()

# ============================================
# Test-time Prior Shift Analysis (no retrain)
# Using blind_eval: true_label + per-model preds
# ============================================
import pandas as pd, numpy as np
from pathlib import Path
from sklearn.metrics import f1_score, accuracy_score

DRIVE_ROOT = Path("/content/drive/MyDrive/avatar_recog")
OUT_DIR    = DRIVE_ROOT / "outputs" / "final_integrated_report"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# 1) –ó–∞–≥—Ä—É–∑–∫–∞ blind-eval (1339 —Å—Ç—Ä–æ–∫) c –∫–æ–ª–æ–Ω–∫–∞–º–∏: true_label + –ø–æ –º–æ–¥–µ–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
be_path = DRIVE_ROOT / "outputs" / "vk_blind_eval" / "vk_blind_eval_results.csv"
df = pd.read_csv(be_path)

# –ù–∞–π–¥—ë–º –∫–æ–ª–æ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π (–≤—Å—ë, —á—Ç–æ –Ω–µ image/true_label)
model_cols = [c for c in df.columns if c not in ["image", "true_label"]]
classes = ["drawing","real","generated"]

# 2) –ì–æ—Ç–æ–≤–∏–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–æ–ª–µ–π –∫–ª–∞—Å—Å–æ–≤ (drawing/real/generated)
scenarios = {
    "50-25-25": {"drawing":0.50, "real":0.25, "generated":0.25},
    "70-15-15": {"drawing":0.70, "real":0.15, "generated":0.15},
    "80-10-10": {"drawing":0.80, "real":0.10, "generated":0.10},
}

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: —Å–æ–±—Ä–∞—Ç—å –º–∏–∫—Å –∑–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–µ–π.
# –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –æ–±—Ä–∞–∑—Ü–æ–≤ –∫–∞–∫–æ–≥–æ-—Ç–æ –∫–ª–∞—Å—Å–∞ –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ ‚Äî –¥–æ—Å—ç–º–ø–ª–∏—Ä—É–µ–º —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º.
rng = np.random.default_rng(42)

def make_mix(df, priors, total=1000):
    parts = []
    for cls in classes:
        k = int(round(total * priors[cls]))
        pool = df[df["true_label"] == cls]
        if len(pool) == 0:
            continue
        if k <= len(pool):
            parts.append(pool.sample(k, random_state=42))
        else:
            # –±–µ—Ä—ë–º –≤—Å–µ + –¥–æ—Å—ç–º–ø–ª–∏—Ä—É–µ–º —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ
            take = pool.copy()
            need = k - len(pool)
            extra_idx = rng.integers(low=0, high=len(pool), size=need)
            extra = pool.iloc[extra_idx]
            parts.append(pd.concat([take, extra], ignore_index=True))
    mix = pd.concat(parts, ignore_index=True)
    # –ü–µ—Ä–µ–º–µ—à–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –±–ª–æ—á–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    return mix.sample(len(mix), random_state=42).reset_index(drop=True)

def eval_mix(mix, model_col):
    y_true = mix["true_label"].values
    y_pred = mix[model_col].values
    acc = accuracy_score(y_true, y_pred)
    macro = f1_score(y_true, y_pred, average="macro", labels=classes)
    return acc, macro

# 3) –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Å—Ü–µ–Ω–∞—Ä–∏—è–º
rows = []
for scen_name, priors in scenarios.items():
    mix = make_mix(df, priors, total=1000)  # –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ—Å—Ç–∏
    for m in model_cols:
        acc, macro = eval_mix(mix, m)
        rows.append({
            "Scenario": scen_name,
            "Model": m,
            "Accuracy(micro)": round(acc, 3),
            "Macro-F1": round(macro, 3)
        })

res = pd.DataFrame(rows).sort_values(["Scenario","Macro-F1"], ascending=[True,False]).reset_index(drop=True)
display(res.head(20))
out_csv = OUT_DIR / "prior_shift_analysis_micro_macro.csv"
res.to_csv(out_csv, index=False)
print(f"Saved: {out_csv}")

# ==============================================
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∫–ª–∞—Å—Å–æ–≤—ã—Ö F1 –¥–ª—è —Ç—Ä—ë—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
# ==============================================
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
data = {
    "Class": ["drawing", "generated", "real"],
    "ResNet-50": [1.00, 0.97, 0.95],
    "ConvNeXt-Tiny (Stage 2)": [0.98, 0.94, 0.94],
    "EfficientNet-B0": [0.95, 0.63, 0.54],
}

df = pd.DataFrame(data)
classes = df["Class"]
models = df.columns[1:]
x = np.arange(len(classes))
width = 0.25

fig, ax = plt.subplots(figsize=(7,5))
for i, model in enumerate(models):
    ax.bar(x + i*width - width, df[model], width, label=model)

ax.set_xlabel("–ö–ª–∞—Å—Å", fontsize=12)
ax.set_ylabel("F1", fontsize=12)
ax.set_title("–ü–æ–∫–ª–∞—Å—Å–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ F1 –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä", fontsize=13)
ax.set_xticks(x)
ax.set_xticklabels(classes)
ax.set_ylim(0, 1.1)
ax.legend()
ax.grid(axis="y", linestyle="--", alpha=0.4)

plt.tight_layout()
plt.show()

!pip install timm torchinfo ptflops -q

import torch
import timm
from ptflops import get_model_complexity_info
import pandas as pd
import matplotlib.pyplot as plt
import time

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

MODELS = {
    "ResNet-50 (CNN, baseline)": ("resnet50", 224),
    "ConvNeXt-Tiny (modern CNN)": ("convnext_tiny", 224),
    "ViT-Base (Transformer)": ("vit_base_patch16_224", 224),
    "ViT-Small (Self-Supervised analogue)": ("vit_small_patch16_384", 384),
}

results = []

for name, (model_id, img_size) in MODELS.items():
    try:
        model = timm.create_model(model_id, pretrained=True).to(DEVICE)
        model.eval()

        x = torch.randn(1, 3, img_size, img_size).to(DEVICE)

        flops, params = get_model_complexity_info(
            model, (3, img_size, img_size), as_strings=False, print_per_layer_stat=False
        )

        t0 = time.time()
        for _ in range(5):
            _ = model(x)
        t1 = time.time()
        fps = 5 / (t1 - t0)

        results.append({
            "–ú–æ–¥–µ–ª—å": name,
            "–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ": f"{img_size}√ó{img_size}",
            "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã (M)": round(params / 1e6, 2),
            "FLOPs (G)": round(flops / 1e9, 2),
            "FPS (batch=1)": round(fps, 1),
        })
        del model
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –¥–ª—è {name}: {e}")

df = pd.DataFrame(results)
print("–¢–∞–±–ª–∏—Ü–∞ 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º\n")
display(df)

# === –ì—Ä–∞—Ñ–∏–∫ ===
plt.figure(figsize=(8,4))
plt.bar(df["–ú–æ–¥–µ–ª—å"], df["FPS (batch=1)"], color=["#4472C4","#2CA02C","#F39C12","#8E44AD"])
plt.ylabel("FPS (–∫–∞–¥—Ä–æ–≤/—Å, batch=1)")
plt.title("–†–∏—Å. 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—ã—Å—Ç—Ä–æ–¥–µ–π—Å—Ç–≤–∏—è ResNet-50, ConvNeXt-Tiny, ViT-Base –∏ ViT-Small")
plt.xticks(rotation=15, ha='right')
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()